<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="15 Optimum Approximation | Project XXII" />
<meta property="og:type" content="book" />





<meta name="author" content="Zhengyuan Gao" />

<meta name="date" content="2020-12-19" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="15 Optimum Approximation | Project XXII">

<title>15 Optimum Approximation | Project XXII</title>

<link href="libs/tufte-css/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding/datatables.js"></script>
<link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>



</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Project XXII<p><p class="author">Zhengyuan Gao</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BA; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Preface</a>
<a href="part-i-inference-based-upon-logic-and-logical-computation.html">PART I: Inference Based upon Logic and Logical Computation</a>
<a href="sub-logic.html"><span class="toc-section-number">1</span> Logic</a>
<a href="sub-set-theory.html"><span class="toc-section-number">2</span> Set Theory</a>
<a href="sub-axioms.html"><span class="toc-section-number">3</span> Axioms</a>
<a href="sub-inferknow.html"><span class="toc-section-number">4</span> Inference and Knowledge</a>
<a href="sub-incomplete.html"><span class="toc-section-number">5</span> Incompleteness</a>
<a href="part-ii-infinitesimal-changes-and-their-consequences.html">PART II: Infinitesimal Changes and their Consequences</a>
<a href="sub-continuity.html"><span class="toc-section-number">6</span> Continuity</a>
<a href="sub-calculus.html"><span class="toc-section-number">7</span> Calculus</a>
<a href="ch-DE.html"><span class="toc-section-number">8</span> Differential Equations</a>
<a href="ch-CalUn.html"><span class="toc-section-number">9</span> Calculus under Uncertainty</a>
<a href="part-iii-emergence-of-abstract-interactions.html">PART III: Emergence of Abstract Interactions</a>
<a href="ch-vecMat.html"><span class="toc-section-number">10</span> Vector and Matrix</a>
<a href="ch-MatComp.html"><span class="toc-section-number">11</span> Matrix Computation</a>
<a href="ch-eigen.html"><span class="toc-section-number">12</span> Eigenvalues and Eigenvectors</a>
<a href="ch-UnMulti.html"><span class="toc-section-number">13</span> Uncertainty in Multiple Dimensions</a>
<a href="part-iv-three-masons-to-illuminate-the-dual-world.html">PART IV: Three Masons to Illuminate the Dual World</a>
<a href="ch-representation.html"><span class="toc-section-number">14</span> Representation</a>
<a id="active-page" href="ch-optApp.html"><span class="toc-section-number">15</span> Optimum Approximation</a><ul class="toc-sections">
<li class="toc"><a href="#sub:appSys"> Approximating Systems</a></li>
<li class="toc"><a href="#sub:Optimization"> Optimization and Inequalities</a></li>
<li class="toc"><a href="#sub:Proj1"> Miscellaneous Examples: Part 1</a></li>
</ul>
<a href="bibliography.html">Bibliography</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="ch:optApp" class="section level1">
<h1>
<span class="header-section-number">15</span> Optimum Approximation</h1>
<p>The adjective word optimum and the noun approximation reflect two philosophical, one secular and one sacred, attitudes towards a closeted world. Suppose the representation was thought of relating to the omniscience of God; in that case, the approximation should distract our attention from omnipotences to a milieu of <em>bounded beings</em> who are continuously driven by the operations with imperfect, limited scopes.<label for="tufte-sn-366" class="margin-toggle sidenote-number">366</label><input type="checkbox" id="tufte-sn-366" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">366</span> If bounded operations are defined in a proper space (a <a href="ch-MatComp.html#sub:vecSpaces">vector space</a> with defined <a href="ch-vecMat.html#sub:vec">norms</a> and <a href="ch-vecMat.html#sub:linearity">linear structures</a>), we can also call these <strong>bounded beings</strong> continuous beings.</span> The preference of aiming for “optimum” as bounded beings is metaphysical: it is believed that optimality relates to evidence of imperfection of the Creator’s wisdom.<label for="tufte-sn-367" class="margin-toggle sidenote-number">367</label><input type="checkbox" id="tufte-sn-367" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">367</span> The metaphysical roots of the <em>principle of extremes</em> (<em>principle of least action</em> in science and <em>principle of maximum utility</em> in social science) can date back to Aristotle’s statement: “Nature does nothing in vain.” It can be interpreted as follows. If there is a purpose in the nature of having imperfection, then it should lead to the disclosure of the perfect means. Later, <span class="citation">Newton (<a href="bibliography.html#ref-Newton1846">1846</a>)</span> wrote the following sentence as the first rule of his reasoning in Philosophy. “We are to admit no more causes of natural things than such as are both true and sufficient to explain their appearances.” <span class="citation">Newton (<a href="bibliography.html#ref-Newton1846">1846</a>)</span> commented on this rule by writing, “Nature is pleased with simplicity and affects not the pomp of superfluous causes.”</span></p>
<p>Our world is too complex, while our life-time, our intellectual scales, and our physical abilities are too limited compared to the orders of magnitudes of such complexity. We have neither the knowledge nor the computational facility required to formulate all the terms correctly in a system of equations for understanding the universe, the earth, the society, or even ourselves. On the other hand, the logical machinery, together with the axiomatizations of laws and orders, has established a closeted limited realm within which the procedures and subroutines of bounded beings make explicit the advantages of natural modularities and hierarchical organizations to achieve constructive triumphs.</p>
<p>The ideology descending from optimum approximation is <a href="ch-DE.html#sub:MecWorld">reductionism</a>. It allows us to move from an intractable situation with a full set of elements to an easy-going place in the absence of non-principal ones. Unlike the <strong>holism</strong> flavor wrapped in Leibniz’s <a href="ch-representation.html#sub:pseudoRandom">representation</a>, the approximating thing is imperfect so that the ontological identity no longer holds.<label for="tufte-sn-368" class="margin-toggle sidenote-number">368</label><input type="checkbox" id="tufte-sn-368" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">368</span> If we set the ontological identity as <span class="math display">\[f =\sum_{i=1}^{\infty} \langle f, \phi_i\rangle \phi_i,\]</span>
then the approximation is intended to reduce the synthesis to, say a combination of <span class="math inline">\(N\)</span> principal objects, <span class="math inline">\(\sum_{i=1}^{N} \langle f, \phi_i\rangle \phi_i\)</span> by hoping that the approximating function can still reflect the causal relation of <span class="math inline">\(f\)</span>.</span>
However, with the approximation, the remaining skeleton of the <a href="ch-representation.html#sub:pseudoRandom">representational problem</a> preserves conceptual relations, approaches to the original norms, and provides understandable cognitions and constitutions that can be optimally embodied and socialized by <strong>bounded beings</strong>.</p>
<div id="sub:appSys" class="section level2">
<h2>
<span class="header-section-number">15.1</span> Approximating Systems</h2>
<p>One guiding principle in model development is Occam’s razor: “keep it simple.” More precisely, the model should be no more complicated than is necessary to represent the principal features. The intention of approximation is to replace some complicated system with a new system, one that is simpler and easier to work with, at the price of some (hopefully small) difference between the two systems. The new system is called an <em>approximating system</em>.</p>
<p>There are two critical criteria in using the approximation as a razor: first, how much nicer (or simpler) is the approximating system? Second, how similar is the approximating system to the original (or the true) one? Of course, the answers to these two questions depend on the exact meanings of nicer and similarity, which vary according to the context. The analyses of giving precise measurements to these two criteria are tied to the notations of <a href="ch-vecMat.html#sub:vec">norms</a>.<label for="tufte-sn-369" class="margin-toggle sidenote-number">369</label><input type="checkbox" id="tufte-sn-369" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">369</span> Recall that in one of the simplest approximations, the <a href="sub-calculus.html#sub:Taylor">Taylor series</a>, if the function <span class="math inline">\(f:\mathbb{R}\rightarrow \mathbb{R}\)</span> is <span class="math inline">\(n\)</span>-th order differentiable, then we have an <span class="math inline">\(n\)</span>-th order <a href="sub-calculus.html#sub:Taylor">polynomial approximation</a> <span class="math inline">\(p_n(x)=\sum_{i=1}^{n} c_i(x -a)^{i}\)</span> where <span class="math inline">\(c_i\)</span> is the <span class="math inline">\(i\)</span>-th order differentiation of <span class="math inline">\(f(x)\)</span> evaluated at <span class="math inline">\(x=a\)</span>. This approximation is only good when the error, namely <span class="math inline">\(|f(x)-p_n(x)|\)</span>, is small. Here the <a href="ch-vecMat.html#sub:vec">norm</a>, namely the absolute value, gives a measurement of the error.</span></p>
<p>In the infinite-dimensional space, the <a href="ch-vecMat.html#sub:vec">norm</a> is a special functional mapping from a vector space to the non-negative <a href="sub-continuity.html#sub:completeness">real number field</a>, and it shares some similarities with the <strong>non-negative (real-valued) linear functionals</strong>. The following table makes the comparison.<label for="tufte-sn-370" class="margin-toggle sidenote-number">370</label><input type="checkbox" id="tufte-sn-370" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">370</span> If we replace the linearity in point 3 with the condition <span class="math inline">\(\mathbf{T}(f+g)\leq \mathbf{T}(f)+\mathbf{T}(g)\)</span>, the <strong>non-negative linear functional</strong> <span class="math inline">\(\mathbf{T}\)</span> is called the <em>sub-additive functional</em>.</span></p>
<table>
<colgroup>
<col width="30%">
<col width="38%">
<col width="30%">
</colgroup>
<thead><tr class="header">
<th align="center">Definition</th>
<th align="left"><a href="ch-vecMat.html#sub:vec">Norm</a></th>
<th align="left"><em>Non-negative linear functional</em></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center">1.</td>
<td align="left">Positive definiteness, non-degeneracy:</td>
<td align="left">Positive definiteness, non-degeneracy:</td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="left">
<span class="math inline">\(\|f\|\geq0\)</span> for all <span class="math inline">\(f\in\mathcal{V}\)</span>, and <span class="math inline">\(\|f\|=0\)</span> if and only if <span class="math inline">\(f=0\)</span>.</td>
<td align="left">
<span class="math inline">\(\mathbf{T}(f)\geq0\)</span> for all <span class="math inline">\(f\in\mathcal{V}\)</span>, and <span class="math inline">\(\mathbf{T}(f)=0\)</span> if and only if <span class="math inline">\(f=0\)</span>.</td>
</tr>
<tr class="odd">
<td align="center">2.</td>
<td align="left">Multiplicativity:</td>
<td align="left">Multiplicativity:</td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="left">
<span class="math inline">\(\|af\|=|a|\times\|f\|\)</span> for all <span class="math inline">\(f\in\mathcal{V}\)</span> and scalars <span class="math inline">\(a\)</span>.</td>
<td align="left">
<span class="math inline">\(|\mathbf{T}(af)|=|a|\times|\mathbf{T}(f)|\)</span> for all <span class="math inline">\(f\in\mathcal{V}\)</span>. and scalars <span class="math inline">\(a\)</span>.</td>
</tr>
<tr class="odd">
<td align="center">3.</td>
<td align="left">
<a href="sub-continuity.html#sub:continuousFunc">Triangle inequality</a>:</td>
<td align="left">Linearity:</td>
</tr>
<tr class="even">
<td align="center"></td>
<td align="left">
<span class="math inline">\(\|f+g\|\leq\|f\|+\|g\|\)</span> for <span class="math inline">\(f,g\in\mathcal{V}\)</span>.</td>
<td align="left">
<span class="math inline">\(\mathbf{T}(f+g)=\mathbf{T}(f)+\mathbf{T}(g)\)</span> for <span class="math inline">\(f,g\in\mathcal{V}\)</span>.</td>
</tr>
</tbody>
</table>
<p>In between the linear functionals and norms in the infinite-dimensional vector space lies the <a href="ch-vecMat.html#sub:vec">inner product</a> because the <a href="ch-vecMat.html#sub:vec">inner product</a> always gives rise to a norm <span class="math inline">\(\|f\|=\sqrt{\langle f, f\rangle}\)</span>, and always acquires the linear property in a <a href="ch-representation.html#sub:conjugacy">Hilbert space</a>, <span class="math inline">\(\mathbf{T}(f)= \langle f, \cdot \rangle\)</span>. The only difference between the <a href="ch-vecMat.html#sub:vec">norm</a> and the <a href="ch-vecMat.html#sub:vec">inner product</a> is that functional defined by a norm only focus on the distance (a real value) while the functional defined by an inner product focus on both the distance and angle (between two points).<label for="tufte-sn-371" class="margin-toggle sidenote-number">371</label><input type="checkbox" id="tufte-sn-371" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">371</span> Notice that the <a href="ch-representation.html#sub:conjugacy">orthogonality</a> (90 degree angle) defined by the <a href="ch-vecMat.html#sub:vec">inner product</a> cannot be specified only by the <a href="ch-vecMat.html#sub:vec">norms</a>. To see this, notice that the orthogonality is characterized by the <a href="ch-vecMat.html#sub:vec">generalized Pythagoras’ theorem</a>:
<span class="math display">\[\|f+g\|^{2}=\|f\|^2 +\|g\|^2.\]</span>
The equality holds for every orthogonal pair <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> from an <a href="ch-representation.html#sub:innerProd">inner product space</a>. But this equality is a special case of
<span class="math display">\[\begin{align*}
\|f+g\|^2 &amp;= \langle f+g, f+g \rangle\\
&amp;= \|f\|^2+\|g\|^2 + 2\langle f, g\rangle 
\end{align*}\]</span>
when <span class="math inline">\(\langle f, g\rangle=\|f\|\|g\|\cos90^{\circ}=0\)</span> (see @(eq:inner-2)). In other words, the definition of orthogonality makes use of the angle information contained in the inner product <span class="math inline">\(\langle \cdot, \cdot \rangle :\mathcal{V}\times\mathcal{V}\rightarrow \mathbb{R}\)</span>.</span> Thus, we can say that the vector space attached to the <a href="ch-vecMat.html#sub:vec">norm</a> structure requires less specifications than the <a href="ch-vecMat.html#sub:vec">inner product</a> structure, and thus the <a href="ch-vecMat.html#sub:vec">norm</a> structure is more general than the <a href="ch-vecMat.html#sub:vec">inner product</a> structure.</p>
<p>One important motivation for extending the attention from the inner product structure to the norm is that we can confront a “bigger” dual that is <a href="sub-continuity.html#sub:completeness">complete</a>. We discussed in chapter <a href="ch-representation.html#sub:dualBasis">14.4</a> that the <a href="ch-representation.html#sub:dualBasis">dual spaces</a> of the <a href="ch-representation.html#sub:conjugacy">Hilbert spaces</a> or the finite-dimensional <a href="ch-MatComp.html#sub:vecSpaces">vector spaces</a> preserve structures from the original ones. But in general, a <a href="#dualBasis">dual space</a> is “bigger” than its original space. A “bigger” often brings in additional complications. But if the <a href="ch-vecMat.html#sub:vec">vector space</a> is attached to the <a href="ch-vecMat.html#sub:vec">norm</a>, the “bigger” size allows its <a href="ch-representation.html#sub:dualBasis">dual space</a> to makes the <a href="sub-continuity.html#sub:completeness">completion</a> of itself.</p>
<ul>
<li>A vector space <span class="math inline">\(\mathcal{V}\)</span> endowed with a <a href="ch-vecMat.html#sub:vec">norm</a> on <span class="math inline">\(\mathcal{V}\)</span> is called the <em>normed vector space</em>, denoted by <span class="math inline">\((\mathcal{V},\|\cdot\|)\)</span>. The <em>dual normed space</em> <span class="math inline">\(\mathcal{V}^{*}\)</span> consists of all the continuous <a href="ch-MatComp.html#sub:vecSpaces">linear functionals</a> of the <strong>normed vector space</strong>. When the dual space <span class="math inline">\(\mathcal{V}^*\)</span> is equipped with the <em>supremum norm</em> or the <em>infimum norm</em><label for="tufte-sn-372" class="margin-toggle sidenote-number">372</label><input type="checkbox" id="tufte-sn-372" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">372</span> Note that definitions given in <a href="ch-optApp.html#eq:norm-2">(15.2)</a> and <a href="ch-optApp.html#eq:norm-4">(15.4)</a> are simply the normalized versions of <a href="ch-optApp.html#eq:norm-1">(15.1)</a> and <a href="ch-optApp.html#eq:norm-3">(15.3)</a>, respectively. The equivalence between <a href="ch-optApp.html#eq:norm-1">(15.1)</a> and <a href="ch-optApp.html#eq:norm-3">(15.3)</a> gives a <a href="ch-representation.html#sub:dualBasis">duality</a> that holds for the functions and their bounds. Such a duality is analogous to the one in the inner product space, i.e., the duality between the basis function and Fourier series coefficient. Examples will be given in sec[?]</span>
<span class="math display" id="eq:norm-4">\[
\begin{align}
\|\mathbf{T}\| &amp;= \sup \left\{ \frac{|\mathbf{T}(f)|}{\|f\|} \,:\,  f\in\mathcal{V},\ \right \} \tag{15.1}\\
&amp;= \sup_{ f\in\mathcal{V}}\frac{|\mathbf{T}(f)|}{\|f\|}=\sup_{ f\in\mathcal{V},\,  \|f\|=1}|\mathbf{T}(f)| \tag{15.2}\\
&amp;= \inf \left\{ M \,:\, \frac{|\mathbf{T}(f)|}{\|f\|}\leq M,\, f\in\mathcal{V} \right\} \tag{15.3}\\
&amp; = \inf_{ f\in\mathcal{V},\,  \|f\|=1, |\mathbf{T}(f)|\leq M} M \tag{15.4}
\end{align}\]</span>
for any <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a> <span class="math inline">\(\mathbf{T}\in\mathcal{V}^*\)</span>, then the <strong>dual normed space</strong> is a <em>complete normed space</em> (even if <span class="math inline">\(\mathcal{V}\)</span> is not complete). A <em>complete normed space</em> is also called the <em>Banach space</em>.</li>
</ul>
<p>An <a href="ch-representation.html#sub:innerProd">inner product space</a> is a particular kind of <strong>normed vector space</strong>. So the <a href="ch-representation.html#sub:conjugacy">Hilbert space</a> is a particular kind of <strong>Banach space</strong>.</p>
<div class="solution">
<p class="solution-begin">
Equivalence of the supremum norm and the infimum norm <span id="sol-start-88" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-88', 'sol-start-88')"></span>
</p>
<div id="sol-body-88" class="solution-body" style="display: none;">
<p>Let <span class="math inline">\(\sup_{f\in\mathcal{V},\,\|f\|=1}|\mathbf{T}(f)|=\overline{M}\)</span> and let <span class="math inline">\(\inf_{f\in\mathcal{V},\,\|f\|=1,|\mathbf{T}(f)|\leq M}M=\underline{M}\)</span>. We need to show <span class="math inline">\(\underline{M}=\overline{M}\)</span>. Suppose <span class="math inline">\(\underline{M}\neq\overline{M}\)</span>. So there is some <span class="math inline">\(\epsilon\in\mathbb{R}\)</span> such that <span class="math inline">\(\underline{M}+\epsilon=\overline{M}\)</span>.</p>
<p>Suppose <span class="math inline">\(\epsilon&gt;0\)</span>. The definition of the infimum norm
<span class="math display">\[\inf_{f\in\mathcal{V},\,\|f\|=1,|\mathbf{T}(f)|\leq M}M=\underline{M}\]</span> says that <span class="math inline">\(|\mathbf{T}(f)|\leq\underline{M}\)</span> for all <span class="math inline">\(\mathbf{T}\in\mathcal{V}^{*}\)</span>. Then for all <span class="math inline">\(\mathbf{T}\in\mathcal{V}^{*}\)</span>, there is
<span class="math display">\[|\mathbf{T}(f)|\leq\underline{M}\Leftrightarrow|\mathbf{T}(f)|+\epsilon\leq\underline{M}+\epsilon=\overline{M}\]</span>
which says that <span class="math inline">\(|\mathbf{T}(f)|\leq\overline{M}-\epsilon\)</span>. As <span class="math inline">\(\epsilon&gt;0\)</span>, we can say <span class="math inline">\(|\mathbf{T}(f)|\leq\overline{M}-\epsilon/2\)</span> for all <span class="math inline">\(\mathbf{T}\in\mathcal{V}^{*}\)</span>. Thus, we find another <a href="sub-continuity.html#sub:completeness">supremum</a> <span class="math display">\[\sup_{f\in\mathcal{V},\,\|f\|=1}|\mathbf{T}(f)|=\overline{M}-\epsilon/2\]</span> which contradicts with the definition of <a href="sub-continuity.html#sub:completeness">supremum</a>. So <span class="math inline">\(\epsilon\)</span> cannot be positive.</p>
<p>Suppose <span class="math inline">\(\epsilon&lt;0\)</span>. So <span class="math inline">\(\underline{M}=\overline{M}-\epsilon\)</span> implies <span class="math inline">\(\underline{M}&gt;\overline{M}\)</span>. It says that
<span class="math display">\[\inf\left\{ M\,:\,\frac{|\mathbf{T}(f)|}{\|f\|}\leq M,\, f\in\mathcal{V}\right\} &gt;\sup\left\{ \frac{|\mathbf{T}(f)|}{\|f\|}\,:\, f\in\mathcal{V},\ \right\}\]</span> an obvious contradiction. So <span class="math inline">\(\epsilon\)</span> cannot be negative.</p>
<p>We can conclude that <span class="math inline">\(\epsilon\)</span> has to be zero. The result follows.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>The <a href="sub-continuity.html#sub:completeness">supremum</a> and <a href="sub-continuity.html#sub:completeness">infimum</a> on the <a href="sub-continuity.html#sub:completeness">real number field</a> make sure that a continuous function over a bounded interval <span class="math inline">\([a,b]\)</span> always has its maximum and minimum. The <strong>supremum norm</strong> or the <strong>infimum norm</strong> offers a strict criterion for assessing the continuity.<label for="tufte-sn-373" class="margin-toggle sidenote-number">373</label><input type="checkbox" id="tufte-sn-373" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">373</span> We have seen in figure <a href="ch-representation.html#fig:stepFun">14.4</a> that a convergent sequence of continuous functions may not preserve the continuity under the <span class="math inline">\(l_2\)</span>-norm. Hence, the space of all continuous functions <span class="math inline">\(\mathcal{C}([a,b])\)</span> under the <span class="math inline">\(l_{2}\)</span>-norm is incomplete. But if we assess the continuity using the <strong>supremum norm</strong>, we can see that the functions in the convergent sequence are all discontinuous as
<span class="math display">\[\sup_{x\in [0,\pi]} f_n(x)= 1\, \\
\sup_{x\in [-\pi,0)} f_n(x)= 0\]</span>
for any <span class="math inline">\(n\in\mathbb{N}\)</span>. Also for the <strong>infimum norm</strong>, the discontinuity appears at <span class="math inline">\(1/n\)</span>, as for any <span class="math inline">\(n\in\mathbb{N}\)</span>
<span class="math display">\[\inf_{x\in [1/n,\pi]} f_n(x)= 1 \\
\inf_{x\in [-\pi,1/n)} f_n(x)= 0.\]</span> Thus, the function in figure <a href="ch-representation.html#fig:stepFun">14.4</a> should not belong to the space <span class="math inline">\(\mathcal{C}([a,b])\)</span> under the <strong>supremum norm</strong> or <strong>infimum norm</strong>. In fact, <span class="math inline">\(\mathcal{C}([a,b])\)</span> becomes a <a href="sub-continuity.html#sub:completeness">complete</a> <strong>normed space</strong>, namely a <strong>Banach space</strong>, under the <strong>supremum norm</strong> or <strong>infimum norm</strong>.</span> The <a href="sub-continuity.html#sub:completeness">completion</a> of the <strong>dual normed space</strong> is due to this strict criterion: if the functionals under the best scenario and the worst scenario are bounded/continuous according to the <strong>supremum norm</strong> or the <strong>infimum norm</strong>, all the other <a href="ch-MatComp.html#sub:vecSpaces">linear functionals</a> should also be bounded/continuous.</p>
<p>Note that the <strong>dual normed space</strong> is <a href="sub-continuity.html#sub:completeness">complete</a> even though the original <strong>normed space</strong> is not. This feature is useful. As many approximation problems cannot find an exact answer in the original setting, people have to switch to the <strong>dual normed space</strong> where the “completion” of the space guarantees the existence of the solution. For example, if
a system <span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{b}\)</span> consists of <span class="math inline">\(n\)</span> linear equations but only <span class="math inline">\(k\)</span> unknown variables (<span class="math inline">\(k&lt;n\)</span>). Then the system is an <a href="ch-MatComp.html#sub:GElimination">overdetermined system</a>. That is, no solution <span class="math inline">\(\mathbf{x}^*\)</span> exists for <span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{b}\)</span>. To resolve the problem, one needs to reformulate an <strong>approximating system</strong>. Suppose that the <strong>approximating system</strong> is to make the residual vector <span class="math inline">\(\mathbf{A}\mathbf{x}-\mathbf{x}\)</span> as “small” as possible, and this system is solvable under some functional operation. The solution, say <span class="math inline">\(\hat{\mathbf{x}}\)</span>, will not belong to the set containing the original object but some other set.<label for="tufte-sn-374" class="margin-toggle sidenote-number">374</label><input type="checkbox" id="tufte-sn-374" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">374</span> Let <span class="math inline">\(\mathbf{x}\)</span> belong to the vector space <span class="math inline">\(\mathbb{R}^{k}\)</span>. The solution vector <span class="math inline">\(\hat{\mathbf{x}}\)</span> usually is from the space <span class="math inline">\(\mathcal{L}(\mathbb{R}^{k},\mathbb{R}^{k})\)</span>, a set of all linear operators mapping from <span class="math inline">\(\mathbb{R}^{k}\)</span> to <span class="math inline">\(\mathbb{R}^{k}\)</span>. The linear operator <span class="math inline">\(\hat{\mathbf{x}}\)</span> consists of linear functionals <span class="math inline">\(\hat{x}_1, \dots, \hat{x}_k\)</span>. So the solutions <span class="math inline">\(\hat{x}_1, \dots, \hat{x}_k\)</span> are from in the <strong>dual normed space</strong>. In other words, each <span class="math inline">\(\hat{x}_i\)</span> of the solution vector is a functional belonging to the <strong>dual normed space</strong> rather than a scalar.</span>
In particular, if the solution set of the <strong>approximating system</strong> belongs to the <strong>dual normed vector space</strong>, we expect that the solution will always exist. So we call the solution of the <strong>approximating system</strong> the <em>approximation</em>.</p>
<p>To fulfill the goal of pushing the solution set to a <strong>Banach space</strong>, when we model an <strong>approximating system</strong>, we often consider the setup to be solvable by one continuous linear <a href="ch-MatComp.html#sub:vecSpaces">operator</a> or a sequence of them. Also, we hope that the norms of these <a href="ch-MatComp.html#sub:vecSpaces">operators</a> should approximate the unit value so that these <strong>approximations</strong> have the tendencies to preserve the <a href="ch-vecMat.html#sub:vec">norms</a> of the “solutions” in the original system.</p>
<p>Formally speaking, solving an <strong>approximating system</strong> depends on the <em>“hat” operator</em> <span class="math inline">\(\hat{(\cdot)}:\mathcal{V}\rightarrow\mathcal{V}^{*}\)</span>, namely <span class="math inline">\(\hat{(\cdot)} \in \mathcal{L}(\mathcal{V},\mathcal{V}^{*})\)</span> where <span class="math inline">\(\mathcal{L}(\mathcal{V},\mathcal{V}^{*})\)</span> denotes the set of all continuous linear mapping from <span class="math inline">\(\mathcal{V}\)</span> to <span class="math inline">\(\mathcal{V}^{*}\)</span>.<label for="tufte-sn-375" class="margin-toggle sidenote-number">375</label><input type="checkbox" id="tufte-sn-375" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">375</span> The <strong>hat operator</strong> emerged in several fields under different names. The successive series started in physics (quantum position measurement) around the 1920s, then it spread to operation research (optimizer), statistics and information theory (point estimator), economics and finance (pricing operator). Indeed, there is a hidden inter-connection among these subjects. All their intentions are trying to approximate the real-world phenomena (the quantum movements, the transportation flows, the observable samples or the receiving signals, and changes of commodities) with some analytical systems that are believed to be solvable (eigensystem, linear programming, likelihoods/entropies, and supply-demand models). The <strong>hat operator</strong> did a wonderful job as it can always provide a “solution” to any of these real-world problems. However, many people are apparently ignoring (or perhaps unaware) the fact that these “solutions” given by the dual, and blindly worship some of these operators as “the” solutions of the real phenomena.</span> On many occasions, we want the <strong>hat operator</strong> to preserve the norm such that <span class="math inline">\(\|\hat{f}\|=\|f\|\)</span> for every <span class="math inline">\(f\in\mathcal{V}\)</span>.</p>
<p>All <strong>approximations</strong> can be conducted in either one-step or multiple-steps. Let’s consider two representative examples.</p>
<p><span class="newthought">One-step approximation </span></p>
<p>The <strong>approximation</strong> <span class="math inline">\(\hat{\mathbf{x}}=(\mathbf{A}^{\top}\mathbf{A})^{-1}\mathbf{A}^{\top}\mathbf{b}\)</span> of the system <span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{b}\)</span> is a <strong>hat operator</strong>. It is also the solution of the following <strong>approximating system</strong>
of minimizing the residual <span class="math inline">\(\mathbf{A}\mathbf{x}-\mathbf{b}\)</span> in terms of the <span class="math inline">\(\ell_2\)</span>-norm:
<span class="math display">\[\min_{\mathbf{x}}(\mathbf{A}\mathbf{x}-\mathbf{b})^\top (\mathbf{A}\mathbf{x}-\mathbf{b}).\]</span>
To see this, the norm <span class="math inline">\((\mathbf{A}\mathbf{x}-\mathbf{b})^\top (\mathbf{A}\mathbf{x}-\mathbf{b})\)</span> is a quadratic term. By taking a second derivative of the norm with respect to <span class="math inline">\(\mathbf{x}\)</span>, we have <span class="math inline">\(\mathbf{A}^{\top}\mathbf{A}\geq0\)</span>. According to the optimality criterion given in chapter <a href="sub-calculus.html#sub:opt">7.3</a>, we know that any <span class="math inline">\(\mathbf{x}\)</span> that can attain zero for the first derivative of the norm will give the minimum of the norm. The first derivative is
<span class="math inline">\(2\mathbf{A}^\top (\mathbf{A}\mathbf{x}-\mathbf{b})=0 \Leftrightarrow \mathbf{A}^\top\mathbf{A}\mathbf{x}=\mathbf{A}^\top\mathbf{b}.\)</span>
By inverting <span class="math inline">\(\mathbf{A}^\top\mathbf{A}\)</span> on both sides, we have the solution <span class="math inline">\(\hat{\mathbf{x}}=(\mathbf{A}^{\top}\mathbf{A})^{-1}\mathbf{A}^{\top}\mathbf{b}\)</span>. Note that this solution exists even if <span class="math inline">\(\mathbf{A}\)</span> is not a square matrix (the number of unknowns and the number of equations do not match). That is to say, the solution <span class="math inline">\(\hat{\mathbf{x}}\)</span> may or may not be the solution of <span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{b}\)</span>.</p>
<p><span class="newthought">Sequential approximation </span></p>
<p>Consider a dynamical system of simultaneous equations:
<span class="math display">\[u_{t}(x_{j})=\sum_{s=1}^{n}p_{js}u_{t-1}(x_{s}),\, j=1,\dots n,\]</span>
where <span class="math inline">\(u_t(\cdot)\)</span> stands for an unknown function at time <span class="math inline">\(t\)</span>, <span class="math inline">\(x_{j}\)</span> is the state value of the input of the function, <span class="math inline">\(p_{js}\)</span> is from an <span class="math inline">\(n\times n\)</span>
<a href="ch-UnMulti.html#sub:Markov">probability transition matrix</a> <span class="math inline">\(\mathbf{P}=[p]_{js}\)</span>. The system can be written in matrix form <span class="math display">\[\mathbf{u}_t=\mathbf{P}\mathbf{u}_{t-1}.\]</span>
Because <span class="math inline">\(\mathbf{u}_t(\cdot)\)</span> is an unknown vector given by an unknown function, we need to approximate <span class="math inline">\(\mathbf{u}_t(\cdot)\)</span> by some tractable candidate. Let the <strong>approximation</strong> at <span class="math inline">\(t\)</span>-step be
<span class="math inline">\(\hat{\mathbf{u}}_t= [\langle \mathbf{c}_t, \phi(x_1)\rangle, \dots, \langle \mathbf{c}_t, \phi(x_n)\rangle]^\top\)</span>, where <span class="math inline">\(\phi(\cdot)\)</span> is the <a href="ch-representation.html#sub:innerProd">basis function</a>, and <span class="math inline">\(\mathbf{c}_t\)</span> is the coefficient vector. The <strong>approximating system</strong> is
<span class="math display">\[\begin{align*}
\mbox{at time }t: &amp; \,\, \hat{\mathbf{u}}_t= [\langle \hat{\mathbf{c}}_t, \phi(x_1)\rangle, \dots, \langle \hat{\mathbf{c}}_t, \phi(x_n)\rangle]^\top \\
\mbox{ where } \hat{\mathbf{c}}_t \mbox{ is from } &amp;\,\, \min_{\mathbf{c}_t} \| \hat{\mathbf{u}}_t - \mathbf{P}\hat{\mathbf{u}}_{t-1}\|\\
\mbox{at time }t+1: &amp; \, \, \hat{\mathbf{u}}_{t+1}= [\langle \hat{\mathbf{c}}_{t+1}, \phi(x_1)\rangle, \dots, \langle \hat{\mathbf{c}}_{t+1}, \phi(x_n)\rangle]\top \\
\mbox{ where } \hat{\mathbf{c}}_{t+1} \mbox{ is from } &amp;\,\, \min_{\mathbf{c}_{t+1}} \| \hat{\mathbf{u}}_{t+1}- \mathbf{P}\hat{\mathbf{u}}_{t}\|\\
\vdots &amp;
\end{align*}
\]</span>
Note that we have a sequence of <strong>hat operators</strong> <span class="math inline">\(\{\hat{\mathbf{u}}_1, \hat{\mathbf{u}}_2,\dots, \hat{\mathbf{u}}_t\}\)</span> for this system up to time <span class="math inline">\(t\)</span> rather than a single one.</p>
<p>By now, we know that if we can present the original system by an approximating one whose solution is defined on the <strong>dual normed space</strong>, the solution for the <strong>approximating system</strong> always exists. But we haven’t known yet how to specify a general routine to set up the <strong>approximating system</strong>.</p>
<p>Recall the previous routine of representing an object in the <a href="ch-representation.html#sub:innerProd">inner product space</a>. We project any object of the <a href="ch-representation.html#sub:innerProd">inner product space</a> onto a basis system that spans the space. The basis system is known; therefore, they are tractable for presenting the object. Similarly, an <strong>approximating system</strong> emerges when we project some object that belongs to an “unapproachable” set onto an “approachable” set. The minimizations in both above examples are actually the <strong>metric projection operators</strong> for the <strong>normed vector space</strong>.</p>
<p>For a <strong>normed vector space</strong> <span class="math inline">\((\mathcal{V},\|\cdot\|)\)</span>, the operator <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}(f)\)</span> is called <em>metric projection operator</em> if it yields the multi-valued operator between the point <span class="math inline">\(f\in\mathcal{V}\)</span> and the nearest point(s) <span class="math inline">\(\hat{f}\in\mathcal{Y}\subset\mathcal{V}\)</span>:
<span class="math display">\[\mathbf{P}_{\mathcal{Y}}(f)=\hat{f},\,\,\mbox{with }\,  \|f - \hat{f}\|=\inf_{g\in\mathcal{Y}}\|f-g\|\]</span>
where the set <span class="math inline">\(\mathcal{Y}\)</span> is projected set.</p>
<p>If we consider <span class="math inline">\(f\)</span> as the original system of interests, setting up an <strong>approximating system</strong> is equivalent to defining a <strong>metric projection operator</strong> of <span class="math inline">\(f\)</span> on the <a href="ch-UnMulti.html#sub:WLLN">a priori</a> selected set <span class="math inline">\(\mathcal{Y}\)</span> where the systems are all solvable or computable (or understandable) by human beings.<label for="tufte-sn-376" class="margin-toggle sidenote-number">376</label><input type="checkbox" id="tufte-sn-376" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">376</span> The projected set <span class="math inline">\(\mathcal{Y}\)</span> includes the <strong>approximating system</strong> but does not necessarily <span class="math inline">\(f\)</span>.</span></p>
<p>Notice that the <strong>metric projection operator</strong> is a multi-valued operator. The multi-valued operator allows us to project the original system onto multiple approximating ones.<label for="tufte-sn-377" class="margin-toggle sidenote-number">377</label><input type="checkbox" id="tufte-sn-377" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">377</span> There could be multiple <span class="math inline">\(\hat{f}_i\)</span> such that
<span class="math display">\[\inf_{g\in\mathcal{Y}}\|f-g\|=\\
\|f-\hat{f}_1\|=\|f-\hat{f}_2\|=\cdots\]</span>
That is to say, you can project <span class="math inline">\(f\)</span> to several directions, and multiple nearest points are corresponding to this projection. Thus the <strong>metric projection operator</strong> is <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}(f)=\{\hat{f}_{i}\}\)</span>.</span> But the cost of this generality is that it is ambiguous to proceed on which system. To avoid ambiguity, people often shift their attention to the set <span class="math inline">\(\mathcal{Y}\)</span> that can generate a single-valued <strong>metric projection operator</strong>, namely a unique nearest point from the projection:
<span class="math display">\[\mathbf{P}_{\mathcal{Y}}(f)=\hat{f},\,\,\mbox{with }\, \|f-\hat{f}\|=\min_{g\in\mathcal{Y}}\|f-g\|.\]</span>
A technical discussion about how to characterize <span class="math inline">\(\mathcal{Y}\)</span> to achieve this goal will be given later. For the moment, I should give a remark about the role of uniqueness. Looking for a unique optimum under the <a href="sub-calculus.html#sub:opt">maximization/minimization</a> rather than multiple optima under the <a href="sub-continuity.html#sub:completeness">supremum/infimum</a> somehow relates to the visions towards the world’s order. There are various arguments in favor of the unique optimal outcome.<label for="tufte-sn-378" class="margin-toggle sidenote-number">378</label><input type="checkbox" id="tufte-sn-378" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">378</span> When the light rays are refracted in a glass window, they bend and follow the path that minimizes their travel time. A solid body moves along the direction with the least resistance through a fluid. Soap films seek to minimize their surface area, and adopt a spherical shape. While the circle shape is also the one on which Queen Dido formed the longest loop in order to enclose the largest possible territory.</span> Amongst all the arguments, perhaps the most fundamental one comes from <em>monotheism</em>, where one can derive a unique logical limit of the universal order.</p>
<div class="solution">
<p class="solution-begin">
Descartes’ proposition of NOT MANY GODS <span id="sol-start-89" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-89', 'sol-start-89')"></span>
</p>
<div id="sol-body-89" class="solution-body" style="display: none;">
<p>Proposition V in <span class="citation">Spinoza (<a href="bibliography.html#ref-Spinoza1943">1905</a>)</span></p>
<blockquote>
<blockquote>
<p>“God’s existence is known merely from the consideration of his nature”</p>
</blockquote>
</blockquote>
<p>Proposition IX in <span class="citation">Spinoza (<a href="bibliography.html#ref-Spinoza1943">1905</a>)</span></p>
<blockquote>
<blockquote>
<p>“God is omniscient”</p>
</blockquote>
</blockquote>
<p>Proposition XI in <span class="citation">Spinoza (<a href="bibliography.html#ref-Spinoza1943">1905</a>)</span></p>
<blockquote>
<blockquote>
<p>“There are not many gods”</p>
</blockquote>
</blockquote>
<p>Here is the proof of proposition XI.</p>
<p>Suppose there are many gods, say A and B.</p>
<p>Then A and B are both omniscient (Proposition IX).</p>
<p>A is the cause of the idea of B. (Proposition V)</p>
<p>B is the cause of the idea of A. (Proposition V)</p>
<p>Therefore there will be some perfection in A that is not self-caused, and likewise with B.</p>
<p>Neither A nor B is omniscient, hence they cannot be gods.</p>
<p>Therefore, there is only one God.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
</div>
<div id="sub:Optimization" class="section level2">
<h2>
<span class="header-section-number">15.2</span> Optimization and Inequalities</h2>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:Cone"></span>
<img src="fig/Part4/cone.gif" alt="The union of infinite many nested balls gives a cone" width="100%"><!--
<p class="caption marginnote">-->Figure 15.1: The union of infinite many nested balls gives a cone<!--</p>-->
<!--</div>--></span>
</p>
<p>How to characterize the unique optimum? <a href="ch-optApp.html#sub:appSys">Monotheism</a> attaches a pyramid to the <a href="ch-UnMulti.html#sub:Markov">hierarchical chain of beings</a>. In this pyramid, this chain’s order follows a monotonic structure, and at the ultimate end, there sits one and only one being, the <a href="sub-inferknow.html#determinism">first cause</a>. This kind of pyramidal shape provides a primitive way of characterizing the uniqueness for the optimum. Different mythological symbols in several religions advertised such a characterization.
Moreover, the mythology tells us that the pyramid is far from static. The positions of the beings give them the incentives (called <em>potential energies</em> in physics) to triggers the movements within the chain. Behind these tendencies, there were forces and power (called <em>kinetic energies</em> in physics) that were actually functioning on shatter or re-order the pyramid. Consequently, ascending and descending transitions in some subsets of the pyramid happened to be the themes in various myths and religious stories. They provided variational melodies on the static staffs.<label for="tufte-sn-379" class="margin-toggle sidenote-number">379</label><input type="checkbox" id="tufte-sn-379" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">379</span> Also, they delivered several ideas/clues on how to preserve the principal order in the plays without losing attractive variations and on how to implant different seeds on the followers’ thoughts (so that the “prophets” in those sects were able to “forecast” the development of some projected movements in reality).</span></p>
<p>When we retreat to the mathematical aspect, the characterization of the unique optimality is about the (dynamical) utilization of the inequality to shape a <strong>conical</strong> type subset where things are in order. In a nutshell, a real-valued inequality provides a <strong>conical</strong> shape where the projection can attain one “peak.”<label for="tufte-sn-380" class="margin-toggle sidenote-number">380</label><input type="checkbox" id="tufte-sn-380" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">380</span> The inequalities of beings evolve under some functions or operations during the movements. In myths, most principal characters had to (at least pretended to) continuously adapt themselves to the movements so that their orders can be preserved in the storylines). The dynamical storylines preserved the (principal) subset of maintaining the chain of the inequality and pointing out the ascending/descending direction of the desired final stage.</span> This property turns out to be useful if we allow the <a href="ch-representation.html#sub:conjugacy">projection operator</a> to keep “stirring” the system (so that it can make iterative moves to approximate the object). Some special shape reserves the order, and the inequality serves as an engine force to guide the projections (and avoid them blowing up the iterations). This kind of idea was extensively studied in the <em>optimization theory</em>, a subject being largely concerned with the maximization or minimization of real-valued function(al)s over a given subset.<label for="tufte-sn-381" class="margin-toggle sidenote-number">381</label><input type="checkbox" id="tufte-sn-381" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">381</span> In 1920s, the military squad, Blackett’s Circus, applied the theory to military operations and developed a formal discipline now called operation research. Since then, the <strong>optimization theory</strong> has been applied to many other fields. For example, it helps economists to quantify the ideology about the <em>laissez-faire</em> by the physiocrats or the <em>invisible hand</em> by Adam Smith to describe the movements of achieving an “optimal” situation of some economic system based on the principle of maximizing individual <a href="sub-axioms.html#sub:zorn">happinesses</a> or <a href="sub-axioms.html#sub:zorn">utilities</a>.</span></p>
<p>In the <a href="sub-continuity.html#sub:completeness">real number field</a> <span class="math inline">\(\mathbb{R}\)</span>, one can easily attain the optimum (maximum or minimum), as <span class="math inline">\(\mathbb{R}\)</span> is an <a href="sub-set-theory.html#sub:order">ordered</a> <a href="ch-MatComp.html#sub:vecSpaces">field</a> where the inequality order is well defined. For example, if the minimum point in a set <span class="math inline">\(\mathcal{X}\subset \mathbb{R}\)</span> exists, then the inequality
<span class="math display">\[x^{*}\leq x \mbox{ for all } x\in\mathcal{X}\]</span> immediately defines the minimum <span class="math inline">\(x^{*}\)</span>.</p>
<p>Things become non-trivial for the vectors. When we compare the vector <span class="math inline">\([3,1]\)</span> with <span class="math inline">\([2,2]\)</span>, the <a href="sub-set-theory.html#sub:order">ordering axioms</a> given in chapter <a href="sub-set-theory.html#sub:order">2.3</a> are violated. The order <span class="math inline">\(3&gt;1\)</span> for the first entities in both vectors contradicts with the order <span class="math inline">\(1&lt;2\)</span> for the second ones. One resolution is to map the vectors onto the <a href="sub-continuity.html#sub:completeness">real number field</a> where the ordering structure re-emerges on the <a href="sub-set-theory.html#sub:func">image</a> of the mappings. For a function <span class="math inline">\(f:\mathbb{R}^k\rightarrow\mathbb{R}\)</span>, if the inequality <span class="math inline">\(f(\mathbf{x})\geq f(\mathbf{x}^{*})\)</span> holds for all <span class="math inline">\(\mathbf{x}\in \mathbb{R}^{k}\)</span>, then we can say that <span class="math inline">\(x^{*}\)</span> gives the minimum value of the function <span class="math inline">\(f\)</span>.<label for="tufte-sn-382" class="margin-toggle sidenote-number">382</label><input type="checkbox" id="tufte-sn-382" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">382</span> There are infinitely many choices for selecting such a function <span class="math inline">\(f\)</span>. For example, by <a href="ch-vecMat.html#sub:vec"><span class="math inline">\(l_2\)</span>-norm</a>, <span class="math inline">\([3,1]\)</span> is <span class="math inline">\(\sqrt{10}\)</span>, and <span class="math inline">\([2,2]\)</span> is <span class="math inline">\(\sqrt{8}\)</span>. Thus one can order <span class="math inline">\([3,1]\)</span> and <span class="math inline">\([2,2]\)</span> by this norm (the size of a vector).</span></p>
<p>But the inequality <span class="math inline">\(f(\mathbf{x})\geq f(\mathbf{x}^{*})\)</span> doesn’t give any information about the “order” around the point <span class="math inline">\(\mathbf{x}^{*}\)</span>. More specifically, it doesn’t tell what kind of <a href="sub-continuity.html#sub:continuousFunc">neighborhood</a> of <span class="math inline">\(\mathbf{x}^{*}\)</span> can endow the optimal <span class="math inline">\(f(\mathbf{x}^{*})\)</span>. We need an informative inequality that can reveal some specific <a href="sub-continuity.html#sub:continuousFunc">topological structure</a> around the vector <span class="math inline">\(\mathbf{x}^{*}\)</span>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:PointLine"></span>
<img src="fig/Part4/PointLine.png" alt="A curve and its tangent lines " width="100%"><!--
<p class="caption marginnote">-->Figure 15.2: A curve and its tangent lines <!--</p>-->
<!--</div>--></span>
</p>
<p>Figure <a href="ch-optApp.html#fig:PointLine">15.2</a> reveals an interesting “duality” between points (of a curve) and their tangent lines. The shape of the curve can be specified not only by the function, but also by the collection of derivatives of this function. Recall that derivatives are about local behaviors. Given that the optimality of the function induces the inequality <span class="math inline">\(f(\mathbf{x})\geq f(\mathbf{x}^{*})\)</span>, can this “duality” disclose another inequality that makes use of the local information?</p>
<ul>
<li>
<strong>Convexity</strong> and <strong>variational inequality</strong> : Consider a set <span class="math inline">\(\mathcal{Y}\)</span>. Assume that <span class="math inline">\(\mathbf{y}_i,\mathbf{y}_j\in\mathcal{Y}\)</span> and <span class="math inline">\(\alpha \in [0,1]\)</span>. If
<span class="math display">\[\alpha \mathbf{y}_i + (1-\alpha)\mathbf{y}_j \in \mathcal{Y},\]</span>
then the set <span class="math inline">\(\mathcal{Y}\)</span> is called a <em>convex set</em>. In a finite-dimensional vector space <span class="math inline">\(\mathcal{V}=\mathbb{R}^{k}\)</span>, suppose that <span class="math inline">\(f:\mathcal{Y}\rightarrow \mathbb{R}\)</span> is <a href="sub-calculus.html#sub:diffInt">differentiable</a> on the <strong>convex set</strong> <span class="math inline">\(\mathcal{Y}\subset\mathcal{V}\)</span>, and the <a href="sub-continuity.html#sub:completeness">minimum</a> exists such that<label for="tufte-sn-383" class="margin-toggle sidenote-number">383</label><input type="checkbox" id="tufte-sn-383" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">383</span> Or say <span class="math display">\[\mathbf{y}^{*} = \arg\min_{\mathbf{y}\in\mathcal{Y}} f(\mathbf{y}),\]</span> where <span class="math inline">\(\arg\)</span> stands for the argument of the function.</span>
<span class="math display">\[f(\mathbf{y}^{*}) = \min_{\mathbf{y}\in\mathcal{Y}} f(\mathbf{y}),\]</span>
then we have the <em>variational inequality</em><label for="tufte-sn-384" class="margin-toggle sidenote-number">384</label><input type="checkbox" id="tufte-sn-384" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">384</span> Or say <span class="math display">\[\langle \nabla f(\mathbf{y}^{*}),\:\mathbf{y}^{*}\rangle \leq \langle \nabla f(\mathbf{y}^{*}),\:\mathbf{y}\rangle.\]</span></span>
<span class="math display">\[\left\langle \nabla f(\mathbf{y}^{*}),\:\mathbf{y} - \mathbf{y}^{*} \right\rangle \geq 0\]</span>
where <span class="math inline">\(\nabla f(\mathbf{y}^{*})\)</span> is the <a href="ch-vecMat.html#sub:linearity">gradient</a> of <span class="math inline">\(f\)</span> evaluated at <span class="math inline">\(\mathbf{y}^{*}\)</span>.</li>
</ul>
<div class="solution">
<p class="solution-begin">
Variational inequality <span id="sol-start-90" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-90', 'sol-start-90')"></span>
</p>
<div id="sol-body-90" class="solution-body" style="display: none;">
<p>To derive the <strong>variational inequality</strong>, let’s consider any <span class="math inline">\(\mathbf{y}\in\mathcal{Y}\)</span> around the minimum <span class="math inline">\(\mathbf{y}^{*}\)</span>.
The <strong>convexity</strong> tells that
<span class="math display">\[(1-\epsilon)\mathbf{y}^{*}+\epsilon\mathbf{y}=\mathbf{y}^{*}+\epsilon(\mathbf{y}-\mathbf{y}^{*})\in\mathcal{Y}\]</span>
for any small <span class="math inline">\(\epsilon&gt;0\)</span>. The minimum condition tells that
<span class="math display">\[f\left(\mathbf{y}^{*}+\epsilon(\mathbf{y}-\mathbf{y}^{*})\right)\geq f(\mathbf{y}^{*}).\]</span>
Let’s only consider <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{y}^{*}\)</span> in the <span class="math inline">\(i\)</span>-th entry. We know that
<span class="math display">\[\left[\frac{f\left(\mathbf{y}^{*}+\epsilon(\mathbf{y}-\mathbf{y}^{*})\right)-f(\mathbf{y}^{*})}{\epsilon(y_i-y_i^{*})}\right](y_i-y_i^{*})\geq0,\]</span>
By taking <span class="math inline">\(\epsilon\)</span> to zero, we have
<span class="math display">\[\lim_{\epsilon \rightarrow 0} \left[\frac{f\left(\mathbf{y}^{*}+\epsilon(\mathbf{y}-\mathbf{y}^{*})\right)-f(\mathbf{y}^{*})}{\epsilon(y_i-y_i^{*})}\right] = \frac{\partial
  f(\mathbf{y}^{*})}{\partial y_i}.\]</span>
We can apply the same argument to the rest entries. Then we have the inequality
<span class="math display">\[\left\langle \nabla f(\mathbf{y}^{*}),\:\mathbf{y} - \mathbf{y}^{*} \right\rangle \geq 0.\]</span></p>
<p>The result follows.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>The inequality set up an additional <a href="ch-DE.html#sub:ode">equilibrium</a> condition regarding the optimum.
The gradient <span class="math inline">\(\nabla f(\mathbf{x}^{*})\)</span> is about the effect of an <a href="sub-calculus.html#sub:diffInt">infinitesimal change</a> of <span class="math inline">\(f(\mathbf{x})\)</span> around <span class="math inline">\(\mathbf{x}^{*}\)</span>. The inequality of this <a href="sub-calculus.html#sub:diffInt">infinitesimal change</a> <span class="math inline">\(\langle \nabla f(\mathbf{y}^{*}),\:\mathbf{y}^{*}\rangle \leq \langle \nabla f(\mathbf{y}^{*}),\:\mathbf{y}\rangle\)</span> points out the direction on <span class="math inline">\(\mathcal{Y}\)</span> by which the linear function <span class="math inline">\(\langle \nabla f(\mathbf{y}^{*}),\:\cdot \rangle\)</span> can move towards its minimum.</p>
<p>The <strong>variational inequality</strong> also provides a standard way of constructing <a href="sub-continuity.html#sub:Cauchy">fixed point iterations</a>. By modifying the inequality a little bit, we have
<span class="math display">\[\left\langle \left(\mathbf{y}^{*}- \epsilon\nabla f(\mathbf{y}^{*}) \right)- \mathbf{y}^{*},\:\mathbf{y} - \mathbf{y}^{*} \right\rangle \leq 0\]</span>
for any arbitrary positive real number <span class="math inline">\(\epsilon\)</span>. The inequality implies the following <a href="sub-continuity.html#sub:Cauchy">fixed point</a> condition<label for="tufte-sn-385" class="margin-toggle sidenote-number">385</label><input type="checkbox" id="tufte-sn-385" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">385</span> The gradient <span class="math inline">\(\nabla f(\cdot)\)</span> is a <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a>, so it is not necessarily in <span class="math inline">\(\mathcal{Y}\)</span>. But given that <span class="math inline">\(\epsilon\)</span> is small, we can replace <span class="math inline">\(\mathbf{y}^{*}- \epsilon\nabla f(\mathbf{y}^{*})\)</span> with a projection of <span class="math inline">\(\mathbf{y}^{*}- \epsilon\nabla f(\mathbf{y}^{*})\)</span> onto <span class="math inline">\(\mathcal{Y}\)</span>, i.e. <span class="math inline">\(\hat{\mathbf{y}}=\mathbf{P}_{\mathcal{Y}}(\mathbf{y}^{*}- \epsilon\nabla f(\mathbf{y}^{*}) ) \in \mathcal{Y}\)</span> so that
<span class="math display">\[\left\langle  \hat{\mathbf{y}} - \mathbf{y}^{*},\:\mathbf{y} - \mathbf{y}^{*} \right\rangle \leq 0.\]</span>
Since the above inequality holds for any <span class="math inline">\(\mathbf{y}\in\mathcal{Y}\)</span> and <span class="math inline">\(\hat{\mathbf{y}}\in\mathcal{Y}\)</span>, we may have
<span class="math display">\[\left\langle \hat{\mathbf{y}} - \mathbf{y}^{*},\:\hat{\mathbf{y}} - \mathbf{y}^{*} \right\rangle \leq 0\]</span>
that is only true when <span class="math inline">\(\hat{\mathbf{y}} = \mathbf{y}^{*}\)</span>.</span>
<span class="math display">\[\mathbf{y}^{*}=\mathbf{P}_{\mathcal{Y}}(\mathbf{y}^{*}- \epsilon\nabla f(\mathbf{y}^{*}) )\]</span> where <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}(\cdot)\)</span> is the <a href="ch-optApp.html#sub:appSys">metric projection</a> onto the subset <span class="math inline">\(\mathcal{Y}\)</span>.
The iteration of this fixed point condition can be written as <span class="math display">\[\mathbf{y}_{n+1}=\mathbf{P}_{\mathcal{Y}}(\mathbf{y}_n- \epsilon\nabla f(\mathbf{y}_n) ).\]</span></p>
<p>In fact, building up a <a href="ch-optApp.html#sub:appSys">metric projection operator</a> on the <strong>convex set</strong> and setting up a <strong>variational inequality</strong> are two sides of one coin. To grab some intuition of this statement, note that the <a href="ch-optApp.html#sub:appSys">metric projection</a> of some element in <span class="math inline">\(\mathcal{V}\)</span> onto the subset <span class="math inline">\(\mathcal{Y}\subset \mathcal{V}\)</span> defines the inequality of <span class="math inline">\(\hat{f}\in\mathcal{Y}\)</span> such that <span class="math display">\[\|f-\hat{f}\|\leq\|f-g\|\quad\mbox{for any }g\in\mathcal{Y}.\]</span>
Meanwhile, the <a href="ch-vecMat.html#sub:vec">norm</a> itself is a function(al) whose <a href="sub-set-theory.html#sub:func">image</a> is the <a href="sub-continuity.html#sub:completeness">real number field</a>. Thus, the <strong>variational inequality</strong> may come out as a “dual” effect of some “differentiation” of the <a href="ch-vecMat.html#sub:vec">norm</a>.</p>
<p>Here are some technical details. First of all, notice that if <span class="math inline">\(\mathcal{V}\)</span> is a real-valued <a href="ch-representation.html#sub:conjugacy">Hilbert space</a><label for="tufte-sn-386" class="margin-toggle sidenote-number">386</label><input type="checkbox" id="tufte-sn-386" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">386</span> To attain this inequality, we need to explicitly restrict our attention to the real-valued inner product, as the complex-valued inner product distorts the ordering structure.</span> and <span class="math inline">\(\mathcal{Y}\subset\mathcal{V}\)</span>, then the <a href="ch-optApp.html#sub:appSys">metric projection</a> (of the subset <span class="math inline">\(\mathcal{Y}\)</span>) <span class="math display">\[\mathbf{P}_{\mathcal{Y}}(f)=\arg\min_{g\in\mathcal{Y}}\|f-g\|, \mbox{ for } f\in\mathcal{V}\]</span> shares the same property of the <a href="ch-representation.html#sub:conjugacy">projection operator</a> defined for the whole <a href="ch-representation.html#sub:conjugacy">Hilbert space</a> <span class="math inline">\(\mathcal{V}\)</span> (<span class="math inline">\(\mathbf{P}_{\mathcal{V}}\)</span> in chapter <a href="ch-representation.html#sub:conjugacy">14.3</a>), namely <span class="math display">\[\mathbf{P}_{\mathcal{Y}}\mathbf{P}_{\mathcal{Y}}=\mathbf{P}_{\mathcal{Y}}.\]</span>
So the projection of some object onto a subset in the Hilbert space is about finding the minimum distance between the object and this subset.<label for="tufte-sn-387" class="margin-toggle sidenote-number">387</label><input type="checkbox" id="tufte-sn-387" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">387</span> If <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}\mathbf{P}_{\mathcal{Y}}\neq \mathbf{P}_{\mathcal{Y}}\)</span>, there would be more than one value for a single projection. It contradicts the uniqueness requirement for the minimum operation. Assume that <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}f=g_1^{*}\)</span> and <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}\mathbf{P}_{\mathcal{Y}}f=g_2^{*}\)</span> are two different projection points. Then there would be
<span class="math display">\[\|f-g_1^*\| \neq \|f-g^*_2\|.\]</span>
We have either
<span class="math display">\[\|f-g_1^*\| &gt; \|f-g^*_2\|\\ \mbox{or } \|f-g_1^*\| &lt; \|f-g^*_2\|\]</span> If <span class="math inline">\(\|f-g_1^*\| &gt; \|f-g^*_2\|\)</span>, then <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}(f)=g_2^{*}\)</span> contradicts with <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}(f)=g_1^{*}\)</span>. A similar argument holds
for the other case.</span> The <a href="ch-optApp.html#sub:appSys">metric projection</a> is also a <a href="ch-optApp.html#sub:appSys">hat operator</a> because it preserves the norm of the projected object (<span class="math inline">\(\|\mathbf{P}_{\mathcal{Y}}\|=1\)</span>).</p>
<p>Secondly, we want to see a minimum in the real-valued <a href="ch-representation.html#sub:conjugacy">Hilbert space</a> can be obtained by a <strong>variational inequality</strong> if the neighborhood of the minimum is convex. For a <strong>convex</strong> subset <span class="math inline">\(\mathcal{Y}\)</span> of the real-valued <a href="ch-representation.html#sub:conjugacy">Hilbert space</a> <span class="math inline">\(\mathcal{V}\)</span>, let <span class="math inline">\(f\in\mathcal{V}\)</span> and <span class="math inline">\(g\in\mathcal{Y}\)</span>, and let <span class="math inline">\(\hat{f}=\mathbf{P}_{\mathcal{Y}}(f)\in\mathcal{Y}\)</span> be the <a href="ch-representation.html#sub:conjugacy">projection operator</a>, then the following two definitions are equivalent:</p>
<ol style="list-style-type: decimal">
<li><p><a href="ch-optApp.html#sub:appSys">metric projection operator</a>: <span class="math inline">\(\hat{f}=\mathbf{P}_{\mathcal{Y}}(f)=\arg\min_{g\in\mathcal{Y}}\|f-g\|\)</span>.</p></li>
<li><p><strong>variational inequality</strong>: <span class="math inline">\(\langle f-\hat{f},\, \hat{f}-g\rangle\geq0\)</span> or say <span class="math inline">\(\langle f-\hat{f},\, \hat{f}\rangle\geq \langle f-\hat{f},\, g\rangle\)</span></p></li>
</ol>
<div class="solution">
<p class="solution-begin">
Proof <span id="sol-start-91" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-91', 'sol-start-91')"></span>
</p>
<div id="sol-body-91" class="solution-body" style="display: none;">
<p><span class="newthought">Equivalence of 1. and 2. </span></p>
<p>From 2. to 1.</p>
<p>The inequality <span class="math inline">\(\langle g- \hat{f},\, f-\hat{f}\rangle\leq0\)</span> holds for <span class="math inline">\(g\in\mathcal{Y}\)</span>, then
<span class="math display">\[\begin{align*}
\|f-\hat{f}\|^{2}&amp;=\langle f-\hat{f},\: f-\hat{f}\rangle\\
&amp;=\langle f-g,\:f-\hat{f} \rangle+\langle g-\hat{f},\: f-\hat{f}\rangle\\
&amp;\leq\langle f-g,\: f-\hat{f} \rangle\leq\|f-g\|\|f-\hat{f}\|.
\end{align*}
\]</span>
Hence <span class="math inline">\(\|f-\hat{f}\|\leq\|f-g\|\)</span>, so <span class="math inline">\(\hat{f}=\mathbf{P}_{\mathcal{Y}}(f)=\min_{g\in\mathcal{Y}}\|f-g\|\)</span>.</p>
<p>From 1. to 2.</p>
<p>Suppose the inequality fails, namely <span class="math inline">\(\langle g- \hat{f},\, f-\hat{f}\rangle &gt; 0\)</span> for some <span class="math inline">\(g\)</span>. Let <span class="math inline">\(g_{\alpha}=\alpha g+(1-\alpha)\hat{f}\)</span>. If <span class="math inline">\(0&lt;\alpha&lt;1\)</span>, then the <strong>convexity</strong> tells that <span class="math inline">\(g_{\alpha}\in\mathcal{Y}\)</span>
and
<span class="math display">\[\begin{align*}
\|f-g_{\alpha}\|^{2} &amp;=\langle f-g_{\alpha},\: f-g_{\alpha}\rangle\\
&amp;=\langle f-\alpha g-(1-\alpha)\hat{f},\: f-\alpha g-(1-\alpha)\hat{f}\rangle\\
&amp;=\|f-\hat{f}\|^{2} - 2\alpha\langle g-\hat{f},\: f-\hat{f}\rangle+\alpha^{2}\|g-\hat{f}\|^{2}
\end{align*}\]</span>
Note that <span class="math inline">\(\langle g- \hat{f},\, f-\hat{f}\rangle &gt; 0\)</span> for some <span class="math inline">\(g\)</span>. So the term <span class="math display">\[-2\alpha\langle g-\hat{f},\: f-\hat{f}\rangle+\alpha^{2}\|g-\hat{f}\|^{2}\]</span> can be negative if <span class="math inline">\(\alpha\)</span> is sufficiently small, e.g. <span class="math display">\[0&lt;\lambda&lt;\frac{2\langle g-\hat{f},\: f-\hat{f}\rangle}{\|g-\hat{f}\|^{2}}.\]</span> In this case, we have <span class="math inline">\(\|f-\hat{f}\|&gt;\|f-g_{\alpha}\|\)</span> hence <span class="math inline">\(\hat{f}\)</span> cannot be <span class="math inline">\(\min_{g\in\mathcal{Y}}\|f-g\|\)</span>, namely <span class="math inline">\(\hat{f}\neq\mathbf{P}_{\mathcal{Y}}(f)\)</span>.</p>
<p><span class="newthought">Derivation of the variational inequality </span></p>
<p>Consider <span class="math inline">\(\|f-g\|^2\)</span> that has a quadratic form. Let <span class="math inline">\(f^{*}\)</span> be the minimum.
The (functional) gradient evaluated at <span class="math inline">\(f^{*}\)</span> is <span class="math inline">\(2(f^{*}-g)\)</span>. the <strong>variational inequality</strong> says
<span class="math display">\[\left\langle f^{*}-g ,\:f - f^{*} \right\rangle = \left\langle f - f^{*} ,\: f^{*} - g \right\rangle  \geq 0 \\
\mbox{or } \left\langle g - f^{*} ,\:f - f^{*} \right\rangle \leq 0\]</span></p>
<p>The inequality also tells that the minimum is unique. Suppose there are two minima <span class="math inline">\(\mathbf{f}_1\)</span> and <span class="math inline">\(\mathbf{f}_2\)</span>.
The inequality implies
<span class="math inline">\(\left\langle g-f_1^{*} ,\:f^{*}_2 - f_1^{*} \right\rangle\)</span> and
<span class="math inline">\(\left\langle g-f_2^{*} ,\:f^{*}_1 - f_2^{*} \right\rangle\)</span>.
Adding up two inequalities, we have
<span class="math display">\[\left\langle f^{*}_1-f_2^{*} ,\:f^{*}_1 - f_2^{*} \right\rangle = \|f^{*}_1-f^{*}_2\|\leq 0\]</span>
which only holds at <span class="math inline">\(f^{*}_1=f^{*}_2\)</span>.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>The above statement deduces that the <a href="ch-optApp.html#sub:appSys">metric projection operator</a> is <a href="sub-continuity.html#sub:Cauchy">Lipschitz continuous</a> with the <a href="sub-continuity.html#sub:Cauchy">Lipschitz constant</a> of one, called <em>non-expansive</em>:<label for="tufte-sn-388" class="margin-toggle sidenote-number">388</label><input type="checkbox" id="tufte-sn-388" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">388</span> Also, the <a href="ch-optApp.html#sub:appSys">metric projection operator</a> is <a href="sub-continuity.html#sub:completeness">monotone</a>, in the sense that
<span class="math display">\[\langle f_1-f_2, \mathbf{P}_{\mathcal{Y}}(f_1)- \mathbf{P}_{\mathcal{Y}}(f_2) \rangle\geq 0\]</span>
for all <span class="math inline">\(f_1,f_2\in\mathcal{V}\)</span>. The monotonicity means if <span class="math inline">\(f_1(\mathbf{x})\geq f_2(\mathbf{x})\)</span>, the inequality holds under the projection, namely
<span class="math inline">\(\mathbf{P}_{\mathcal{Y}}(f_1)(\mathbf{x}) \geq \mathbf{P}_{\mathcal{Y}}(f_2)(\mathbf{x})\)</span>.</span>
<span class="math display">\[\| \mathbf{P}_{\mathcal{Y}}(f_1)- \mathbf{P}_{\mathcal{Y}}(f_2) \|\leq \|f_1 -f_2\|\]</span>
for all <span class="math inline">\(f_1,f_2\in\mathcal{V}\)</span>.
This result serves as the foundation in many practical approximation exercises. As in practice, instead of approximating the “ideal” element <span class="math inline">\(f\)</span>, one often approximates the element <span class="math inline">\(f+\delta f\)</span>, where <span class="math inline">\(\delta f\)</span> is some “variation of <span class="math inline">\(f\)</span>.” The <strong>non-expansivity</strong> says that
<span class="math display">\[\| \mathbf{P}_{\mathcal{Y}}(f + \delta f)- \mathbf{P}_{\mathcal{Y}}(f) \|\leq \|\delta f\|\]</span> so that the error incurred in the approximation/projection is no larger than the error in the original variation. This is a desirable property for recursively approximating the object. Because in each step of the iteration, the projection will not blow up the whole scheme.</p>
<div class="solution">
<p class="solution-begin">
Monotonicity and non-expansivity <span id="sol-start-92" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-92', 'sol-start-92')"></span>
</p>
<div id="sol-body-92" class="solution-body" style="display: none;">
<p>The <strong>variational inequality</strong> tells that
<span class="math display">\[\langle f_{1}-\mathbf{P}_{\mathcal{Y}}(f_{1}),\mathbf{P}_{\mathcal{Y}}(f_{1})-g\rangle\geq0\;\mbox{for any }g\in\mathcal{Y}.\]</span>
By choosing <span class="math inline">\(g=\mathbf{P}_{\mathcal{Y}}(f_{2})\)</span>, we have <span class="math display">\[\langle f_{1}-\mathbf{P}_{\mathcal{Y}}(f_{1}),\mathbf{P}_{\mathcal{Y}}(f_{1})-\mathbf{P}_{\mathcal{Y}}(f_{2})\rangle\geq0.\]</span>
By the same argument, we have
<span class="math display">\[\langle\mathbf{P}_{\mathcal{Y}}(f_{1})-f_{2},\mathbf{P}_{\mathcal{Y}}(f_{1})-\mathbf{P}_{\mathcal{Y}}(f_{2})\rangle\geq0.\]</span></p>
<p>Now we consider
<span class="math display">\[\begin{align*}
\langle f_{1}-f_{2},\mathbf{P}_{\mathcal{Y}}(f_{1})-\mathbf{P}_{\mathcal{Y}}(f_{2})\rangle  &amp;=\langle f_{1}-\mathbf{P}_{\mathcal{Y}}(f_{1}),\mathbf{P}_{\mathcal{Y}}(f_{1})-\mathbf{P}_{\mathcal{Y}}(f_{2})\rangle\\
&amp;   +\langle\mathbf{P}_{\mathcal{Y}}(f_{1})-\mathbf{P}_{\mathcal{Y}}(f_{2}),\mathbf{P}_{\mathcal{Y}}(f_{1})-\mathbf{P}_{\mathcal{Y}}(f_{2})\rangle\\
&amp;   +\langle\mathbf{P}_{\mathcal{Y}}(f_{1})-f_{2},\mathbf{P}_{\mathcal{Y}}(f_{1})-\mathbf{P}_{\mathcal{Y}}(f_{2})\rangle.
\end{align*}
\]</span>
We know that the first and the third terms on the right are non-negative. The second term is <span class="math display">\[\|\mathbf{P}_{\mathcal{Y}}(f_{1})-\mathbf{P}_{\mathcal{Y}}(f_{2})\|^{2}\]</span>
that is always non-negative. Thus the result of monotonicity follows.</p>
<p>Using the monotonicity result, we have
<span class="math display">\[
\begin{align*}
\|f_{1}-f_{2}\|^{2} &amp;=  \|f_{1}-\mathbf{P}_{\mathcal{Y}}(f_{1})+\mathbf{P}_{\mathcal{Y}}(f_{1})-\mathbf{P}_{\mathcal{Y}}(f_{2})+\mathbf{P}_{\mathcal{Y}}(f_{2})-f_{2}\|^{2}\\
&amp;=  \|\mathbf{P}_{\mathcal{Y}}(f_{1})-\mathbf{P}_{\mathcal{Y}}(f_{2})\|^{2}+\left\Vert (f_{1}-\mathbf{P}_{\mathcal{Y}}(f_{1}))-(f_{2}-\mathbf{P}_{\mathcal{Y}}(f_{2}))\right\Vert ^{2}\\
&amp;   +2\left\langle \mathbf{P}_{\mathcal{Y}}(f_{1})-\mathbf{P}_{\mathcal{Y}}(f_{2}),\: f_{1}-\mathbf{P}_{\mathcal{Y}}(f_{1})-\left(f_{2}-\mathbf{P}_{\mathcal{Y}}(f_{2})\right)\right\rangle \\
&amp;=  \|\mathbf{P}_{\mathcal{Y}}(f_{1})-\mathbf{P}_{\mathcal{Y}}(f_{2})\|^{2}+\left\Vert (f_{1}-\mathbf{P}_{\mathcal{Y}}(f_{1}))-(f_{2}-\mathbf{P}_{\mathcal{Y}}(f_{2}))\right\Vert ^{2}\\
&amp;   +2\left\langle \mathbf{P}_{\mathcal{Y}}(f_{1})-\mathbf{P}_{\mathcal{Y}}(f_{2}),\: f_{1}-f_{2}\right\rangle -2\|\mathbf{P}_{\mathcal{Y}}(f_{1})-\mathbf{P}_{\mathcal{Y}}(f_{2})\|^{2}\\
&amp;\geq   \|\mathbf{P}_{\mathcal{Y}}(f_{1})-\mathbf{P}_{\mathcal{Y}}(f_{2})\|^{2}+\left\Vert (f_{1}-\mathbf{P}_{\mathcal{Y}}(f_{1}))-(f_{2}-\mathbf{P}_{\mathcal{Y}}(f_{2}))\right\Vert ^{2}.
\end{align*}
\]</span>
where the inequality comes from
<span class="math display">\[\left\langle \mathbf{P}_{\mathcal{Y}}(f_{1})-\mathbf{P}_{\mathcal{Y}}(f_{2}),\: f_{1}-f_{2}\right\rangle \ge\|\mathbf{P}_{\mathcal{Y}}(f_{1})-\mathbf{P}_{\mathcal{Y}}(f_{2})\|^{2}.\]</span>
So the result follows.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>In particular, if the projection is orthogonal, namely <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}\)</span> is <a href="ch-representation.html#sub:conjugacy">self-adjoint</a>:
<span class="math display">\[\langle f_1, \mathbf{P}_{\mathcal{Y}}f_2 \rangle = \langle \mathbf{P}_{\mathcal{Y}} f_1, f_2 \rangle\]</span>
then the <a href="ch-optApp.html#sub:appSys">metric projection operator</a> <span class="math inline">\(\hat{f}=\mathbf{P}_{\mathcal{Y}} f\)</span> has the following properties<label for="tufte-sn-389" class="margin-toggle sidenote-number">389</label><input type="checkbox" id="tufte-sn-389" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">389</span> The projection property <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}\mathbf{P}_{\mathcal{Y}}=\mathbf{P}_{\mathcal{Y}}\)</span> tells that <span class="math inline">\(\|\mathbf{P}_{\mathcal{Y}}\|=1\)</span>. By using this result, we have
<span class="math display">\[
\begin{align*}
\langle  \mathbf{P}_{\mathcal{Y}}f, f \rangle &amp;= \langle \mathbf{P}_{\mathcal{Y}} \mathbf{P}_{\mathcal{Y}} f, f \rangle\\
&amp;= \langle \mathbf{P}_{\mathcal{Y}}  f, \mathbf{P}_{\mathcal{Y}} f \rangle \\
&amp;=\|\mathbf{P}_{\mathcal{Y}}f\|^2 =\|f\|^2.
\end{align*}\]</span></span>
<span class="math display">\[\mbox{1. } \langle  \hat{f}, f \rangle = \|\hat{f}\|^2=\|f\|^2,\,\, \mbox{2. }\langle f-\hat{f},\, \hat{f}\rangle = 0\]</span>
for any <span class="math inline">\(f\in\mathcal{V}\)</span>. The first says the norm of the hat operator is exactly the same as that of the original object. The second says that the error of the projection <span class="math inline">\(f-\hat{f}\)</span> and the projection <span class="math inline">\(\hat{f}\)</span> should be orthogonal to each other.<label for="tufte-sn-390" class="margin-toggle sidenote-number">390</label><input type="checkbox" id="tufte-sn-390" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">390</span> The way of having an orthogonal projection for a <strong>convex set</strong> <span class="math inline">\(\mathcal{Y}\)</span> in the Hilbert space is about having a conical shape of such <strong>convex set</strong> <span class="math inline">\(\mathcal{Y}\)</span>.</span></p>
<div class="solution">
<p class="solution-begin">
Definition and remark about the cone <span id="sol-start-93" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-93', 'sol-start-93')"></span>
</p>
<div id="sol-body-93" class="solution-body" style="display: none;">
<p>Notice that the derivative is a <a href="ch-MatComp.html#sub:vecSpaces">functional</a>, and the <a href="ch-vecMat.html#sub:linearity">gradient</a> is from the <a href="ch-optApp.html#sub:appSys">dual norm space</a>.
Consider a function <span class="math inline">\(f:\mathcal{Y}\rightarrow \mathbb{R}\)</span>. The <a href="ch-vecMat.html#sub:vec">inner product</a> of the <strong>variational inequality</strong>, i.e. <span class="math inline">\(\left\langle \nabla f(\mathbf{y}^{*}),\:\mathbf{y} - \mathbf{y}^{*} \right\rangle \geq 0\)</span>, consists of a <em>dual pair</em> such that <span class="math display">\[\langle\cdot,\cdot\rangle:\mathcal{Y}^{*}\times \mathcal{Y} \rightarrow \mathbb{R}.\]</span>
The inequality of the <strong>dual pair</strong> closely relate to the geometric interpretation about the <strong>cone</strong> in the <a href="ch-representation.html#sub:conjugacy">Hilbert space</a>.<label for="tufte-sn-391" class="margin-toggle sidenote-number">391</label><input type="checkbox" id="tufte-sn-391" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">391</span> 
In <a href="ch-optApp.html#sub:appSys">Banach space</a>, we can use a nested sequence of <a href="sub-continuity.html#sub:continuousFunc">balls</a> to define <strong>cone</strong>. Consider the following sequence
<span class="math display">\[\mathcal{B}_{r_1}(f_1)\subseteq \mathcal{B}_{r_2}(f_2)\cdots \]</span>where the radius <span class="math inline">\(r_n\)</span> goes to infinity. The union of these <a href="sub-continuity.html#sub:continuousFunc">balls</a> <span class="math inline">\(\cup_{i=1}^{\infty}\mathcal{B}_{r_{i}}\)</span> is called a <em>cone</em>. See figure <a href="ch-optApp.html#fig:Cone">15.1</a>.
It is known that if the <a href="ch-optApp.html#sub:appSys">Banach space</a> <span class="math inline">\(\mathcal{V}\)</span> is constructed by the union of the nested sequence of <a href="sub-continuity.html#sub:continuousFunc">balls</a>, then its <a href="ch-optApp.html#sub:appSys">dual norm space</a> <span class="math inline">\(\mathcal{V}\)</span> is always <strong>convex</strong>.
But the multiple-valued projection in the <a href="ch-optApp.html#sub:appSys">Banach space</a> loses almost all the properties we learned so far, so we’d better focus on the Hilbert space cases.</span></p>
<p>For a subset <span class="math inline">\(\mathcal{Y}\)</span> in a real-valued vector space <span class="math inline">\(\mathcal{V}\)</span> whose dual space is <span class="math inline">\(\mathcal{V}^*\)</span>, the set
<span class="math display">\[\left\{f\in \mathcal{V}^{*}:\langle f,g\rangle \geq 0\quad \forall g\in \mathcal{Y}\right\},\]</span>
is called the <em>dual cone</em> of <span class="math inline">\(\mathcal{Y}\)</span>. The <strong>dual one</strong> is always <strong>convex</strong>. Therefore, it is a <strong>convex cone</strong>.</p>
<p>For the orthogonal projection, we know that
<span class="math display">\[\langle f-\hat{f},\, \hat{f}\rangle = 0.\]</span>
With the <strong>variational inequality</strong>
<span class="math display">\[\langle f-\hat{f},\, \hat{f}-g\rangle\geq0,\]</span>
we know that if <span class="math inline">\(\hat{f}\)</span> is from an orthogonal projection, then <span class="math display">\[\langle f-\hat{f},\: g\rangle\leq0\quad\mbox{for all }g\in\mathcal{Y}\]</span>
forms a <strong>dual cone</strong>. In other words, the variational inequality specifies a <strong>convex cone</strong> structure on <span class="math inline">\(\mathcal{V}^{*}\)</span> that is similar to a <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> of <span class="math inline">\(\mathcal{V}^{*}\)</span>.</p>
<table>
<colgroup>
<col width="55%">
<col width="44%">
</colgroup>
<thead><tr class="header">
<th align="left"><a href="ch-MatComp.html#sub:vecSpaces">Subspace</a></th>
<th align="left"><em>Convex cone</em></th>
</tr></thead>
<tbody><tr class="odd">
<td align="left">A subset <span class="math inline">\(\mathcal{Y}\)</span> of <span class="math inline">\(\mathcal{V}\)</span> is a sub-space if <span class="math inline">\(\alpha f_1 + \beta f_2 \in\mathcal{Y}\)</span> whenever <span class="math inline">\(f_1,f_2\in\mathcal{Y}\)</span> and <span class="math inline">\(\alpha,\beta\in\mathbb{R}\)</span>
</td>
<td align="left">A subset <span class="math inline">\(\mathcal{Y}\)</span> of <span class="math inline">\(\mathcal{V}\)</span> is a <strong>convex cone</strong> if <span class="math inline">\(\alpha f_1 + \beta f_2 \in\mathcal{Y}\)</span> whenever <span class="math inline">\(f_1,f_2\in\mathcal{Y}\)</span> and <span class="math inline">\(\alpha,\beta \geq 0\)</span>
</td>
</tr></tbody>
</table>
<p>You can see that every <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> is a <strong>convex cone</strong>, but not conversely.</p>
<p>Therefore, constructing an orthogonal projection in the real-valued Hilbert space is about projecting onto a set whose dual is a <strong>dual cone</strong> (or a <a href="ch-MatComp.html#sub:vecSpaces">subspace</a>). Perhaps the simplest way of having an orthogonal projection is to project the object onto a <strong>convex cone</strong> or a <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> (so that <span class="math inline">\(\mathcal{Y}\)</span> is either a <strong>convex cone</strong> or a <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> of <span class="math inline">\(\mathcal{V}\)</span>) because, in this case, the dual of <span class="math inline">\(\mathcal{Y}\)</span> is always a <strong>convex cone</strong> (the dual in a Hilbert space is itself).</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
</div>
<div id="sub:Proj1" class="section level2">
<h2>
<span class="header-section-number">15.3</span> Miscellaneous Examples: Part 1</h2>
<p><span class="newthought">Multiple linear regression and hyperplane </span></p>
<p>Consider the <span class="math inline">\(n\)</span>-length data vectors from an <a href="ch-eigen.html#sub:matNorms">AR(2)</a> model
<span class="math display">\[\mathbf{x}_T = c_1\mathbf{x}_{T-1} + c_2\mathbf{x}_{T-2} + \mathbf{e}\]</span>
where <span class="math inline">\(\mathbf{e}\in\mathbb{R}^{n}\)</span> is a vector of errors.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb107-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>); time =<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">50</span></a>
<a class="sourceLine" id="cb107-2" data-line-number="2">xT.model =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(time)); xT.model[<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>] =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>) <span class="co"># At t=1,2, give the initials random values </span></a>
<a class="sourceLine" id="cb107-3" data-line-number="3"></a>
<a class="sourceLine" id="cb107-4" data-line-number="4"><span class="co"># An "explosive" AR(2)</span></a>
<a class="sourceLine" id="cb107-5" data-line-number="5"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(<span class="kw">length</span>(time)<span class="op">-</span><span class="dv">2</span>)){ </a>
<a class="sourceLine" id="cb107-6" data-line-number="6">  e =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb107-7" data-line-number="7">  xT.model[t<span class="op">+</span><span class="dv">2</span>] =<span class="st"> </span>xT.model[t<span class="op">+</span><span class="dv">1</span>] <span class="op">-</span><span class="st"> </span><span class="fl">1.2</span><span class="op">*</span><span class="st"> </span>xT.model[t] <span class="op">+</span><span class="st"> </span>e</a>
<a class="sourceLine" id="cb107-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb107-9" data-line-number="9">xT =<span class="st"> </span>xT.model[<span class="dv">3</span><span class="op">:</span><span class="kw">length</span>(time)];  <span class="co"># x_T</span></a>
<a class="sourceLine" id="cb107-10" data-line-number="10">xT.lag =<span class="st"> </span>xT.model[<span class="dv">2</span><span class="op">:</span>(<span class="kw">length</span>(time)<span class="op">-</span><span class="dv">1</span>)]; <span class="co"># x_{T-1} </span></a>
<a class="sourceLine" id="cb107-11" data-line-number="11">xT.lag2 =<span class="st"> </span>xT.model[<span class="dv">1</span><span class="op">:</span>(<span class="kw">length</span>(time)<span class="op">-</span><span class="dv">2</span>)] <span class="co"># x_{T-2}</span></a></code></pre></div>
<p>The <a href="ch-eigen.html#sub:matNorms">AR(2)</a> model tells us that <span class="math inline">\(\mathbf{x}_T\)</span> is “generated” by two “older” vectors <span class="math inline">\(\mathbf{x}_{T-1}\)</span> and <span class="math inline">\(\mathbf{x}_{T-2}\)</span>. Therefore, it is natural to think that <span class="math inline">\(\mathbf{x}_T\)</span> must be in a kind of subspace of the space “generated” by <span class="math inline">\(\mathbf{x}_{T-1}\)</span> and <span class="math inline">\(\mathbf{x}_{T-2}\)</span>. Also, you should not be surprised if I project the <span class="math inline">\(\mathbf{x}_T\)</span> onto such a space.</p>
<p>By consider <span class="math inline">\([\mathbf{x}_{T-1},\mathbf{x}_{T-2}]^{\top}\)</span> as a <span class="math inline">\(n\times 2\)</span> matrix <span class="math inline">\(\mathbf{X}\)</span>, we can form the <a href="ch-representation.html#sub:conjugacy">projection matrix</a> <span class="math display">\[\mathbf{P}_{\mathbf{X}}=\mathbf{X}(\mathbf{X}^{\top}\mathbf{X})^{-1}\mathbf{X}^{\top}.\]</span> By multiplying the matrices, we can verify the projection property <span class="math inline">\(\mathbf{P}_{\mathbf{X}}\mathbf{P}_{\mathbf{X}}=\mathbf{P}_{\mathbf{X}}\)</span>.
The projection of <span class="math inline">\(\mathbf{x}_T\)</span> gives <span class="math display">\[\mathbf{P}_{\mathbf{X}}\mathbf{x}_{T} = \mathbf{X} \hat{\mathbf{c}}= \hat{c}_1\mathbf{x}_{T-1} + \hat{c}_2\mathbf{x}_{T-2}\]</span><br>
where the first equality is another way of presenting the <a href="ch-representation.html#sub:conjugacy">Riesz representation</a> for the projection operator (matrix).<label for="tufte-sn-392" class="margin-toggle sidenote-number">392</label><input type="checkbox" id="tufte-sn-392" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">392</span> The <a href="ch-optApp.html#sub:appSys">hat operator</a> <span class="math inline">\(\hat{\mathbf{c}}=(\mathbf{X}^{\top}\mathbf{X})^{-1}\mathbf{X}^{\top}\mathbf{x}_{T}\)</span> is analogous to the previous <a href="ch-vecMat.html#sub:linearity">least square estimator</a> in the simple <a href="ch-vecMat.html#sub:linearity">linear regression</a>. Indeed, this <a href="ch-eigen.html#sub:matNorms">AR(2)</a> is a <em>multiple linear regression</em> model where the <em>regressors</em>, the variables to be regressed on, have the number more than one. The <strong>regressors</strong> in this <a href="ch-eigen.html#sub:matNorms">AR(2)</a> are <span class="math inline">\(\mathbf{x}_{T-1}\)</span> and <span class="math inline">\(\mathbf{x}_{T-2}\)</span>.</span></p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" data-line-number="1">X =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(xT.lag,xT.lag2),<span class="dt">ncol=</span><span class="dv">2</span>);</a>
<a class="sourceLine" id="cb108-2" data-line-number="2"><span class="co"># An estimate of c(1, -1.2)</span></a>
<a class="sourceLine" id="cb108-3" data-line-number="3">chat =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>X) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>xT; chat</a></code></pre></div>
<pre><code>##            [,1]
## [1,]  0.9973927
## [2,] -1.1978226</code></pre>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:projR2"></span>
<img src="fig/Part4/projR2.png" alt="Project the 3-dimensional data points onto a 2-dimensional hyperplane" width="100%"><!--
<p class="caption marginnote">-->Figure 15.3: Project the 3-dimensional data points onto a 2-dimensional hyperplane<!--</p>-->
<!--</div>--></span>
</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb110-1" data-line-number="1"><span class="co"># plot</span></a>
<a class="sourceLine" id="cb110-2" data-line-number="2"><span class="kw">library</span>(scatterplot3d)</a>
<a class="sourceLine" id="cb110-3" data-line-number="3">p =<span class="st"> </span><span class="kw">scatterplot3d</span>(xT, xT.lag,xT.lag2, <span class="dt">type=</span><span class="st">"h"</span>, <span class="dt">angle=</span><span class="dv">45</span>, <span class="dt">color=</span><span class="st">"blue"</span>, <span class="dt">highlight.3d=</span><span class="ot">TRUE</span>, <span class="dt">pch =</span> <span class="dv">16</span>)</a>
<a class="sourceLine" id="cb110-4" data-line-number="4">p<span class="op">$</span><span class="kw">plane3d</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="kw">t</span>(chat)), <span class="dt">draw_polygon =</span> <span class="ot">TRUE</span>) <span class="co"># The approximating plane for the points in R^3 </span></a></code></pre></div>
<p>Figure <a href="ch-optApp.html#fig:projR2">15.3</a> shows that the projection of <span class="math inline">\(\mathbf{x}_{T}\)</span> locates closely to the plane <span class="math inline">\(x_t = \hat{c}_1x_{t-1} + \hat{c}_2 x_{t-2}\)</span>
where <span class="math inline">\(x_t\)</span>, <span class="math inline">\(x_{t-1}\)</span>, <span class="math inline">\(x_{t-2}\)</span> are the variable corresponding to the data realizations <span class="math inline">\(\mathbf{x}_{t}\)</span>, <span class="math inline">\(\mathbf{x}_{t-1}\)</span>, <span class="math inline">\(\mathbf{x}_{t-2}\)</span> respectively. The dimension of this plane is two, while the data set of the model is three. Basically, the <strong>multiple linear regression</strong> conducts a projection onto the space one dimensional lower than the original space.<label for="tufte-sn-393" class="margin-toggle sidenote-number">393</label><input type="checkbox" id="tufte-sn-393" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">393</span> Usually people write a multiple linear regression as <span class="math inline">\(Y = \sum_{i=1}^{k} \beta_i X_i +\varepsilon\)</span>. The variable <span class="math inline">\(Y\)</span> together with <span class="math inline">\(k-1\)</span> <strong>regressor</strong> <span class="math inline">\(\{X_i\}_{i=1}^{k}\)</span>s form a <span class="math inline">\(k\)</span>-dimensional regresion problem. We can understand that such a linear regression is to reduce some situation from <span class="math inline">\(k\)</span>-dimemsion to <span class="math inline">\(k-1\)</span>-dimension by explaining one selected variable <span class="math inline">\(Y\)</span> in terms of a <a href="ch-vecMat.html#sub:vec">linear combination</a> of the <span class="math inline">\(k-1\)</span> <strong>regressors</strong>.</span></p>
<p>Geometrically speaking, a set that is one dimensional lower than the original space and that is form by a linear continuous <a href="ch-MatComp.html#sub:vecSpaces">function(al)</a> is called the <strong>hyperplane</strong>. Let <span class="math inline">\(\mathcal{V}\)</span> be a vector space. Given a linear function(al) <span class="math inline">\(g\in \mathcal{V}\)</span> and a scalar <span class="math inline">\(a\)</span> from the <a href="ch-MatComp.html#sub:vecSpaces">scalar field</a> <span class="math inline">\(\mathbb{F}\)</span>, the set
<span class="math display">\[\mathcal{S}=\{g\in\mathcal{V}:\langle f, \,g\rangle=a, a\in \mathbb{F} \}\]</span> is a <em>hyperplane</em> in <span class="math inline">\(\mathcal{V}\)</span> determined by <span class="math inline">\(f\)</span> and <span class="math inline">\(a\)</span>. In the previous example, the <strong>hyperplane</strong> is given by <span class="math display">\[\mathcal{S}=\left\{\left[\begin{array}{c}
x_{t}\\
x_{t-1}\\
x_{t-2}
\end{array}\right]\in\mathbb{R}^{3}:\, \left\langle \left[\begin{array}{c}
-1\\
\hat{c}_{1}\\
\hat{c}_{2}
\end{array}\right]\,,\,\left[\begin{array}{c}
x_{t}\\
x_{t-1}\\
x_{t-2}
\end{array}\right]\right\rangle =0 \right\},\]</span>
a <span class="math inline">\(2\)</span>-dimensional plane in <span class="math inline">\(\mathbb{R}^{3}\)</span>.</p>
<p><span class="newthought">Separation and non-arbitrage </span></p>
<p>When the <a href="ch-MatComp.html#sub:vecSpaces">scalar field</a> <span class="math inline">\(\mathbb{F}\)</span> of the <strong>hyperplane</strong> is restricted to <span class="math inline">\(\mathbb{R}\)</span>, one can use inequalities to illustrate how a hyperplane can separate the space into two, i.e. <span class="math inline">\(\{f\in\mathcal{V}:\langle f, \,g\rangle \leq a \}\)</span> and <span class="math inline">\(\{f\in\mathcal{V}:\langle f, \,g\rangle &gt; a \}\)</span>.<label for="tufte-sn-394" class="margin-toggle sidenote-number">394</label><input type="checkbox" id="tufte-sn-394" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">394</span> These two sets are called the <em>half-spaces</em> determined by the <strong>hyperplane</strong> <span class="math inline">\(\mathcal{S}\)</span>. If a subset belongs to one of this half-space, then this subset is on one-side of the <strong>hyperplane</strong>.</span></p>
<p>The separation is about forming a dichotomy between the objects of interests and the rest. For example, assume that <span class="math inline">\(\mathcal{V}=\mathbb{R}^{n}\)</span>. Let <span class="math inline">\(\mathcal{Y}\in\mathcal{V}\)</span> be a <a href="ch-optApp.html#sub:Optimization">convex set</a>. Let <span class="math inline">\(\mathbf{z}\in\mathcal{V}\)</span> and <span class="math inline">\(\mathbf{z}\notin\mathcal{Y}\)</span>. Then exist non-zero vector <span class="math inline">\(\mathbf{p}\in\mathcal{V}\)</span> and a real number <span class="math inline">\(a\in\mathbb{R}\)</span> such that
<span class="math display">\[\mathbf{p}^{\top} \mathbf{y} \geq a&gt; \mathbf{p}^{\top}\mathbf{z},\]</span>
for any <span class="math inline">\(\mathbf{y}\in\mathcal{Y}\)</span>.</p>
<p>Here the <strong>hyperplane</strong> <span class="math inline">\(\mathcal{S}=\{\mathbf{y}\in\mathbb{R}^{n}:\langle \mathbf{p}, \,\mathbf{y} \rangle=a \}\)</span> strictly separate <span class="math inline">\(\mathbf{z}\)</span> and <span class="math inline">\(\mathcal{Y}\)</span>. This result comes from the <a href="ch-optApp.html#sub:Optimization">variational inequality</a>.<label for="tufte-sn-395" class="margin-toggle sidenote-number">395</label><input type="checkbox" id="tufte-sn-395" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">395</span> Let <span class="math inline">\(\hat{\mathbf{z}}\)</span> be the projection of <span class="math inline">\(\mathbf{z}\)</span> on the set <span class="math inline">\(\mathcal{Y}\)</span>, <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}\mathbf{z}=\hat{\mathbf{z}}\in\mathcal{Y}\)</span>. The <a href="ch-optApp.html#sub:Optimization">variational inequality</a> becomes <span class="math display">\[(\hat{\mathbf{z}}-\mathbf{z})^\top (\mathbf{y}-\hat{\mathbf{z}})\geq0\]</span>
for all <span class="math inline">\(\mathbf{y}\in\mathcal{Y}\)</span>. Let <span class="math inline">\(\mathbf{p}=\hat{\mathbf{z}}-\mathbf{z}\)</span> and consider non-zero <span class="math inline">\(\mathbf{p}\)</span>. Let <span class="math inline">\(\mathbf{p}^{\top}\hat{\mathbf{z}}=a\)</span>. Then the inequality gives
<span class="math display">\[\mathbf{p}^{\top}(\mathbf{y}-\hat{\mathbf{z}})\geq0\Leftrightarrow \mathbf{p}^{\top} \mathbf{y} \geq a.\]</span>
On the other hand, <span class="math display">\[a- \mathbf{p}^{\top}\mathbf{z} =\mathbf{p}^{\top}(\hat{\mathbf{z}}-\mathbf{z})=\|\hat{\mathbf{z}}-\mathbf{z}\|^{2}&gt;0\]</span> implies <span class="math inline">\(a&gt;\mathbf{p}^{\top}\mathbf{z}\)</span>.
The result follows.</span></p>
<p>In economics, the vector <span class="math inline">\(\mathbf{p}\)</span> in the above inequalities is often interpreted as a price vector; and the set <span class="math inline">\(\mathcal{Y}\)</span> is treated as a feasible set for production while <span class="math inline">\(\mathbf{z}\notin\mathcal{Y}\)</span> is a non-feasible plan, i.e. the non-profitable production allocation.<label for="tufte-sn-396" class="margin-toggle sidenote-number">396</label><input type="checkbox" id="tufte-sn-396" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">396</span> In this context, the price vector <span class="math inline">\(\mathbf{p}=\hat{\mathbf{z}}-\mathbf{z}\)</span> can be understood as the gain (or the residual) in the projection of some impossibility onto some possibility (of course, the possibile output <span class="math inline">\(\hat{\mathbf{z}}\)</span> should be larger than the impossible <span class="math inline">\(\mathbf{z}\)</span> to attain a positive price). Let <span class="math inline">\(a=0\)</span>, the result says
<span class="math display">\[\mathbf{p}^{\top}\hat{\mathbf{z}}=(\hat{\mathbf{z}}-\mathbf{z})^{\top}\hat{\mathbf{z}}=0,\]</span> either <span class="math inline">\(\mathbf{p}=0\)</span> or <span class="math inline">\(\hat{\mathbf{z}}=0\)</span>.</span></p>
<p>The development of these inequalities touch the core of the pricing theory in economics and finance. The basic idea is that
if the price vector is “generated” by a convex set, a <strong>hyperplane</strong> consisting of this price vector and a vector of the pricing commodities should be able to separate some infeasible situations regarding on those commodities from the feasible ones.</p>
<p>One typical “infeasible” or “unpleasant”" case in the financial market is the <strong>arbitrage</strong> (making something from nothing).
Let <span class="math inline">\(\mathbf{A}\in\mathbb{R}^{n\times m}\)</span>, <span class="math inline">\(\mathbf{p},\mathbf{y}\in\mathbb{R}^{n}\)</span>, <span class="math inline">\(\mathbf{x}\in\mathbb{R}^{m}\)</span>, and <span class="math inline">\(\mathbf{x} \succeq \mathbf{0}\)</span>. Here <span class="math inline">\(\mathbf{x} \succeq \mathbf{0}\)</span> means all the entries of the vector <span class="math inline">\(\mathbf{x}\)</span> are non-negative. If we assume that the price vector has a <a href="#">representation</a> <span class="math inline">\(\mathbf{p}=\mathbf{A}\mathbf{x}\)</span> with the positive value “basis” vector <span class="math inline">\(\mathbf{x}\)</span> over <span class="math inline">\(m\)</span> states, then the matrix <span class="math inline">\(\mathbf{A}\in\mathbb{R}^{n\times m}\)</span> can be defined as the pricing plan for <span class="math inline">\(n\)</span> commodities under <span class="math inline">\(m\)</span> states or in financial market it can be be defined as the units of <span class="math inline">\(n\)</span> securities under <span class="math inline">\(m\)</span> states.<label for="tufte-sn-397" class="margin-toggle sidenote-number">397</label><input type="checkbox" id="tufte-sn-397" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">397</span> The vector <span class="math inline">\(\mathbf{A}\)</span> is also known as the pricing kernel.</span></p>
<p>Let <span class="math inline">\(\mathbf{y} \in \mathbb{R}^{n}\)</span> be a production plan or a porfolio of <span class="math inline">\(n\)</span> commodities (or securities). Having an <em>arbitrage</em> means
<span class="math display">\[\begin{align*}
\mbox{Case I, negative profit (or negative market value): }&amp; \quad \mathbf{p}^{\top}\mathbf{y}&lt;0\\
\mbox{Case II, profitable production plan (positive payoff): }&amp;\quad \mathbf{A}^{\top}\mathbf{y}\succeq 0.
\end{align*}\]</span></p>
<p>One fundamental theory about the financial market is that there is no <strong>arbitrage</strong> when the market is efficient.
In our quantitative context, this statement means if the price vector <span class="math inline">\(\mathbf{p}\)</span> can be efficiently linearly represented some positive vector <span class="math inline">\(\mathbf{x}\)</span> under the linear transformation <span class="math inline">\(\mathbf{A}\)</span>, then there can only be a dichotomy between case I and case II.</p>
<p>This dichotomy actually comes from the following <strong>Farkas’ lemma</strong> (another dichotomy).</p>
<ul>
<li>
<em>Farkas’ lemma</em> : The system I and the system II
<span class="math display">\[\begin{align*}
\mbox{system I: }&amp; \quad \left\{\mathbf{x} : \mathbf{A}\mathbf{x}=\mathbf{p},\,  \mathbf{x}\succeq \mathbf{0}\right\}\\
\mbox{system II: }&amp;\quad \left\{\mathbf{y} : \mathbf{A}^{\top}\mathbf{y} \succeq \mathbf{0}, \, \mathbf{p}^{\top}\mathbf{y}&lt;0\right\}
\end{align*}
\]</span> cannot be solved simultanelously.<label for="tufte-sn-398" class="margin-toggle sidenote-number">398</label><input type="checkbox" id="tufte-sn-398" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">398</span> That is, if system I has a solution for <span class="math inline">\(\mathbf{x}\)</span>, then system II will have no solution for <span class="math inline">\(\mathbf{y}\)</span>, vice versa.</span>
</li>
</ul>
<p>As you can see, the <strong>Farkas’ lemma</strong> says if system I is true, then the condition <span class="math inline">\(\mathbf{p}^{\top}\mathbf{y}&lt;0\)</span> and <span class="math inline">\(\mathbf{A}^{\top}\mathbf{y}\succeq\mathbf{0}\)</span> cannot both be true at the same.<label for="tufte-sn-399" class="margin-toggle sidenote-number">399</label><input type="checkbox" id="tufte-sn-399" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">399</span> One can rule out the <strong>arbitrage</strong> opportunity if system I is true. While if system II is true, it violates the <strong>arbitrage</strong> definition. Based on these deduction, one can claim that the price given by <span class="math inline">\(\mathbf{p}=\mathbf{A}\mathbf{x}\)</span> is a <strong>non-arbitrage price</strong> (also known as the <em>fundamental theorem in the asset pricing</em>). There are many variations of this non-arbitrage claim, but they basically follow the same primitive logic as above.</span></p>
<p>The result behind the <strong>non-arbitrage pricing</strong> and <strong>Farkas’ lemma</strong> is a <strong>hyperplane</strong> <span class="math inline">\(\mathcal{S}=\{\mathbf{y}\in\mathbb{R}^{n}:\langle \mathbf{p}, \,\mathbf{y} \rangle=0\}\)</span> separating system I and system II.
Because the system I is in fact equivalent to
<span class="math display">\[\mbox{system I': } \quad \left\{\mathbf{y}:\,\mathbf{A}^{\top}\mathbf{y} \succeq \mathbf{0}, \, \mathbf{p}^{\top}\mathbf{y}\geq 0\right\}.\]</span></p>
<div class="solution">
<p class="solution-begin">
Equivalence of system I and system I’ <span id="sol-start-94" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-94', 'sol-start-94')"></span>
</p>
<div id="sol-body-94" class="solution-body" style="display: none;">
<p>Let <span class="math inline">\(\mathcal{Y} = \left\{\mathbf{A}\mathbf{x} : \mathbf{x}\succeq 0 \right\}\)</span>. Notice that <span class="math inline">\(\mathcal{Y}\)</span> is a <a href="ch-optApp.html#sub:Optimization">conex set</a>.<label for="tufte-sn-400" class="margin-toggle sidenote-number">400</label><input type="checkbox" id="tufte-sn-400" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">400</span> In fact, the set <span class="math inline">\(\mathcal{Y}\)</span> is a <a href="ch-optApp.html#sub:Optimization">convex cone</a> as any element from <span class="math inline">\(\mathcal{Y}\)</span> is a <em>conical sum</em> <span class="math inline">\(\sum_{i=1}^{m}\mathbf{a}_i x_i\)</span>, a sum of columns vectors <span class="math inline">\(\mathbf{a}_i\)</span> of <span class="math inline">\(\mathbf{A}\)</span> with positive coefficients <span class="math inline">\(\{x_i\}_{i=1}^{m}\)</span>, which coincides with the previous definition of the <a href="ch-optApp.html#sub:Optimization">convex cone</a>.</span> The previous separation inequalities tell us that for any <span class="math inline">\(\mathbf{p}_0 \notin \mathcal{Y}\)</span>, there exist <span class="math inline">\(\mathbf{y}\in\mathbb{R}^n\)</span> and <span class="math inline">\(a\in\mathbb{R}\)</span> such that
<span class="math display">\[\mathbf{y}^{\top} \mathbf{p}_1 \geq a&gt; \mathbf{y}^{\top}\mathbf{p}_0,\mbox{ for any }\mathbf{p}_1\in\mathcal{Y} .\]</span></p>
<p>By restricting our attention to the <strong>hyperplane</strong> <span class="math inline">\(\mathcal{S}=\{\mathbf{y}\in\mathbb{R}^{n}:\langle \mathbf{p}, \,\mathbf{y} \rangle=0\}\)</span>, we have<label for="tufte-sn-401" class="margin-toggle sidenote-number">401</label><input type="checkbox" id="tufte-sn-401" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">401</span> Notice that the inner product <span class="math inline">\(\langle \mathbf{p}, \mathbf{y} \rangle = a\)</span> has the “sandwich form” expressions. <span class="math display">\[\langle \mathbf{A}\mathbf{x}, \mathbf{y} \rangle = \langle \mathbf{x}, \mathbf{A}^{\top}\mathbf{y} \rangle = a.\]</span>
We will come back to this “sandwich form” in sec[?].</span>
<span class="math display">\[\mathbf{p}_1^{\top} \mathbf{y} = \langle \mathbf{A}\mathbf{x}_1, \mathbf{y} \rangle = \langle \mathbf{x}_1, \mathbf{A}^{\top}\mathbf{y} \rangle  \geq 0 &gt; \mathbf{p}_0^{\top}\mathbf{y}=0\,\mbox{ for any }\mathbf{p}_1\in\mathcal{C} \mbox{ and }\mathbf{p}_0\notin \mathcal{C}.\]</span>
Thus, if <span class="math inline">\(\mathbf{p}\in \mathcal{C}\)</span>, then <span class="math inline">\(\mathbf{p}\)</span> must behave like <span class="math inline">\(\mathbf{p}_1\)</span>, so <span class="math inline">\(\mathbf{p}^{\top} \mathbf{y}\geq 0\)</span>, and since <span class="math inline">\(\mathbf{x}\succeq 0\)</span> we have <span class="math inline">\(\mathbf{A}^{\top}\mathbf{y}\succeq 0\)</span>. Otherwise, for <span class="math inline">\(\mathbf{p}\notin\mathcal{C}\)</span>, <span class="math inline">\(\mathbf{p}\)</span> must behave like <span class="math inline">\(\mathbf{p}_0\)</span> such that <span class="math inline">\(\mathbf{p}\neq \mathbf{A}\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{p}^{\top} \mathbf{y}&lt;0\)</span>.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
</div>
</div></body></html>

<p style="text-align: center;">
<a href="ch-representation.html"><button class="btn btn-default">Previous</button></a>
<a href="bibliography.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-12-19
</p>
</div>
</div>



</body>
</html>
