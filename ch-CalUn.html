<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="9 Calculus under Uncertainty | Project XXII" />
<meta property="og:type" content="book" />





<meta name="author" content="Zhengyuan Gao" />

<meta name="date" content="2020-11-15" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="9 Calculus under Uncertainty | Project XXII">

<title>9 Calculus under Uncertainty | Project XXII</title>

<link href="libs/tufte-css/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding/datatables.js"></script>
<link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>



</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Project XXII<p><p class="author">Zhengyuan Gao</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BA; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Preface</a>
<a href="part-i-inference-based-upon-logic-and-logical-computation.html">PART I: Inference Based upon Logic and Logical Computation</a>
<a href="sub-logic.html"><span class="toc-section-number">1</span> Logic</a>
<a href="sub-set-theory.html"><span class="toc-section-number">2</span> Set Theory</a>
<a href="sub-axioms.html"><span class="toc-section-number">3</span> Axioms</a>
<a href="sub-inferknow.html"><span class="toc-section-number">4</span> Inference and Knowledge</a>
<a href="sub-incomplete.html"><span class="toc-section-number">5</span> Incompleteness</a>
<a href="part-ii-infinitesimal-changes-and-their-consequences.html">PART II: Infinitesimal Changes and their Consequences</a>
<a href="sub-continuity.html"><span class="toc-section-number">6</span> Continuity</a>
<a href="sub-calculus.html"><span class="toc-section-number">7</span> Calculus</a>
<a href="ch-DE.html"><span class="toc-section-number">8</span> Differential Equations</a>
<a id="active-page" href="ch-CalUn.html"><span class="toc-section-number">9</span> Calculus under Uncertainty</a><ul class="toc-sections">
<li class="toc"><a href="#sub:rv"> Random Variables and Distributions</a></li>
<li class="toc"><a href="#sub:divRV"> Examples: Divisibility and Infinite Divisibility of Random Variables</a></li>
<li class="toc"><a href="#sub:ex"> Expectation</a></li>
<li class="toc"><a href="#sub:conProb"> Conditional Probability and Conditional Expectation</a></li>
<li class="toc"><a href="#sub:consp"> * Miscellaneous: Subjective Belief, Borel-Kolomogorov Paradox and Conspiracy</a></li>
</ul>
<a href="part-iii-emergence-of-abstract-interactions.html">PART III: Emergence of Abstract Interactions</a>
<a href="ch-vecMat.html"><span class="toc-section-number">10</span> Vector and Matrix</a>
<a href="ch-MatComp.html"><span class="toc-section-number">11</span> Matrix Computation</a>
<a href="ch-eigen.html"><span class="toc-section-number">12</span> Eigenvalues and Eigenvectors</a>
<a href="ch-UnMulti.html"><span class="toc-section-number">13</span> Uncertainty in Multiple Dimensions</a>
<a href="part-iv-three-masons-to-illuminate-the-dual-world.html">PART IV: Three Masons to Illuminate the Dual World</a>
<a href="ch-representation.html"><span class="toc-section-number">14</span> Representation</a>
<a href="ch-optApp.html"><span class="toc-section-number">15</span> Optimum Approximation</a>
<a href="bibliography.html">Bibliography</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="ch:CalUn" class="section level1">
<h1>
<span class="header-section-number">9</span> Calculus under Uncertainty</h1>
<p>In history, perhaps ironically, the concepts of <a href="ch-CalUn.html#sub:rv">randomness</a>, <a href="ch-CalUn.html#sub:ex">expectation</a>, and <a href="sub-incomplete.html#sub:beyond2">probability</a> were systematically formed together with the emergence of <a href="ch-DE.html#sub:MecWorld">mechanical determinism</a>. The studies of gamblings, lotteries, and dices in the 17th century contributed to the understanding of the tendency to produce stable relative frequencies. The economic needs, for example, actuarial competencies for insurance and annuities, were also in response to the demand for a special calculus to govern the nations under the economic uncertainty.</p>
<p>Also, in the 17th century, Blaise Pascal extended the scope of <a href="sub-incomplete.html#sub:beyond2">probability theory</a> from the secular games of chance to the epistemology. Pascal gave some <em>game-theoretic</em> considerations that concern belief in God, which is known as <em>Pascal’s wager</em> (le pari de Pascal), and which also marked the initiative of <em>decision theory</em>, a theory of deciding what to do when it is uncertain what will happen.</p>
<p><strong>Pascal’s wager</strong> of God can be summarized by the four possibilities listed in the following table:</p>
<table>
<thead><tr class="header">
<th align="left">Worship</th>
<th align="center">Existence</th>
<th align="center">Non-existence</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Belief</td>
<td align="center">infinite gain</td>
<td align="center">finite loss</td>
</tr>
<tr class="even">
<td align="left">Disbelief</td>
<td align="center">infinite loss</td>
<td align="center">finite gain</td>
</tr>
</tbody>
</table>
<p>According to this table, belief in God is better off, a dominating pay-off amongst the four, thus Pascal advocated the belief. He transferred the structure of reasoning about games of chance to the <a href="sub-inferknow.html#sub:inferknow">inference</a> that is not founded on any chance set-up but on two possible states of faith.</p>
<p>Nowadays, <strong>Pascal’s wager</strong> is not considered as a convincing argument.<label for="tufte-sn-120" class="margin-toggle sidenote-number">120</label><input type="checkbox" id="tufte-sn-120" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">120</span> One major suspicion (proposed by William James) is that a person who becomes a believers for the reasons urged by Pascal is not going to get the pay-offs listed in the table. In fact, this suspicion can be extended to all game-theoretic analyses when the measurements of pay-offs are unreliable. </span> But it is a reasonable starting point to consider the <em>dual roles</em> of <a href="sub-incomplete.html#sub:beyond2">probability</a>. On one side, the probability tends to give an objective fact of nature independent of anyone’s knowledge or evidence; on the other side, the probability is also connected with one’s belief warranted by the evidence. These two distinct understandings of probability often cause ambiguity.</p>
<p>Perhaps it is better returned to the Aristotelian opinion about the truth:</p>
<blockquote>
<p>Truth exists; we can know it, but not always.</p>
</blockquote>
<p>The existence of truth presupposes that probability is a substitute to describe our knowledge of the truth under uncertainty. <a href="ch-CalUn.html#sub:rv">Uncertainty</a> is in our minds, not in objects. Our conception of truth - indeed our conception of anything - must inevitably be anchored in a metaphysical ground created by ourselves. This ground could be “incomplete.” Only sometimes we can measure the truth, and there are good and bad ways of doing it. Therefore, it is critical to consider these dual roles of probability as an integrated object.</p>
<div id="sub:rv" class="section level2">
<h2>
<span class="header-section-number">9.1</span> Random Variables and Distributions</h2>
<p>We have seen that a <a href="sub-logic.html#sub:zeroth">proposition</a> is a <a href="sub-incomplete.html#sub:beyond2">probability event</a>. <strong>Randomness</strong> existed when the probability of a proposition given stated evidence or model is not <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>. <em>Randomness</em> is a synonym for the unknown. Randomness reflects the absence of knowledge of the cause or of what determines whether a proposition is true. It is a property measuring the <em>uncertainty</em> and expressing ignorance of causes.</p>
<p>An event or a state is random only if it is unknown (in its totality). The <strong>random variable</strong> represents such an unknown quantity. Variables are propositions that take specific values, meanwhile some thing(s) will cause such eventual values. Thus, the random variable refers to the <a href="sub-logic.html#sub:zeroth">proposition</a> with an unknown cause.</p>
<ul>
<li>
<em>Random variable</em>: Suppose that we have the probability <span class="math inline">\(\mathbb{P}:\mathcal{X}\rightarrow[0,1]\)</span> that maps events in <span class="math inline">\(\mathcal{X}\)</span> to <span class="math inline">\([0,1]\)</span>. Suppose that these events of <span class="math inline">\(\mathcal{X}\)</span> are unknown. A random variable <span class="math inline">\(Y\)</span> is a <strong>measurable map</strong><label for="tufte-sn-121" class="margin-toggle sidenote-number">121</label><input type="checkbox" id="tufte-sn-121" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">121</span> Intuitively, the <em>measurable map</em> is analogous to a <a href="sub-continuity.html#sub:continuousFunc">continuous function</a> that preserves the <a href="sub-incomplete.html#sub:beyond2"><span class="math inline">\(\sigma\)</span>-algebra</a> structure of the <a href="sub-incomplete.html#sub:beyond2">probability space</a> <span class="math inline">\((\mathcal{X},\sigma(\mathcal{X}),\mathbb{P})\)</span>. Technically, the measurable map means that, for every <span class="math inline">\(y\)</span>, <span class="math inline">\(\{x:\, Y(x)\leq y\}\in\sigma(\mathcal{X})\)</span>.</span> <span class="math display">\[Y:\,\mathcal{X}\rightarrow\mathbb{R}\]</span>
that assigns a real number <span class="math inline">\(y\in\mathbb{R}\)</span> to each possible outcome in <span class="math inline">\(\mathcal{X}\)</span>. In addition, if a <strong>random variable</strong> takes on values from a discrete set of numbers, <span class="math inline">\(Y:\,\mathcal{X}\rightarrow\mathbb{N}\)</span>, then <span class="math inline">\(Y\)</span> is called the <em>discrete random variable</em>.</li>
</ul>
<p>We use capital letters for <strong>random variables</strong> and lower case letters for their possible determined outcomes. An important convention is that we can describe a random variable <span class="math inline">\(Y\)</span> without having to describe the cause <span class="math inline">\(x\)</span>. The probability of the underlying unknown events is equivalent to the probability of the random variable:
<span class="math display">\[\mathbb{P}\left\{ \left.x\in\mathcal{X}\right|\; Y(x)=y\right\} =\mathbb{P}(Y=y).\]</span>
If we consider a set <span class="math inline">\(\mathcal{Y}\)</span> of possible values of <span class="math inline">\(Y\)</span>, then the equality becomes <span class="math display">\[\mathbb{P}\left\{ \left.x\in\mathcal{X}\right|\; Y(x)\in\mathcal{Y}\right\} =\mathbb{P}(Y\in\mathcal{Y}).\]</span></p>
<p>Let’s consider an experiment of tossing a (fair) coin three times. Let <span class="math inline">\(Y\)</span> be the number of heads (H) until the first tail (T), and let <span class="math inline">\(Z\)</span> be the total number of heads. The results of the tosses are unknown yet. Thus both <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> are <strong>discrete random variables</strong>. We have the following tables for this experiment<label for="tufte-sn-122" class="margin-toggle sidenote-number">122</label><input type="checkbox" id="tufte-sn-122" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">122</span> If you don’t know the specific three outcomes of the tosses, the proposition “total number of heads is 1” is not known to be true because you don’t know the cause (three outcomes) and because there is no other information that would let you deduce the outcomes.</span>:</p>
<table>
<colgroup>
<col width="13%">
<col width="10%">
<col width="10%">
<col width="10%">
<col width="10%">
<col width="10%">
<col width="10%">
<col width="10%">
<col width="10%">
</colgroup>
<thead><tr class="header">
<th align="left"><span class="math inline">\(x\)</span></th>
<th align="center">HHH</th>
<th align="center">HHT</th>
<th align="center">HTH</th>
<th align="center">HTT</th>
<th align="center">THH</th>
<th align="center">THT</th>
<th align="center">TTH</th>
<th align="center">TTT</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(\mathbb{P}(\{x\})\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{8}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(Y(x)\)</span></td>
<td align="center"><span class="math inline">\(3\)</span></td>
<td align="center"><span class="math inline">\(2\)</span></td>
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(Z(x)\)</span></td>
<td align="center"><span class="math inline">\(3\)</span></td>
<td align="center"><span class="math inline">\(2\)</span></td>
<td align="center"><span class="math inline">\(2\)</span></td>
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(2\)</span></td>
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(0\)</span></td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col width="23%">
<col width="19%">
<col width="19%">
<col width="19%">
<col width="19%">
</colgroup>
<thead><tr class="header">
<th align="left">Observation or Relization</th>
<th align="center"><span class="math inline">\(y=3\)</span></th>
<th align="center"><span class="math inline">\(y=2\)</span></th>
<th align="center"><span class="math inline">\(y = 1\)</span></th>
<th align="center"><span class="math inline">\(y=0\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Probability of <span class="math inline">\(Y\)</span>: <span class="math inline">\(\mathbb{P}_Y(y)\)</span>
</td>
<td align="center"><span class="math inline">\(\frac{1}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{4}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{2}\)</span></td>
</tr>
<tr class="even">
<td align="left">Probability of <span class="math inline">\(Z\)</span>: <span class="math inline">\(\mathbb{P}_{Z}(z)\)</span>
</td>
<td align="center"><span class="math inline">\(\frac{1}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{3}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{3}{8}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{8}\)</span></td>
</tr>
</tbody>
</table>
<p>Notice the outcomes of <span class="math inline">\(\mathcal{X}\)</span> are equally distributed because we assume the coin is a fair coin. Otherwise, one side may be more likely to appear than the other, and the outcomes cannot be equal. For a finite set <span class="math inline">\(\mathcal{A}\)</span> of objects in <span class="math inline">\(\mathcal{X}\)</span>, if each outcome is equally likely, then we can define<span class="math display">\[\mathbb{P}(\mathcal{A})=\frac{\mbox{size of }\mathcal{A}}{\mbox{size of }\mathcal{X}},\]</span>
which is called the <em>uniform distribution</em>. The probability of <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span> above are calculated based on the <strong>uniform distribution</strong>. For example, <span class="math display">\[\mathbb{P}(Y=1)=\mathbb{P}\left\{ \left.x\in\mbox{HTH, HTT}\right|\; Y(x)=1\right\} =\frac{1}{8}+\frac{1}{8}=\frac{1}{4}.\]</span></p>
<p>The calculation of the probabilities depends on the <strong>distribution</strong> of the <strong>random variables</strong>. The <strong>(cumulative) distribution function</strong> of the random variable is called its <a href="sub-incomplete.html#sub:beyond2">probability law</a>. The <em>distribution function</em> is defined as <span class="math inline">\(F(y)=\mathbb{P}(Y\leq y)\)</span>. Especially, for any interval <span class="math inline">\((a,b]\)</span> we have: <span class="math display">\[\mathbb{P}(a&lt;Y\leq b)=\mathbb{P}(Y\leq b)-\mathbb{P}(Y\leq a)=F(b)-F(a).\]</span> A consequence of this formula is that if <span class="math inline">\(a&lt;b\)</span>, then <span class="math inline">\(F(a)\leq F(b)\)</span> so <span class="math inline">\(F(\cdot)\)</span> is a non-decreasing function. It is also easy to see that as <span class="math inline">\(y\rightarrow-\infty\)</span>, <span class="math inline">\(F(y)\rightarrow0\)</span> and as <span class="math inline">\(y\rightarrow\infty\)</span>, <span class="math inline">\(F(y)\rightarrow1\)</span>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:cdfpd"></span>
<img src="fig/Part2/cdfpd.png" alt="Cumulative distribution function (step) and probability mass (histogram)" width="100%"><!--
<p class="caption marginnote">-->Figure 9.1: Cumulative distribution function (step) and probability mass (histogram)<!--</p>-->
<!--</div>--></span>
</p>
<p>The possible values of a <strong>discrete random variable</strong> <span class="math inline">\(Y\)</span> are <a href="sub-axioms.html#sub:axSet">countable</a>, such as <span class="math inline">\(\{y_{1},y_{2}\dots,\}\)</span>. The <strong>distribution function</strong> of <strong>discrete random variable</strong> <span class="math inline">\(Y\)</span> is a step function (see figure <a href="ch-CalUn.html#fig:cdfpd">9.1</a>). If this step function <span class="math inline">\(F(\cdot)\)</span> of <span class="math inline">\(Y\)</span> has a jump of size <span class="math inline">\(k\)</span> at one point <span class="math inline">\(a\)</span>, then we have <span class="math inline">\(\mathbb{P}(Y=a)=k\)</span>.<label for="tufte-sn-123" class="margin-toggle sidenote-number">123</label><input type="checkbox" id="tufte-sn-123" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">123</span> For any positive <span class="math inline">\(\epsilon\)</span>, there is<span class="math display">\[\begin{align*} \mathbb{P}(Y=a)   =&amp;\mathbb{P}(Y\leq a)-\mathbb{P}(Y&lt;a)\\
    =&amp;F(a)-\lim_{\epsilon\rightarrow0}\mathbb{P}(Y\leq a-\epsilon)\\
    =&amp;F(a)-\lim_{\epsilon\rightarrow0}F(a-\epsilon)=k. \end{align*}\]</span>
Because <span class="math inline">\(F\)</span> is <a href="sub-continuity.html#sub:continuousFunc">discontinuous</a> at <span class="math inline">\(a\)</span>, the difference <span class="math inline">\(F(a)-\lim_{\epsilon\rightarrow0}F(a-\epsilon)\)</span> equals to the size of the jump.</span> For any value <span class="math inline">\(y\in\{y_{1},y_{2}\dots,\}\)</span>, the <strong>discrete probability</strong> <span class="math inline">\(f(y)=\mathbb{P}(Y=y)\)</span> is called as the <em>probability mass function</em>. The distribution of this <strong>discrete variable</strong> is given by <span class="math display">\[F(y)=\mathbb{P}(Y\leq y)=\sum_{y_{i}\leq y}f(y_{i})\]</span> for countably many <span class="math inline">\(y_{i}\)</span>.</p>
<div class="solution">
<p class="solution-begin">
R code for plotting distribution function <span id="sol-start-109" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-109', 'sol-start-109')"></span>
</p>
<div id="sol-body-109" class="solution-body" style="display: none;">
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1">y =<span class="st"> </span><span class="dv">0</span><span class="op">:</span><span class="dv">3</span>; <span class="co"># values of y</span></a>
<a class="sourceLine" id="cb16-2" data-line-number="2">py =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">4</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">8</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">8</span>); <span class="co"># corresponding probability mass </span></a>
<a class="sourceLine" id="cb16-3" data-line-number="3">cdfy =<span class="st"> </span><span class="kw">cumsum</span>(py)<span class="op">/</span><span class="kw">sum</span>(py); <span class="co"># cumulative distribution function </span></a>
<a class="sourceLine" id="cb16-4" data-line-number="4"></a>
<a class="sourceLine" id="cb16-5" data-line-number="5"><span class="co"># plot </span></a>
<a class="sourceLine" id="cb16-6" data-line-number="6"><span class="kw">library</span>(ggplot2) </a>
<a class="sourceLine" id="cb16-7" data-line-number="7">pdata =<span class="st"> </span><span class="kw">data.frame</span>(y,py,cdfy)</a>
<a class="sourceLine" id="cb16-8" data-line-number="8">fig =<span class="st"> </span><span class="kw">ggplot</span>(pdata, <span class="kw">aes</span>(y, cdfy))</a>
<a class="sourceLine" id="cb16-9" data-line-number="9">fig <span class="op">+</span><span class="st"> </span><span class="kw">geom_col</span>(<span class="kw">aes</span>(y,py), <span class="dt">fill=</span><span class="st">"skyblue"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(y,cdfy), <span class="dt">color=</span><span class="st">"blue"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">"Y"</span>, <span class="dt">y=</span><span class="st">"Probability"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</a></code></pre></div>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>If there exists a function <span class="math inline">\(f\)</span> such that for all <span class="math inline">\(y\)</span>, <span class="math display">\[ f(y)\geq 0,\, \mbox{ and }  \int_{-\infty}^{\infty}f(y)\mbox{d}y=1,\]</span> and for every <span class="math inline">\(a\leq b\)</span>, <span class="math display">\[\mathbb{P}(a&lt;Y\leq b)=\int_{a}^{b}f(y)\mbox{d}y,\]</span>
then <span class="math inline">\(Y\)</span> is a <em>continuous random variable</em>. The function <span class="math inline">\(f\)</span> is called the <em>probability density function</em>. In particular, there is<span class="math display">\[\mathbb{P}(Y\leq y)=F(y)=\int_{-\infty}^{y}f(s)\mbox{d}s,\;\mbox{and }f(s)=\left.\frac{\mbox{d}F(y)}{\mbox{d}y}\right|_{y=s},\]</span>
which also implies the distribution of a <strong>continuous random variable</strong> is <a href="sub-calculus.html#sub:diffInt">differentiable</a>. There are infinitely many ways of generating a distribution function. But it turns out that in practice people only consider a few families of <strong>distribution functions</strong>. The <em>family of distribution functions</em> means a parameterized distribution function such as <span class="math inline">\(F(y,\theta)\)</span> where the parameter <span class="math inline">\(\theta\)</span> is also an <a href="sub-set-theory.html#sub:func">input</a> of the distribution function but <span class="math inline">\(\theta\)</span> has nothing to do with the value of <span class="math inline">\(y\)</span>. The <strong>parameters</strong> allow distribution to take on a variety of shapes, as a different value of <span class="math inline">\(\theta\)</span> gives a different shape of the distribution function.</p>
<p>There are multiple ways to explain why only these families are used.<label for="tufte-sn-124" class="margin-toggle sidenote-number">124</label><input type="checkbox" id="tufte-sn-124" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">124</span> As a matter of fact, all these families more or less connect to one universal family called <em>exponential family</em>. We will come back to this point when we see the law of parsimony in Ch[?].</span> Given our current background, it would be better to consider the viewpoint that these families can generate some kinds of <a href="sub-inferknow.html#sub:dyn">self-similar</a> features. To be more specific, suppose we have two random variables <span class="math inline">\(Y_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span>. Their distribution functions, say <span class="math inline">\(F(y,\theta_{1})\)</span> and <span class="math inline">\(F(y,\theta_{2})\)</span>, are from the same family, then we would expect that the distribution of <span class="math inline">\(Y_{1}+Y_{2}\)</span> is also from the family, i.e. <span class="math inline">\(F(y,\theta_{1}+\theta_{2})\)</span>. This property is the <strong>divisibility in probability</strong>, and if the property can be extended to <a href="sub-incomplete.html#sub:infinity">infinitely</a> many sums, then the property is known as <strong>infinite divisibility</strong>. Let’s consider four representative distribution families to demonstrate these properties in the following section.</p>
</div>
<div id="sub:divRV" class="section level2">
<h2>
<span class="header-section-number">9.2</span> Examples: Divisibility and Infinite Divisibility of Random Variables</h2>
<p><span class="newthought">Binomial random variable </span></p>
<p>A <em>binomial random variable</em> is a variable associating with binary response variables <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. In any random trial, we can always partition the sample by the successful outcomes and the failure outcomes. The <strong>binomial distribution</strong> arises from the context of a sequence of the <strong>independent</strong> trials or experiments.</p>
<ul>
<li>
<em>Independence</em>: For two event sets <span class="math inline">\(\mathcal{A}\)</span> and <span class="math inline">\(\mathcal{B}\)</span>, if <span class="math inline">\(\mathbb{P}(\mathcal{A}\cap\mathcal{B})=\mathbb{P}(\mathcal{A})\mathbb{P}(\mathcal{B})\)</span>, then two events are <strong>independent</strong>. For two <a href="ch-CalUn.html#sub:rv">random variables</a>, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, if <span class="math inline">\(\mathbb{P}(AB)=\mathbb{P}(A)\mathbb{P}(B)\)</span>, then two variables are <strong>independent</strong>.<label for="tufte-sn-125" class="margin-toggle sidenote-number">125</label><input type="checkbox" id="tufte-sn-125" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">125</span> For random variables <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, let <a href="sub-incomplete.html#sub:beyond2"><span class="math inline">\(\sigma\)</span>-algebras</a> of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be <span class="math inline">\(\sigma(A)\)</span> and <span class="math inline">\(\sigma(B)\)</span>. Then the <strong>independence</strong> of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> means that <span class="math inline">\(\sigma(A)\)</span> and <span class="math inline">\(\sigma(B)\)</span> are independent such that <span class="math inline">\(\mathbb{P}(\mathcal{A}\cap\mathcal{B})=\mathbb{P}(\mathcal{A})\mathbb{P}(\mathcal{B})\)</span> for <span class="math inline">\(\mathcal{A}\in\sigma(A)\)</span> and <span class="math inline">\(\mathcal{B}\in\sigma(B)\)</span>.</span>
</li>
</ul>
<p>For <span class="math inline">\(k\)</span> <strong>independent</strong> trails, we suppose that each trial is successful with probability <span class="math inline">\(p\)</span> and unsuccessful with <span class="math inline">\(1-p\)</span>. Let <span class="math inline">\(Y\)</span> be the number of successes in <span class="math inline">\(k\)</span> independent trials. Then <span class="math inline">\(Y\)</span> is said to have a <strong>binomial distribution</strong> with parameters <span class="math inline">\(k\)</span> and <span class="math inline">\(p\)</span>. When the <a href="ch-CalUn.html#sub:rv">random variable</a> <span class="math inline">\(Y\)</span> is distributed with respect to the binomial distribution with parameters <span class="math inline">\(k\)</span> and <span class="math inline">\(p\)</span>, we write <span class="math inline">\(Y\sim\mbox{binom}(k,p)\)</span>. The probability mass function is <span class="math display">\[\mathbb{P}(Y=y)=\binom{k}{y}p^{y}(1-p)^{k-y},\:\mbox{for }y=0,1,\dots,k.\]</span>
It is straightforward to generate binomial probability mass function and its distribution function by computer. From figure <a href="ch-CalUn.html#fig:bincdf">9.2</a>, we can see that given <span class="math inline">\(k=20\)</span>, the distribution shifts toward the right when <span class="math inline">\(p\)</span> increases from <span class="math inline">\(0.3\)</span> to <span class="math inline">\(0.8\)</span>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:bincdf"></span>
<img src="fig/Part2/bincdf.gif" alt="Binomial distribution function with different parameters" width="100%"><!--
<p class="caption marginnote">-->Figure 9.2: Binomial distribution function with different parameters<!--</p>-->
<!--</div>--></span>
</p>
<div class="solution">
<p class="solution-begin">
R code for binomial distribution function <span id="sol-start-110" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-110', 'sol-start-110')"></span>
</p>
<div id="sol-body-110" class="solution-body" style="display: none;">
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="co"># Create a sample of 20 numbers which are incremented by 1. </span></a>
<a class="sourceLine" id="cb17-2" data-line-number="2">y =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">20</span>,<span class="dt">by =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb17-3" data-line-number="3"><span class="co"># Create the binomial density.</span></a>
<a class="sourceLine" id="cb17-4" data-line-number="4">den1 =<span class="st"> </span><span class="kw">dbinom</span>(y,<span class="dv">20</span>,<span class="fl">0.3</span>); den2 =<span class="st"> </span><span class="kw">dbinom</span>(y,<span class="dv">20</span>,<span class="fl">0.5</span>); den3 =<span class="st"> </span><span class="kw">dbinom</span>(y,<span class="dv">20</span>,<span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb17-5" data-line-number="5"><span class="co"># Create the cumulative distribution function</span></a>
<a class="sourceLine" id="cb17-6" data-line-number="6">cdf1 =<span class="st"> </span><span class="kw">cumsum</span>(den1)<span class="op">/</span><span class="kw">sum</span>(den1); cdf2 =<span class="st"> </span><span class="kw">cumsum</span>(den2)<span class="op">/</span><span class="kw">sum</span>(den2); cdf3 =<span class="st"> </span><span class="kw">cumsum</span>(den3)<span class="op">/</span><span class="kw">sum</span>(den3)</a>
<a class="sourceLine" id="cb17-7" data-line-number="7"></a>
<a class="sourceLine" id="cb17-8" data-line-number="8"><span class="co"># Plot</span></a>
<a class="sourceLine" id="cb17-9" data-line-number="9">pdata =<span class="st"> </span><span class="kw">data.frame</span>(y,den1,cdf1,den2,cdf2,den3,cdf3)</a>
<a class="sourceLine" id="cb17-10" data-line-number="10"><span class="kw">png</span>(<span class="dt">file =</span> <span class="st">"dbinom.png"</span>)</a>
<a class="sourceLine" id="cb17-11" data-line-number="11">fig =<span class="st"> </span><span class="kw">ggplot</span>(pdata, <span class="kw">aes</span>(y, cdf1))</a>
<a class="sourceLine" id="cb17-12" data-line-number="12">fig <span class="op">+</span><span class="st"> </span><span class="kw">geom_col</span>(<span class="kw">aes</span>(y,den1), <span class="dt">fill=</span><span class="st">"skyblue"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(y,cdf1), <span class="dt">color=</span><span class="st">"blue"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">"Y"</span>, <span class="dt">y=</span><span class="st">"Probability"</span>, <span class="dt">title=</span><span class="st">"k=20, p=0.3"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</a>
<a class="sourceLine" id="cb17-13" data-line-number="13"><span class="kw">dev.off</span>()</a></code></pre></div>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Given a fixed <span class="math inline">\(p\)</span>, and let <span class="math inline">\(Y_{1}\sim\mbox{binom}(k,p)\)</span> and <span class="math inline">\(Y_{2}\sim\mbox{binom}(m,p)\)</span>, the sum of two binomial distributions is still in the same family. That is, if <span class="math inline">\(Y_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span> are <strong>independent</strong>, then the <a href="ch-CalUn.html#sub:rv">random variable</a> <span class="math inline">\(Z=Y_{1}+Y_{2}\)</span> also has a <strong>binomial distribution</strong>, specifically <span class="math inline">\(Z\sim\mbox{binom}(k+m,\, p)\)</span>.</p>
<p>To see this result, we need to know that the probability distribution of the sum of two or more independent random variables is the <strong>convolution</strong> of their distributions. Let <span class="math inline">\(Y_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span> be two <strong>independent</strong> <a href="ch-CalUn.html#sub:rv">random variables</a> and let <span class="math inline">\(Z=Y_{1}+Y_{2}\)</span>. If <span class="math inline">\(Y_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span> are <a href="ch-CalUn.html#sub:rv">discrete random variables</a> on <span class="math inline">\((-\infty,\infty)\)</span> with probability mass functions <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>, then the <a href="ch-CalUn.html#sub:rv">distribution</a> of <span class="math inline">\(Z\)</span>
is a <strong>convolution</strong> <span class="math display">\[\mathbb{P}(Z=z)=\sum_{k=-\infty}^{\infty}f(Y_{1}=k)g(Y_{2}=z-k).\]</span>
If <span class="math inline">\(Y_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span> are <a href="ch-CalUn.html#sub:rv">continuous random variables</a> on <span class="math inline">\((-\infty,\infty)\)</span> with densities <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>, then the <a href="ch-CalUn.html#sub:rv">distribution</a> of <span class="math inline">\(Z\)</span>
is<span class="math display">\[\mathbb{P}(z)=\int_{-\infty}^{\infty}f(z-y)g(y)\mbox{d}y=\int_{-\infty}^{\infty}f(y)g(z-y)\mbox{d}y.\]</span>
We often use <span class="math inline">\(\mathbb{P}(z)=(f\star g)(z)\)</span> to denote the <strong>convolution</strong> of <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span>. The analytical derivation of convolution is often non-trivial, because of the sum or the integral of two non-linear functions. However, computer can easily implement the numerical convolution.</p>
<div class="solution">
<p class="solution-begin">
Proof of convolution of two binomial random variables <span id="sol-start-111" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-111', 'sol-start-111')"></span>
</p>
<div id="sol-body-111" class="solution-body" style="display: none;">
<p>Now consider <span class="math inline">\(Z=Y_{1}+Y_{2}\)</span> with <span class="math inline">\(Y_{1}\sim\mbox{binom}(k,p)\)</span> and <span class="math inline">\(Y_{2}\sim\mbox{binom}(m,p)\)</span>. The <strong>convolution</strong> implies <span class="math display">\[\begin{align*} \mathbb{P}(Z=n)=&amp; \sum_{i=0}^{n}\mathbb{P}(Y_{1}=i)\mathbb{P}(Y_{2}=n-i) \\
=&amp;  \sum_{i=0}^{n}\left[\binom{k}{i}p^{i}(1-p)^{k-i}\right]\left[\binom{m}{n-i}p^{n-i}(1-p)^{m-n+i}\right] \\
=&amp;  \binom{k+m}{n}p^{n}(1-p)^{k+m-n} =  \mbox{binom}(k+m,p).\end{align*}\]</span>
where <span class="math inline">\(\sum_{i=0}^{n}\binom{k}{i}\binom{m}{n-i}=\sum_{i=0}^{n}\frac{k!}{i!(k-i)!}\times\frac{m!}{(n-i)!(m-n+i)!}=\binom{k+m}{n}\)</span> is called the Chu–Vandermonde identity.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:binconv"></span>
<img src="fig/Part2/binconv.png" alt="Convolution of binom(20,0.3) and binom(30, 0.3) (points), and binom(50,0.3) (bins)" width="100%"><!--
<p class="caption marginnote">-->Figure 9.3: Convolution of binom(20,0.3) and binom(30, 0.3) (points), and binom(50,0.3) (bins)<!--</p>-->
<!--</div>--></span>
</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1"><span class="co"># Create a sequence of 50 indexes</span></a>
<a class="sourceLine" id="cb18-2" data-line-number="2">y =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">50</span>,<span class="dt">by =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb18-3" data-line-number="3"></a>
<a class="sourceLine" id="cb18-4" data-line-number="4"><span class="co"># Create the binomial density.</span></a>
<a class="sourceLine" id="cb18-5" data-line-number="5">den1 =<span class="st"> </span><span class="kw">dbinom</span>(y,<span class="dv">20</span>,<span class="fl">0.3</span>)</a>
<a class="sourceLine" id="cb18-6" data-line-number="6">den2 =<span class="st"> </span><span class="kw">dbinom</span>(y,<span class="dv">30</span>,<span class="fl">0.3</span>)</a>
<a class="sourceLine" id="cb18-7" data-line-number="7">den3 =<span class="st"> </span><span class="kw">dbinom</span>(y,<span class="dv">50</span>,<span class="fl">0.3</span>)</a>
<a class="sourceLine" id="cb18-8" data-line-number="8"></a>
<a class="sourceLine" id="cb18-9" data-line-number="9"><span class="co"># Convolution</span></a>
<a class="sourceLine" id="cb18-10" data-line-number="10">conv=<span class="kw">convolve</span>(den1,den2, <span class="dt">conj=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb18-11" data-line-number="11">pdata =<span class="st"> </span><span class="kw">data.frame</span>(y,den3,conv)</a>
<a class="sourceLine" id="cb18-12" data-line-number="12">fig =<span class="st"> </span><span class="kw">ggplot</span>(pdata, <span class="kw">aes</span>(y, den3))</a>
<a class="sourceLine" id="cb18-13" data-line-number="13">fig <span class="op">+</span><span class="st"> </span><span class="kw">geom_col</span>(<span class="dt">fill=</span><span class="st">"skyblue"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(y,conv),<span class="dt">color=</span><span class="st">"blue"</span>) <span class="op">+</span><span class="st">  </span><span class="kw">theme_minimal</span>()</a></code></pre></div>
<ul>
<li>
<em>Divisibility</em> and <em>infinite divisibility</em>: <strong>Divisibility</strong> of a random variable <span class="math inline">\(Z\)</span> means that <span class="math inline">\(Z\)</span> can be divided into <strong>independent</strong> parts that have the same distribution. A probability distribution <span class="math inline">\(F\)</span> is <strong>infinitely divisible</strong>, if for every positive integer <span class="math inline">\(n\)</span>, there exist <span class="math inline">\(n\)</span>
<em>independent identically distributed</em> (i.i.d.) <a href="ch-CalUn.html#sub:rv">random variables</a> <span class="math inline">\(Y_{1},\dots,Y_{n}\)</span> whose sum <span class="math inline">\(Z=Y_{1}+\cdots+Y_{n}\)</span> has the distribution <span class="math inline">\(F\)</span>.<label for="tufte-sn-126" class="margin-toggle sidenote-number">126</label><input type="checkbox" id="tufte-sn-126" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">126</span> 
The concept of infinite divisibility was introduced to probability in 1929, and later, it was developed in <span class="citation">Kolmogorov (<a href="bibliography.html#ref-Kolmogorov1933">1933</a>)</span>.</span>
</li>
</ul>
<p>According to the definition, if <span class="math inline">\(Z\sim F\)</span> and <span class="math inline">\(Y_{i}\sim F_{n}\)</span> for <span class="math inline">\(i=1,\dots,n\)</span>, then <span class="math display">\[F=\underset{n\mbox{-fold convolution}}{\underbrace{F_{n}\star F_{n}\cdots\star F_{n}}}.\]</span>
<strong>Infinite divisibility</strong> of distributions is preserved under <strong>convolution</strong>.</p>
<p>We can see that <strong>binomial distribution</strong> is <strong>divisible</strong>. A <span class="math inline">\(\mbox{binom}(3,p)\)</span> distribution can be divided into three independent and identical <span class="math inline">\(\mbox{binom}(1,p)\)</span> which is also called the <em>Bernoulli distribution</em>. However, there is no further way of dividing <span class="math inline">\(\mbox{binom}(1,p)\)</span> because <span class="math inline">\(\mbox{binom}(1,p)\)</span> is the non-divisible “atom” of this distribution family. Thus, binomial distribution is not infinitely divisible. But infinitely divisible property holds for some kind of limit version of binomial distribution, which is called <strong>Poisson distribution</strong>.</p>
<p><span class="newthought">Poisson random variable </span></p>
<p>The <strong>Poisson distribution</strong> is used as a model for rare events and events occurring at random over time or space such as traffic accidents, phone calls, bankrupt, or annual deaths by horse kicks in the Prussian army from 1875-1894, etc.. A <em>Poisson distribution</em> comes with the paramter <span class="math inline">\(\lambda\in[0,\infty)\)</span> that stands for the occuring rate of those events. Let <span class="math inline">\(Y\sim\mbox{Poi}(\lambda)\)</span>. The <a href="ch-CalUn.html#sub:rv">probability mass function</a> is
<span class="math display">\[\mathbb{P}(Y=y)=\mbox{e}^{-\lambda}\frac{\lambda^{y}}{y!},\; y=1,2,\dots.\]</span>
If <span class="math inline">\(Y_{1}\sim\mbox{Poi}(\lambda_{1})\)</span> and <span class="math inline">\(Y_{2}\sim\mbox{Poi}(\lambda_{2})\)</span>, then <span class="math inline">\(Y_{1}+Y_{2}\sim\mbox{Poi}(\lambda_{1}+\lambda_{2})\)</span> which means Poisson distribution is <strong>divisible</strong>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:poicdf"></span>
<img src="fig/Part2/poicdf.gif" alt="Poisson distribution function with different parameters" width="100%"><!--
<p class="caption marginnote">-->Figure 9.4: Poisson distribution function with different parameters<!--</p>-->
<!--</div>--></span>
</p>
<div class="solution">
<p class="solution-begin">
R code for Poisson distribution function <span id="sol-start-112" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-112', 'sol-start-112')"></span>
</p>
<div id="sol-body-112" class="solution-body" style="display: none;">
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1"><span class="co"># Create a sample of 20 numbers which are incremented by 1. </span></a>
<a class="sourceLine" id="cb19-2" data-line-number="2">y =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">20</span>,<span class="dt">by =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb19-3" data-line-number="3"><span class="co"># Create the binomial density.</span></a>
<a class="sourceLine" id="cb19-4" data-line-number="4">den1 =<span class="st"> </span><span class="kw">dpois</span>(y,<span class="fl">0.5</span>); den2 =<span class="st"> </span><span class="kw">dpois</span>(y,<span class="fl">1.5</span>); den3 =<span class="st"> </span><span class="kw">dpois</span>(y,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb19-5" data-line-number="5"><span class="co"># Create the cumulative distribution function</span></a>
<a class="sourceLine" id="cb19-6" data-line-number="6">cdf1 =<span class="st"> </span><span class="kw">cumsum</span>(den1)<span class="op">/</span><span class="kw">sum</span>(den1); cdf2 =<span class="st"> </span><span class="kw">cumsum</span>(den2)<span class="op">/</span><span class="kw">sum</span>(den2); cdf3 =<span class="st"> </span><span class="kw">cumsum</span>(den3)<span class="op">/</span><span class="kw">sum</span>(den3)</a>
<a class="sourceLine" id="cb19-7" data-line-number="7">pdata =<span class="st"> </span><span class="kw">data.frame</span>(y,den1,cdf1,den2,cdf2,den3,cdf3)</a>
<a class="sourceLine" id="cb19-8" data-line-number="8"></a>
<a class="sourceLine" id="cb19-9" data-line-number="9"><span class="co"># Plot the figure </span></a>
<a class="sourceLine" id="cb19-10" data-line-number="10"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb19-11" data-line-number="11"><span class="kw">png</span>(<span class="dt">file =</span> <span class="st">"dpois.png"</span>)</a>
<a class="sourceLine" id="cb19-12" data-line-number="12">fig =<span class="st"> </span><span class="kw">ggplot</span>(pdata, <span class="kw">aes</span>(y, cdf1))</a>
<a class="sourceLine" id="cb19-13" data-line-number="13">fig <span class="op">+</span><span class="st"> </span><span class="kw">geom_col</span>(<span class="kw">aes</span>(y,den1), <span class="dt">fill=</span><span class="st">"skyblue"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_step</span>(<span class="kw">aes</span>(y,cdf1), <span class="dt">color=</span><span class="st">"blue"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">"Y"</span>, <span class="dt">y=</span><span class="st">"Probability"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</a>
<a class="sourceLine" id="cb19-14" data-line-number="14"><span class="kw">dev.off</span>()</a></code></pre></div>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>We can view Poisson distribution as a limit of the binomial distribution. Suppose <span class="math inline">\(Y\sim\mbox{binom}(k,p)\)</span> and suppose for some fixed <span class="math inline">\(\lambda\)</span>, we have <span class="math inline">\(p=\lambda/k\)</span>, then when <span class="math inline">\(k\rightarrow\infty\)</span>, the distribution of <span class="math inline">\(Y\)</span> approximates to <span class="math inline">\(\mbox{Poi}(\lambda)\)</span>.</p>
<div class="solution">
<p class="solution-begin">
Proof of the limit of the sum of binomial random variables <span id="sol-start-113" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-113', 'sol-start-113')"></span>
</p>
<div id="sol-body-113" class="solution-body" style="display: none;">
<p><span class="math display">\[\begin{align*}\mathbb{P}(Y=y) &amp;=\binom{k}{y}p^{y}(1-p)^{k-y}=\binom{k}{y}\left(\frac{\lambda}{k}\right)^{y}\left(1-\frac{\lambda}{k}\right)^{k-y}\\
&amp;=\frac{k(k-1)\cdots(k-y+1)\lambda^{y}}{y!\times k^{y}}\left(1-\frac{\lambda}{k}\right)^{k}\left(1-\frac{\lambda}{k}\right)^{-y}\\
&amp;\overset{(a)}{=} 1\cdot\frac{k-1}{k}\cdots\frac{k-y+1}{k}\cdot\frac{\lambda^{y}}{y!}\left(1-\frac{\lambda}{k}\right)^{k}\left(1-\frac{\lambda}{k}\right)^{-y}\\
&amp; \rightarrow1\cdot1\cdots1\frac{\lambda^{y}}{y!}\mbox{e}^{-\lambda}\cdot1 \sim \mbox{Poi}(\lambda).\end{align*}\]</span>
Note that we use the definition <span class="math inline">\(\lim_{k\rightarrow\infty}(1-\frac{\lambda}{k})^{k}=\mbox{e}^{-\lambda}\)</span> in the equality <span class="math inline">\(\overset{(a)}{=}\)</span>.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>By this property, we can see that <strong>Poisson family</strong> is <strong>infinite divisible</strong>. That is, for any <span class="math inline">\(n\in\mathbb{N}\)</span>, a <span class="math inline">\(\mbox{Poi}(\lambda)\)</span> distribution can be divided into <span class="math inline">\(n\)</span>
<strong>independent</strong> <span class="math inline">\(\mbox{Poi}(\lambda/n)\)</span>.</p>
<p><span class="newthought">Normal (Gaussian) random variable </span></p>
<p>Both the <strong>binomial</strong> and <strong>Poisson</strong> families are for <strong>discrete random variables</strong>. The most common <strong>continuous distribution</strong> is the <strong>normal</strong> (or <strong>Gaussian</strong>) distribution. The importance of the normal distribution comes from the <a href="ch-CalUn.html#sub:ex">central limit theorem</a> (see section <a href="ch-CalUn.html#sub:ex">9.3</a>), which tells us that when you take the average of a sufficiently large <strong>independent identical distributed</strong> (i.i.d.) sample, the distribution of the sample averages looks like that of a <strong>normal random variable</strong>. The normal distribution is also commonly used to model measurement errors. The normal density is characterized by two parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>
<span class="math display">\[f(y)=\frac{1}{\sqrt{2\pi\sigma^{2}}}\mbox{e}^{-\frac{(y-\mu)^{2}}{2\sigma^{2}}},\; y\in\mathbb{R}.\]</span>
When <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma=1\)</span>, <span class="math inline">\(Y\sim\mathcal{N}(0,1)\)</span> follows the <em>standard normal distribution</em>. Here are some useful facts:</p>
<ol style="list-style-type: decimal">
<li><p>If <span class="math inline">\(Y\sim\mathcal{N}(\mu,\sigma^{2})\)</span>, then <span class="math inline">\(Z=(Y-\mu)/\sigma\)</span> is a <strong>standard normal distribution</strong>, namely <span class="math inline">\(Z\sim\mathcal{N}(0,1)\)</span>.</p></li>
<li><p>If <span class="math inline">\(Y\sim\mathcal{N}(0,1)\)</span>, then <span class="math inline">\(Z=\mu+\sigma Y\)</span> follows the normal distribution <span class="math inline">\(\mathcal{N}(\mu,\sigma^{2})\)</span>.</p></li>
<li><p>If <span class="math inline">\(Y_{i}\sim\mathcal{N}(\mu_{i},\sigma_{i}^{2})\)</span> are <strong>independent</strong> for <span class="math inline">\(i=1,\dots,n\)</span>, then <span class="math display">\[\sum_{i=1}^{n}Y_{i}\sim\mathcal{N}\left(\sum_{i=1}^{n}\mu_{i},\:\sum_{i=1}^{n}\sigma_{i}^{2}\right).\]</span></p></li>
</ol>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:normcdf"></span>
<img src="fig/Part2/normcdf.gif" alt="Normal distribution function with different parameters" width="100%"><!--
<p class="caption marginnote">-->Figure 9.5: Normal distribution function with different parameters<!--</p>-->
<!--</div>--></span>
</p>
<div class="solution">
<p class="solution-begin">
R code for normal distribution function <span id="sol-start-114" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-114', 'sol-start-114')"></span>
</p>
<div id="sol-body-114" class="solution-body" style="display: none;">
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" data-line-number="1"><span class="co"># Create a sequence of numbers between -10 and 10 incrementing by 0.5. </span></a>
<a class="sourceLine" id="cb20-2" data-line-number="2">y =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dt">by =</span> <span class="fl">.5</span>)</a>
<a class="sourceLine" id="cb20-3" data-line-number="3"><span class="co"># Choose the mean and standard deviation. </span></a>
<a class="sourceLine" id="cb20-4" data-line-number="4">den1 =<span class="st"> </span><span class="kw">dnorm</span>(y, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>); den2 =<span class="st"> </span><span class="kw">dnorm</span>(y, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb20-5" data-line-number="5">den3 =<span class="st"> </span><span class="kw">dnorm</span>(y, <span class="dt">mean =</span> <span class="dv">2</span>, <span class="dt">sd =</span> <span class="dv">2</span>); den4 =<span class="st"> </span><span class="kw">dnorm</span>(y, <span class="dt">mean =</span> <span class="dv">2</span>, <span class="dt">sd =</span> <span class="dv">1</span>) </a>
<a class="sourceLine" id="cb20-6" data-line-number="6">cdf1 =<span class="st"> </span><span class="kw">cumsum</span>(den1)<span class="op">/</span><span class="kw">sum</span>(den1); cdf2 =<span class="st"> </span><span class="kw">cumsum</span>(den2)<span class="op">/</span><span class="kw">sum</span>(den2)</a>
<a class="sourceLine" id="cb20-7" data-line-number="7">cdf3 =<span class="st"> </span><span class="kw">cumsum</span>(den3)<span class="op">/</span><span class="kw">sum</span>(den3); cdf4 =<span class="st"> </span><span class="kw">cumsum</span>(den4)<span class="op">/</span><span class="kw">sum</span>(den4)</a>
<a class="sourceLine" id="cb20-8" data-line-number="8"></a>
<a class="sourceLine" id="cb20-9" data-line-number="9">pdata =<span class="st"> </span><span class="kw">data.frame</span>(y,den1,cdf1,den2,cdf2,den3,cdf3,den4,cdf4)</a>
<a class="sourceLine" id="cb20-10" data-line-number="10"><span class="kw">png</span>(<span class="dt">file =</span> <span class="st">"dnorm1.png"</span>)</a>
<a class="sourceLine" id="cb20-11" data-line-number="11">fig =<span class="st"> </span><span class="kw">ggplot</span>(pdata, <span class="kw">aes</span>(y, cdf1))</a>
<a class="sourceLine" id="cb20-12" data-line-number="12">fig <span class="op">+</span><span class="st"> </span><span class="kw">geom_area</span>(<span class="kw">aes</span>(y,den1), <span class="dt">fill=</span><span class="st">"skyblue"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(y,cdf1), <span class="dt">color=</span><span class="st">"blue"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">"Y"</span>, <span class="dt">y=</span><span class="st">"Probability"</span>) <span class="op">+</span><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">"Y"</span>, <span class="dt">y=</span><span class="st">"Probability"</span>, <span class="dt">title=</span><span class="st">"mu = 0, sigma = 1"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</a>
<a class="sourceLine" id="cb20-13" data-line-number="13"><span class="kw">dev.off</span>()</a></code></pre></div>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>The last fact is from the <strong>infinite divisibility</strong> of the normal random variables. The calculation of proving this argument is more demanding than those in the previous cases. However, we can verify this result experimentally. Using random numbers on a computer to simulate probabilities is called the <em>Monte Carlo method</em>.<label for="tufte-sn-127" class="margin-toggle sidenote-number">127</label><input type="checkbox" id="tufte-sn-127" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">127</span> It was named after the famous Monte Carlo casino in Monaco, although the method first appeared in the Manhattan project.</span> The procedure of this method is as follows. Design a random experiment such that the event  happens in this experiment following the probability law <span class="math inline">\(\mathbb{P}(\cdot)\)</span>. This designed event is called <em>simulated event</em>. Then, the probability <span class="math inline">\(\mathbb{P}(\mathcal{A})\)</span> can be estimated by repeating this random experiment many times and computing the proportion of times that <span class="math inline">\(\mathcal{A}\)</span> occurs.<label for="tufte-sn-128" class="margin-toggle sidenote-number">128</label><input type="checkbox" id="tufte-sn-128" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">128</span> For a rigorous justification of this step, please see sec[?]. </span> <strong>Monte Carlo simulation</strong> is intuitive and matches up with our sense of how probabilities “should” behave.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:simNorm"></span>
<img src="fig/Part2/simNorm.png" alt="Comparison of simulated normal distribution (bins) and the analytical one (curve) " width="100%"><!--
<p class="caption marginnote">-->Figure 9.6: Comparison of simulated normal distribution (bins) and the analytical one (curve) <!--</p>-->
<!--</div>--></span>
</p>
<div class="solution">
<p class="solution-begin">
R code for simulating the normal distributions
</p>
<div class="solution-body">
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" data-line-number="1"><span class="co"># Simulate 10000 times, each simulated event follows N(1,1) </span></a>
<a class="sourceLine" id="cb21-2" data-line-number="2">y1 =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">10000</span>, <span class="dt">mean=</span><span class="dv">1</span>, <span class="dt">sd=</span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb21-3" data-line-number="3"><span class="co"># Simulate 10000 times, each simulated event follows N(1,2) </span></a>
<a class="sourceLine" id="cb21-4" data-line-number="4">y2 =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">10000</span>, <span class="dt">mean=</span><span class="dv">1</span>, <span class="dt">sd=</span><span class="dv">2</span>) </a>
<a class="sourceLine" id="cb21-5" data-line-number="5"></a>
<a class="sourceLine" id="cb21-6" data-line-number="6">z =<span class="st"> </span>y1<span class="op">+</span>y2 <span class="co"># mu is 2, sigma is 1^2+ 2^2 = 5</span></a>
<a class="sourceLine" id="cb21-7" data-line-number="7"></a>
<a class="sourceLine" id="cb21-8" data-line-number="8"><span class="co"># Create N(2, 5) density function</span></a>
<a class="sourceLine" id="cb21-9" data-line-number="9">f =<span class="st"> </span><span class="cf">function</span>(y) <span class="kw">exp</span>(<span class="op">-</span>(y<span class="dv">-2</span>)<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="dv">10</span>)<span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">10</span><span class="op">*</span>pi) </a>
<a class="sourceLine" id="cb21-10" data-line-number="10"><span class="co"># Create the domain of the density function </span></a>
<a class="sourceLine" id="cb21-11" data-line-number="11">y =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">14</span>, <span class="dt">length.out=</span><span class="dv">10000</span>) </a>
<a class="sourceLine" id="cb21-12" data-line-number="12"><span class="co"># Density function on [-10, 14]</span></a>
<a class="sourceLine" id="cb21-13" data-line-number="13">fy =<span class="st"> </span><span class="kw">f</span>(y) </a>
<a class="sourceLine" id="cb21-14" data-line-number="14"></a>
<a class="sourceLine" id="cb21-15" data-line-number="15"><span class="co"># Plot</span></a>
<a class="sourceLine" id="cb21-16" data-line-number="16"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb21-17" data-line-number="17">pdata =<span class="st"> </span><span class="kw">data.frame</span>(y,fy,z)</a>
<a class="sourceLine" id="cb21-18" data-line-number="18">fig =<span class="st"> </span><span class="kw">ggplot</span>(pdata, <span class="kw">aes</span>(y, fy))</a>
<a class="sourceLine" id="cb21-19" data-line-number="19">fig <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(z,<span class="dt">y=</span>..density..), <span class="dt">fill=</span><span class="st">"skyblue"</span>, <span class="dt">bins=</span><span class="dv">200</span>,<span class="dt">alpha=</span><span class="fl">0.8</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(y,fy), <span class="dt">color=</span><span class="st">"blue"</span>)   <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</a></code></pre></div>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p><span class="newthought">Gamma random variable </span></p>
<p>The distributions of Poisson and normal random variables turn out to be the building blocks of general infinitely divisible distributions. Besides these two, the Gamma distribution can construct several important subclasses of infinitely divisible distributions. See ch[?].</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:gammacdf"></span>
<img src="fig/Part2/gammacdf.gif" alt="Gamma distribution function with different parameters" width="100%"><!--
<p class="caption marginnote">-->Figure 9.7: Gamma distribution function with different parameters<!--</p>-->
<!--</div>--></span>
</p>
<div class="solution">
<p class="solution-begin">
R code for Gamma distribution function <span id="sol-start-116" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-116', 'sol-start-116')"></span>
</p>
<div id="sol-body-116" class="solution-body" style="display: none;">
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1"><span class="co"># Create a sequence of numbers between 0 and 10 incrementing by 0.2. </span></a>
<a class="sourceLine" id="cb22-2" data-line-number="2">y =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dt">by =</span> <span class="fl">.2</span>)</a>
<a class="sourceLine" id="cb22-3" data-line-number="3"><span class="co"># Choose the shape paramter (alpha) and rate paramter (beta). </span></a>
<a class="sourceLine" id="cb22-4" data-line-number="4">den1 =<span class="st"> </span><span class="kw">dgamma</span>(y, <span class="dt">shape =</span> <span class="dv">1</span>, <span class="dt">rate =</span> <span class="fl">0.5</span>); den2 =<span class="st"> </span><span class="kw">dgamma</span>(y, <span class="dt">shape =</span> <span class="dv">3</span>, <span class="dt">rate =</span> <span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb22-5" data-line-number="5">den3 =<span class="st"> </span><span class="kw">dgamma</span>(y, <span class="dt">shape =</span> <span class="dv">3</span>, <span class="dt">rate =</span> <span class="dv">1</span>); den4 =<span class="st"> </span><span class="kw">dgamma</span>(y, <span class="dt">shape =</span> <span class="dv">1</span>, <span class="dt">rate =</span> <span class="dv">1</span>) </a>
<a class="sourceLine" id="cb22-6" data-line-number="6">cdf1 =<span class="st"> </span><span class="kw">cumsum</span>(den1)<span class="op">/</span><span class="kw">sum</span>(den1); cdf2 =<span class="st"> </span><span class="kw">cumsum</span>(den2)<span class="op">/</span><span class="kw">sum</span>(den2)</a>
<a class="sourceLine" id="cb22-7" data-line-number="7">cdf3 =<span class="st"> </span><span class="kw">cumsum</span>(den3)<span class="op">/</span><span class="kw">sum</span>(den3); cdf4 =<span class="st"> </span><span class="kw">cumsum</span>(den4)<span class="op">/</span><span class="kw">sum</span>(den4)</a>
<a class="sourceLine" id="cb22-8" data-line-number="8">pdata =<span class="st"> </span><span class="kw">data.frame</span>(y,den1,cdf1,den2,cdf2,den3,cdf3,den4,cdf4)</a>
<a class="sourceLine" id="cb22-9" data-line-number="9"></a>
<a class="sourceLine" id="cb22-10" data-line-number="10"><span class="co"># Plot</span></a>
<a class="sourceLine" id="cb22-11" data-line-number="11"><span class="kw">png</span>(<span class="dt">file =</span> <span class="st">"dgamma1.png"</span>)</a>
<a class="sourceLine" id="cb22-12" data-line-number="12">fig =<span class="st"> </span><span class="kw">ggplot</span>(pdata, <span class="kw">aes</span>(y, cdf1))</a>
<a class="sourceLine" id="cb22-13" data-line-number="13">fig <span class="op">+</span><span class="st"> </span><span class="kw">geom_area</span>(<span class="kw">aes</span>(y,den1), <span class="dt">fill=</span><span class="st">"skyblue"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(y,cdf1), <span class="dt">color=</span><span class="st">"blue"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">"Y"</span>, <span class="dt">y=</span><span class="st">"Probability"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">"Y"</span>, <span class="dt">y=</span><span class="st">"Probability"</span>, <span class="dt">title=</span><span class="st">"shape = 1, rate = 0.5"</span>)  <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</a>
<a class="sourceLine" id="cb22-14" data-line-number="14"><span class="kw">dev.off</span>()</a></code></pre></div>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>The <strong>Gamma distribution</strong> is a family of positive, <a href="ch-CalUn.html#sub:rv">continuous distributions</a> with two parameters. The density curve can take a wide variety of shapes, which allows the distribution to be useful for modeling variables that exhibit skewed and nonsymmetric behavior, for example, income distribution. The <em>Gamma distribution</em> is defined as follows <span class="math display">\[f(y)=\frac{1}{\beta^{\alpha}\Gamma(\alpha)}y^{\alpha-1}\mbox{e}^{-\frac{y}{\beta}},\; y&gt;0\]</span>
where the <em>Gamma function</em> <span class="math inline">\(\Gamma(\alpha)=\int_{0}^{\infty}y^{\alpha-1}e^{-y}\mbox{d}y\)</span> satisfies <span class="math inline">\(\Gamma(\alpha)=(\alpha-1)\Gamma(\alpha-1)\)</span>. Thus <span class="math inline">\(\Gamma(\alpha)\)</span> is the continuous counterpart
of <a href="sub-inferknow.html#sub:dyn">factorials</a>. That is, when <span class="math inline">\(\alpha\in\mathbb{N}\)</span>, <span class="math inline">\(\Gamma(\alpha)=\alpha!\)</span>.
Gamma distribution is also infinitely divisible. The parameter <span class="math inline">\(\alpha\)</span> is called the shape parameter since the shape of the distribution function differs for different values of <span class="math inline">\(\alpha\)</span>. And the parameter <span class="math inline">\(\beta\)</span> is called the scale parameter since it scales the shape of the distribution function horizontally and vertically.</p>
</div>
<div id="sub:ex" class="section level2">
<h2>
<span class="header-section-number">9.3</span> Expectation</h2>
<p>The <strong>expectation</strong> of a <a href="ch-CalUn.html#sub:rv">random variable</a> <span class="math inline">\(Y\)</span> is to deliver the information about the center of <span class="math inline">\(Y\)</span>’s probability function. The <em>expected value</em>, <em>the mean</em>, and <em>the first moment</em>, these are different names for the expectation. The <em>expectation</em> of <span class="math inline">\(Y\)</span> is defined to be<label for="tufte-sn-129" class="margin-toggle sidenote-number">129</label><input type="checkbox" id="tufte-sn-129" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">129</span> The <a href="sub-incomplete.html#sub:fdim">additivity</a> in <span class="math inline">\(\sigma\)</span>-algebra (countable unions of pairwise disjoint sets) implies the <a href="sub-continuity.html#sub:continuousFunc">continuity</a> properties of a probability measure: <br>
- <em>continuity from below</em>: <span class="math inline">\(\lim_{i\rightarrow\infty}\mathbb{P}(\mathcal{A}_{i})=\mathbb{P}\left(\cup_{i=1}^{\infty}\mathcal{A}_{i}\right)\)</span> for <span class="math inline">\(\mathcal{A}_{1}\subset\mathcal{A}_{2}\subset\cdots\)</span>.
<br>
- <em>continuity from above</em>: <span class="math inline">\(\lim_{i\rightarrow\infty}\mathbb{P}(\mathcal{A}_{i})=\mathbb{P}\left(\cap_{i=1}^{\infty}\mathcal{A}_{i}\right)\)</span> for <span class="math inline">\(\mathcal{A}_{1}\supset\mathcal{A}_{2}\supset\cdots\)</span>. <br>
These continuity properties are essential for an well-defined integral (expectation) with respect to the probability law <span class="math inline">\(\mathbb{P}\)</span>.</span><span class="math display">\[\mathbb{E}[Y]=\mu_{Y}=\int y\mathbb{P}(\mbox{d}y)=\int y\mbox{d}F(y)=\begin{cases}
\sum_{y}yf(y) &amp; \mbox{for discrete }Y,\\
\int yf(y)\mbox{d}y &amp; \mbox{for continuous }Y,
\end{cases}\]</span>
where <span class="math inline">\(F\)</span> is the <a href="ch-CalUn.html#sub:rv">distribution function</a>, <span class="math inline">\(f\)</span> is the <a href="ch-CalUn.html#sub:rv">probability mass function</a> in the discrete case and the <a href="ch-CalUn.html#sub:rv">density function</a> in the continuous case.<label for="tufte-sn-130" class="margin-toggle sidenote-number">130</label><input type="checkbox" id="tufte-sn-130" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">130</span> We assume the sum and the integral are well-defined so that the range of <span class="math inline">\(Y\)</span> is bounded, namely <span class="math inline">\(\sum|y|f(y)&lt;\infty\)</span> or <span class="math inline">\(\int|y|f(y)\mbox{d}y&lt;\infty\)</span>. The range of <span class="math inline">\(Y\)</span> is the set of values it can take.</span> The expression <span class="math inline">\(\int y\mathbb{P}(\mbox{d}y)\)</span> or <span class="math inline">\(\int y\mbox{d}F(y)\)</span> is for the general case. If the desnity <span class="math inline">\(f\)</span> or the probability mass function <span class="math inline">\(f\)</span> exists, defining expectation via the integral or the sum becomes feasible. The differential <span class="math inline">\(\mathbb{P}(\mbox{d}y)\)</span> or <span class="math inline">\(\mbox{d}F(y)\)</span> means the infinitesimal change of the probability caused by the infinitesimal change of <span class="math inline">\(Y\)</span>.</p>
<p>For any constant value <span class="math inline">\(c\)</span>, the expectation is itself <span class="math inline">\(\mathbb{E}[c]=c\)</span>. Notice that the expectation is a deterministic summary of the random variable. It is analogous to the average of the deterministic numbers. In other words, the expectation is a weighted average of the values of the random variable, while the weights come from the corresponding probability of the random variable. For example, when <span class="math inline">\(Y\)</span> is <a href="ch-CalUn.html#sub:rv">uniformly distributed</a> on discrete <span class="math inline">\(n\)</span> points, we have <span class="math inline">\(n\)</span>
deterministic outcomes with equal weights <span class="math inline">\(1/n\)</span>. Then the expectation of <span class="math inline">\(Y\)</span>
is the average <span class="math inline">\(n\)</span>-values, namely <span class="math inline">\(\sum_{i=1}^{n}y_{i}/n\)</span>. If <span class="math inline">\(Y\)</span> follows a specific distribution, for example, <span class="math inline">\(Y\sim\mbox{Poi}(\lambda)\)</span>, then the expectation can be computed by the formula in the definition <span class="math display">\[\mathbb{E}[Y]=\sum_{n=0}^{\infty}k\mathbb{P}(Y=k)=\sum_{k=0}^{\infty}k\frac{\mbox{e}^{-\lambda}\lambda^{k}}{k!}=\lambda\mbox{e}^{-\lambda}\sum_{k=1}^{\infty}\frac{\lambda^{k-1}}{(k-1)!}=\lambda\mbox{e}^{-\lambda}\mbox{e}^{\lambda}=\lambda\]</span>
where <span class="math inline">\(\sum_{k=1}^{\infty}\frac{\lambda^{k-1}}{(k-1)!}=\sum_{k=0}^{\infty}\frac{\lambda^{k}}{k!}=\mbox{e}^{\lambda}\)</span> by the definition of <a href="sub-inferknow.html#sub:dyn">exponential function</a>.</p>
<p>For a continuous random variable, when its density <span class="math inline">\(f\)</span> exists, the expectation is essentially an integral. The properties of integrations mentioned in chapter <a href="sub-calculus.html#sub:diffInt">7.1</a> are also available for the expectation operation. For example, the linear property still holds. That is, for constants <span class="math inline">\(a_{1},\dots,a_{n}\)</span>, the <strong>expectation</strong> (integration) operator for the sum can be taken for individual variables of the sum <span class="math display">\[\mathbb{E}\left[\sum_{i=1}^{n}a_{i}Y_{i}\right]=\sum_{i=1}^{n}a_{i}\mathbb{E}\left[Y_{i}\right].\]</span>
In addition, if <span class="math inline">\(Y_{1},\dots,Y_{n}\)</span> are <a href="ch-CalUn.html#sub:rv">independent</a> random variables, then such property can be extended to the multiplication: <span class="math display">\[\mathbb{E}\left[\prod_{i=1}^{n}Y_{i}\right]=\prod_{i=1}^{n}\mathbb{E}\left[Y_{i}\right].\]</span></p>
<div class="solution">
<p class="solution-begin">
Proof
</p>
<div class="solution-body">
<p>By the <a href="ch-CalUn.html#sub:rv">independent</a> property of <span class="math inline">\(\mathbb{P}\)</span>, we have
<span class="math display">\[\begin{align*} \mathbb{E}\left[\prod_{i=1}^{n}Y_{i}\right]    &amp;=\int\prod_{i=1}^{n}y_{i}\mathbb{P}(\mbox{d}y_{1}\cdots\mbox{d}y_{n})\\
    &amp;=\int\prod_{i=1}^{n}y_{i}\mathbb{P}(\mbox{d}y_{1})\cdots\mathbb{P}(\mbox{d}y_{n})\\
    &amp;=\int y_{1}\mathbb{P}(\mbox{d}y_{1})\cdots\int y_{n}\mathbb{P}(\mbox{d}y_{n})=\prod_{i=1}^{n}\mathbb{E}\left[Y_{i}\right].\end{align*}\]</span></p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>One interpretation between the <a href="sub-incomplete.html#sub:beyond2">probability</a> and the <a href="ch-CalUn.html#sub:rv">random variable</a> can be established via the expectation of an <a href="sub-set-theory.html#sub:func">indicator function</a>. Let the indicator function <span class="math inline">\(\mathbf{1}_{\mathcal{Y}}(Y)\)</span> denote whether the value of random variable <span class="math inline">\(Y\)</span> belongs to the set <span class="math inline">\(\mathcal{Y}\)</span>. The expectation of this indicator function gives the probability of <span class="math inline">\(Y\)</span> <span class="math display">\[\mathbb{E}\left[\mathbf{1}_{\mathcal{Y}}(Y)\right]=\int\mathbf{1}_{\mathcal{Y}}(y)\mathbb{P}(\mbox{d}y)=\int_{\mathcal{Y}}\mathbb{P}(\mbox{d}y)=\mathbb{P}(Y\in\mathcal{Y}).\]</span>
For a continuous function <span class="math inline">\(g\)</span>, the expectation <span class="math inline">\(\mathbb{E}[g(Y)]\)</span> with random variable <span class="math inline">\(Y(x)\)</span> defined on a probability space <span class="math inline">\((\mathcal{X},\sigma(\mathcal{X}),\mathbb{P})\)</span> is<label for="tufte-sn-131" class="margin-toggle sidenote-number">131</label><input type="checkbox" id="tufte-sn-131" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">131</span> One can also view <span class="math inline">\(g(Y)\)</span> as a <a href="ch-CalUn.html#sub:rv">random variable</a>. Let <span class="math inline">\(Z=g(Y)\)</span>. For the discrete case, it is easy to verify, <span class="math display">\[\begin{align*}\sum z\mathbb{P}(Z=z)    =&amp; \sum z\sum_{\{y:z=g(y)\}}f(y)\\
=&amp; \sum_{z}\sum_{\{y:z=g(y)\}}g(y)f(y)\\=&amp;\sum_{y}g(y)f(y)\end{align*}\]</span>
which means <span class="math inline">\(\mathbb{E}[Z]=\mathbb{E}[g(Y)]\)</span> - the frist expectation is taken under the probability law of <span class="math inline">\(Z\)</span>
and the second is under that of <span class="math inline">\(Y\)</span>. This result also holds in the continous case.</span> <span class="math display">\[\mathbb{E}[g(Y)]=\int_{\mathcal{X}}g(Y(x))\mathbb{P}(\mbox{d}x)=\int_{\mathbb{R}}g(y)\mathbb{P}(\mbox{d}y)\]</span>
where the infinitesimals <span class="math inline">\(\mbox{d}x\)</span> and <span class="math inline">\(\mbox{d}y\)</span> should satisfy
<span class="math display">\[\mathbb{P}\left\{ \mbox{d}x\in\mathcal{X}:\; Y(\mbox{d}x)\in\mathcal{Y}\right\} =\mathbb{P}(\mbox{d}y\in\mathcal{Y}).\]</span>
In general, it is not true that <span class="math inline">\(\mathbb{E}[g(Y)]=g(\mathbb{E}[Y])\)</span>.<label for="tufte-sn-132" class="margin-toggle sidenote-number">132</label><input type="checkbox" id="tufte-sn-132" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">132</span> When the function <span class="math inline">\(g(\cdot,\theta)\)</span> is a parameterized function, we can, however, interchange the differential and integral operators regarding the parameter <span class="math inline">\(\theta\)</span>. <br> (Differentiation) If <span class="math inline">\(\mathbb{E}[g(Y,\theta)]\)</span> is finite for any <span class="math inline">\(\theta\in\mathbb{R}\)</span>, and <span class="math inline">\(g(y,\theta)\)</span> is always <a href="sub-calculus.html#sub:diffInt">differentiable</a> for any <span class="math inline">\(\theta\)</span>, the derivative <span class="math inline">\(\mbox{d}g(y,\theta)/\mbox{d}\theta\)</span> is finite, then <span class="math display">\[\frac{\mbox{d}}{\mbox{d}\theta}\mathbb{E}[g(Y,\theta)]=\mathbb{E}\left[\frac{\mbox{d}g(Y,\theta)}{\mbox{d}\theta}\right].\]</span>
(Integration) If <span class="math inline">\(g(y,\theta)\)</span> is bounded and <a href="sub-calculus.html#sub:diffInt">integrable</a>, then <span class="math display">\[\int\mathbb{E}\left[g(Y,\theta)\right]\mbox{d}\theta=\mathbb{E}\left[\int g(Y,\theta)\mbox{d}\theta\right]\]</span>
provided that the function <span class="math inline">\(\mathbb{E}[g(Y,\theta)]\)</span> is integrable.</span> For example, when <span class="math inline">\(g(Y)=Y^{2}\)</span>, usually <span class="math inline">\(\mathbb{E}[Y]^{2}\neq(\mathbb{E}[Y])^{2}\)</span>. The expression <span class="math inline">\(\mathbb{E}[Y]^{2}\)</span> is called the <em>second moment</em> (information) of <span class="math inline">\(Y\)</span>. In practice, people often use <strong>variance</strong> instead the <strong>second moment</strong> to indicate the information contained in the integral of squared <span class="math inline">\(Y\)</span>. We define the <em>variance</em> <span class="math inline">\(\sigma^2_Y\)</span> of <span class="math inline">\(Y\)</span> as <span class="math display">\[\mbox{Var}(Y)=\sigma_{Y}^{2}=\mathbb{E}\left[Y-\mathbb{E}[Y]\right]^{2}=\mathbb{E}[Y]^{2}-\mu_{Y}^{2}\]</span>where the square root of the <strong>variance</strong>, namely <span class="math inline">\(\sigma_{Y}\)</span>, is called the <em>standard deviation</em> of <span class="math inline">\(Y\)</span>.</p>
<p>For any constant value <span class="math inline">\(c\)</span>, the variance is zero, namely <span class="math inline">\(\mbox{Var}(c)=0\)</span> as there is no second moment information. For a linear transformation of <span class="math inline">\(Y\)</span>, <span class="math inline">\(aY+b\)</span> where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants, we have <span class="math display">\[\mbox{Var}(aY+b)=a^{2}\mbox{Var}(Y).\]</span>
Both the <strong>variance</strong> and <strong>standard deviation</strong> are measures of spread or variability. They describe how near or far typical outcomes are to the <strong>mean</strong>. The variance is easier to calculate, but the standard deviation is easier to interpret physically, as it has the same unit of measurement as the random variable. With the <strong>mean</strong> and the <strong>standard deviation</strong> of a random variable, we can make a simple summary of the distribution by using these first two moments.</p>
<p>The properties of <strong>expectation</strong> and <strong>variance</strong> motivate us to take another look at sum of <span class="math inline">\(n\)</span> <a href="ch-CalUn.html#sub:divRV">independent identical distributed</a> (i.i.d.) random vairables. Let <span class="math inline">\(Y_{i}\sim\mathcal{N}(\mu,\sigma^{2})\)</span> be <span class="math inline">\(n\)</span> i.i.d. <a href="ch-CalUn.html#sub:divRV">normal random variables</a> with <span class="math inline">\(i=1,\dots,n\)</span>. With the rule of <strong>expectation</strong> and <strong>variance</strong>, <a href="ch-CalUn.html#sub:divRV">infinite divisibility</a> of normal random variable says that<label for="tufte-sn-133" class="margin-toggle sidenote-number">133</label><input type="checkbox" id="tufte-sn-133" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">133</span> The <a href="ch-CalUn.html#sub:divRV">infinite divisibility</a> says <span class="math display">\[\sum_{i=1}^{n}Y_{i}\sim\mathcal{N}\left(n\mu,\: n\sigma^{2}\right).\]</span> Multiplying <span class="math inline">\(1/n\)</span> to <span class="math inline">\(\sum_{i=1}^{n}Y_{i}\)</span> scales the mean by <span class="math inline">\(1/n\)</span> and the variance by <span class="math inline">\(1/n^2\)</span>.</span>
<span class="math display" id="eq:lln1">\[\begin{equation}
\frac{1}{n}\sum_{i=1}^{n}Y_{i}\sim\mathcal{N}\left(\mu,\:\frac{\sigma^{2}}{n}\right).
\tag{9.1}  
\end{equation}\]</span>
If we let <span class="math inline">\(n\rightarrow\infty\)</span>, the variance <span class="math inline">\(\sigma^{2}/n\)</span> converges to zero, which means the spread shrinks to a singleton. Meanwhile, the mean of distribution remains the same as the individual’s mean. Thus, the convergent singleton is the mean value, namely <span class="math inline">\(\sum_{i=1}^{n}Y_{i}/n\rightarrow\mu\)</span>. Furthermore, if we <strong>standardize</strong> the sum:<label for="tufte-sn-134" class="margin-toggle sidenote-number">134</label><input type="checkbox" id="tufte-sn-134" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">134</span> Shifting the sum <span class="math inline">\(\sum_{i=1}^{n}Y_{i}\)</span> by differencing its mean <span class="math inline">\(n\mu\)</span> is to remove the deviation (from the <strong>mean</strong>). The scale factor <span class="math inline">\((\sigma\sqrt{n})^{-1}\)</span> is to standarize the variance. The procedure - summing the i.i.d. random variables, removing the population mean, plus rescaling the variance to attain a <a href="ch-CalUn.html#sub:divRV">standard normal distribution</a> - is called the <em>normalization</em> or <em>standarization</em> of the random variables. </span>
<span class="math display" id="eq:clt1">\[\begin{equation}
\frac{1}{\sigma\sqrt{n}}\left(\sum_{i=1}^{n}Y_{i}-n\mu\right)\sim\mathcal{N}(0,1).
\tag{9.2}  
\end{equation}\]</span>
The theorem of <strong>law of large numbers</strong> and <strong>central limit theorem</strong> basically say that besides <a href="ch-CalUn.html#sub:divRV">normal random variables</a>, equation <a href="ch-CalUn.html#eq:lln1">(9.1)</a> and <a href="ch-CalUn.html#eq:clt1">(9.2)</a> hold for any <span class="math inline">\(n\)</span> i.i.d. <span class="math inline">\(Y\)</span>s.
We will give a further look at these theorems in ch[?].</p>
<p>The <strong>standardization</strong> process may attract your attention on the corresponding distribution of the random variables. So far, we know that each random variable is distributed according to its probability law. And for <span class="math inline">\(n\)</span> random variables, they may have very different laws. However, the standarization says that after some process, there could be one law that summarizes the behaviors of combinatorial <span class="math inline">\(n\)</span> variables. If we only want the summarized information of a random variable, i.e. the 1st order moment, can we use some other probability law instead of the original law? Here comes the trick called the <em>change of probability law</em> (<em>Radon-Nikodym theorem</em>). Let <span class="math inline">\(\mathbb{Q}\)</span> and <span class="math inline">\(\mathbb{P}\)</span> be two probability laws. Then <span class="math display">\[\int g(y)\mathbb{P}(\mbox{d}y)=\int g(y)\frac{\mathbb{P}(\mbox{d}y)}{\mathbb{Q}(\mbox{d}y)}\mathbb{Q}(\mbox{d}y)\]</span>
is valid when <span class="math inline">\(\mathbb{P}(\mbox{d}y)/\mathbb{Q}(\mbox{d}y)\)</span> is a <a href="ch-CalUn.html#sub:rv">density</a> function <span class="math inline">\(f(y)\)</span>. This density is also called the <em>Radon-Nikodym derivative</em>.<label for="tufte-sn-135" class="margin-toggle sidenote-number">135</label><input type="checkbox" id="tufte-sn-135" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">135</span> Technically speaking, this theorem says that the existence of <span class="math inline">\(\mathbb{P}(\mbox{d}y)/\mathbb{Q}(\mbox{d}y)\)</span> requires the condition that for <span class="math inline">\(\mathcal{A}\in\sigma(\mathcal{Y})\)</span>, if <span class="math inline">\(\mathbb{Q}(\mathcal{A})=0\)</span> then <span class="math inline">\(\mathbb{P}(\mathcal{A})=0\)</span>. This condition is to say that law-<span class="math inline">\(\mathbb{P}\)</span> is <em>absolutely continuous</em> with respect to law-<span class="math inline">\(\mathbb{Q}\)</span>. In addition, if <span class="math inline">\(\mathbb{P}(\mathcal{A})=0\)</span> also implies <span class="math inline">\(\mathbb{Q}(\mathcal{A})=0\)</span>, then <span class="math inline">\(\mathbb{Q}\)</span> and <span class="math inline">\(\mathbb{P}\)</span> are <em>mutual absolutely continuous</em>. In this case, both <span class="math inline">\(\mathbb{P}(\mbox{d}y)/\mathbb{Q}(\mbox{d}y)\)</span> and <span class="math inline">\(\mathbb{Q}(\mbox{d}y)/\mathbb{P}(\mbox{d}y)\)</span> exist. For a simple illustration, one can think the existence of a <a href="ch-CalUn.html#sub:rv">density</a> function <span class="math inline">\(f(y)\)</span> means that <span class="math inline">\(\mathbb{P}(\mbox{d}y)=f(y)\mbox{d}y\)</span> and where the <span class="math inline">\(\mathbb{Q}\)</span>-law follows some indicator function of the set <span class="math inline">\(\mathcal{Y}\)</span> excluding <span class="math inline">\(\mathcal{A}\)</span> such that <span class="math inline">\(\mathbb{Q}(\mbox{d}y)= \mathbf{1}_{\mathcal{\mathcal{\mathcal{Y}}}}(y)\mbox{d}y\)</span>.</span></p>
</div>
<div id="sub:conProb" class="section level2">
<h2>
<span class="header-section-number">9.4</span> Conditional Probability and Conditional Expectation</h2>
<p>The expectation results a deterministic value, the <a href="ch-CalUn.html#sub:ex">mean</a>, which shift your concern of <a href="ch-CalUn.html#sub:rv">randomness</a> back to the <a href="ch-DE.html#sub:MecWorld">deterministic</a> world. Coin flips and dice throws, these kinds of events are often thought to be random. But professional gamblers may not agree. Even they knew the probability tricks behind the design of the games, some of them may tend tackle the underlying states; that is, not to consider <span class="math inline">\(Y\)</span> as a <a href="ch-CalUn.html#sub:rv">random variable</a> <span class="math inline">\(Y(x)\)</span>, but as a deterministic function from the underlying <a href="#ch-DE">state</a> <span class="math inline">\(x\)</span> to the <a href="sub-set-theory.html#sub:func">outcome</a> <span class="math inline">\(y\)</span>. They may be right. Indeed, when we flip a coin, many factors would cause the outcome of the flip, the initial impetus, the strength of the gravitational field, the amount of spin, and so on. But suppose that someone’s experience or talent allows him to compute all these complexities, then flipping a coin will no longer be a random event to him.</p>
<p>According to Laplace, the state of the world at a given instant is defined by an infinite number of parameters, subject to an infinite number of <a href="#ch-DE">differential equations</a>. If some “universal mind” could write down all these equations and integrate them, then “the mind” could predict with complete exactness, the entire evolution of the world. But infinity is extremely coarse in the real world. Even a simple event (like a coin flip) might be generated by a rather complex system, with multiple scales of spatial and temporal variability (the wind, the material of the coin, the physical gravity) and internal interactions (fingers’ joints) that vary considerably depending on the type of forces applied to the system. We don’t have the “universal mind” to calculate all the complexities.</p>
<p>All the truths are known because of the <strong>conditions</strong> assumed. When we say the outcome “3” of throwing a die is 1/6, we actually mean the <strong>conditional probability</strong> given that die is fair. Most (if not all) ordinary truths, and scientific ones, are conditional truths. All probability, like truth, is somehow <strong>conditional</strong>.</p>
<p>Leibniz thought of applying metrical probabilities to legal problems when he was a law student.<label for="tufte-sn-136" class="margin-toggle sidenote-number">136</label><input type="checkbox" id="tufte-sn-136" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">136</span> When Leibniz finally got clear about the role of a theory of probability, he called it natural jurisprudence.</span> In the legal process, all inferences are relative to or conditional on the evidence made available to the court. The probability for the <a href="sub-logic.html#sub:zeroth">proposition</a> depends on other conditions that favour the proposition. Leibniz took numerical probability as a primarily epistemic notion, the concept which requires us to recognize differences between what causes things to happen and what tells us that they happen. Leibniz’s investigation of conditional rights is tantamount to a study of those <strong>conditional</strong> (hypothetical) propositions because jurisprudence is such a model that one must deliberate about contingencies. In probability theory, <strong>conditional probabilities</strong> can be used to describe dependencies between two events with respect to the same probability law.</p>
<ul>
<li>
<em>Conditional probability</em>: Let <span class="math inline">\((\Omega,\sigma(\Omega),\mathbb{P})\)</span> be a <a href="sub-incomplete.html#sub:beyond2">probability space</a>, let <span class="math inline">\(\mathcal{A},\mathcal{C}\in\sigma(\Omega)\)</span> and let <span class="math inline">\(\mathbb{P}(\mathcal{C})&gt;0\)</span>, then <span class="math display">\[\mathbb{P}\left(\mathcal{A}|\mathcal{C}\right)=\frac{\mathbb{P}(\mathcal{A}\cap\mathcal{C})}{\mathbb{P}(\mathcal{C})}\]</span>
is called the <strong>conditional probability</strong> of <span class="math inline">\(\mathcal{A}\)</span> given <span class="math inline">\(\mathcal{C}\)</span> with respect to <span class="math inline">\(\mathbb{P}\)</span>
.<label for="tufte-sn-137" class="margin-toggle sidenote-number">137</label><input type="checkbox" id="tufte-sn-137" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">137</span>  The function <span class="math inline">\(\mathbb{P}(\cdot|\mathcal{C}):\,\sigma(\Omega)\rightarrow[0,1]\)</span> is a <a href="sub-incomplete.html#sub:beyond2">probability measure</a> on <span class="math inline">\((\Omega,\sigma(\Omega))\)</span> with the <a href="ch-CalUn.html#sub:ex">density</a> (<a href="ch-CalUn.html#sub:ex">Radon-Nikodym derivative</a>) <span class="math inline">\(\mathbf{1}_{\mathcal{C}}/\mathbb{P}(\mathcal{C})\)</span> or <span class="math inline">\(\mathbb{P}(\mbox{d}y|\mathcal{C})/\mathbb{P}(\mbox{d}y)\)</span> for <span class="math inline">\(\mathbb{P}(\mathcal{C})&gt;0\)</span>. Thus, <span class="math inline">\((\Omega,\sigma(\Omega),\mathbb{P}(\cdot|\mathcal{C}))\)</span> is also a <a href="sub-incomplete.html#sub:beyond2">probability space</a>.</span>
</li>
</ul>
<p>From the definition, it is easy to see that if <span class="math inline">\(\mathcal{C}\)</span> is an <a href="ch-CalUn.html#sub:divRV">independent</a> event of <span class="math inline">\(\mathcal{A}\)</span>, namely <span class="math inline">\(\mathbb{P}(\mathcal{A}\cap\mathcal{C})=\mathbb{P}(\mathcal{A})\mathbb{P}(\mathcal{C})\)</span>, then conditioning on <span class="math inline">\(\mathcal{C}\)</span> has no effect on the probability <span class="math inline">\(\mathbb{P}(\mathcal{A}|\mathcal{C})=\mathbb{P}(\mathcal{A})\mathbb{P}(\mathcal{C})/\mathbb{P}(\mathcal{C})=\mathbb{P}(\mathcal{A})\)</span>. The condition matters when two events are dependent. For two dependent events, the existence of a <a href="sub-set-theory.html#sub:func">causal relation</a> is often the main concern. Recall the way we define a random variable <span class="math inline">\(Y\)</span> - we called <span class="math inline">\(Y\)</span> a kind of function of some unknown <span class="math inline">\(x\)</span>, and treat <span class="math inline">\(x\)</span> as the <a href="ch-CalUn.html#sub:rv">cause of the randomness</a> of <span class="math inline">\(Y\)</span>. Now since the condition becomes available as a random variable, we can reformulate this probabilistic causal-effect relation through the <strong>conditional probability</strong>.</p>
<ul>
<li>
<em>Conditional random variable</em>, <em>joint distribution</em>, <em>marginal distribution</em>: Let <span class="math inline">\(Y\)</span> be a random variable <span class="math inline">\(Y:\,\Omega\rightarrow\mathbb{R}\)</span>, let <span class="math inline">\(X\)</span> be a random element <span class="math inline">\(X:\,\Omega\rightarrow\mathcal{X}\)</span>, then <span class="math inline">\(Y|X\)</span>, called <span class="math inline">\(Y\)</span> conditional on <span class="math inline">\(X\)</span>, is a <a href="ch-CalUn.html#sub:rv">random variable</a> following the <strong>conditional distribution</strong><label for="tufte-sn-138" class="margin-toggle sidenote-number">138</label><input type="checkbox" id="tufte-sn-138" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">138</span> Both <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are defined on the probability space <span class="math inline">\((\Omega,\sigma(\Omega),\mathbb{P})\)</span>. Formally speaking, two random variables are <span class="math inline">\(Y:\,(\Omega,\sigma(\Omega),\mathbb{P})\rightarrow(\mathbb{R},\sigma(\mathbb{R}))\)</span> and <span class="math inline">\(X:\,(\Omega,\sigma(\Omega),\mathbb{P})\rightarrow(\mathcal{X},\sigma(\mathcal{X}))\)</span>. The conditional probability of <span class="math inline">\(Y\in\mathcal{A}\)</span> given <span class="math inline">\(X\in\mathcal{C}\)</span> is <span class="math display">\[\mathbb{P}(Y\in\mathcal{A}\,|X\in\mathcal{C})=\frac{\mathbb{P}(Y\in\mathcal{A},\, X\in\mathcal{C})}{\mathbb{P}(X\in\mathcal{C})}\]</span> where <span class="math inline">\(\mathcal{A}\in\sigma(\mathbb{R})\)</span> and <span class="math inline">\(\mathcal{C}\in\sigma(\mathcal{X})\)</span>.</span> <span class="math display">\[\mathbb{P}(Y\,|X)=\frac{\mathbb{P}(Y,\, X)}{\mathbb{P}(X)}=\begin{cases}
\frac{\mathbb{P}(Y,\, X)}{\sum_{y}\mathbb{P}(Y=y,\, X)}, &amp; Y\mbox{ is discrete},\\
\frac{\mathbb{P}(Y,\, X)}{\int_{\mathbb{R}}\mathbb{P}(\mbox{d}y,\, X)}, &amp; Y\mbox{ is continuous},\\
0 &amp; \mbox{if }\mathbb{P}(X)=0,
\end{cases}\]</span>
where the probability law <span class="math inline">\(\mathbb{P}\)</span> is followed by both <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, and <span class="math inline">\(\mathbb{P}(Y,\, X)\)</span> is called the <strong>joint distribution</strong> of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, and <span class="math inline">\(\mathbb{P}(X)=\int\mathbb{P}(\mbox{d}y,X)\)</span> is the <strong>marginal distribution</strong> of <span class="math inline">\(\mathbb{P}(Y,\, X)\)</span>.</li>
</ul>
<p>The <strong>conditional probability</strong> should not be confused with <a href="ch-DE.html#sub:MecWorld">causation</a>. Notice that the conditional probability <span class="math inline">\(\mathbb{P}(X|Y)=\mathbb{P}(Y,X)/\mathbb{P}(Y)\)</span> is also a probabilistic valid form but it provides ignorant information in the case of <span class="math inline">\(X\)</span> causing <span class="math inline">\(Y\)</span>. When we subsitute the probabilities for knowledge of causes, we implicitly consider the <strong>conditions</strong> as the cause. As we perceive causes as preceding the effect, so also is the condition perceived as occurring earlier in conditional probability. That is, if <span class="math inline">\(X\)</span> is the cause of <span class="math inline">\(Y\)</span>, then the <strong>conditional random variable</strong> <span class="math inline">\(Y|X\)</span> is the only valid causation form associating with the <a href="sub-inferknow.html#sub:dyn">arrow of time</a>.</p>
<p>The conditional structure <span class="math inline">\(Y|X\)</span> induces a “<em>hierarchical</em>” <em>causation</em>. The first layer causation, the causal relation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, is presented in the probabilistic form, but a deeper causation <span class="math inline">\(\Omega\)</span> is still hidden.<label for="tufte-sn-139" class="margin-toggle sidenote-number">139</label><input type="checkbox" id="tufte-sn-139" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">139</span> For the conditional causality <span class="math inline">\(Y|X=x\)</span>, the realization <span class="math inline">\(x\)</span> is drawn from a random variable <span class="math inline">\(X(\Omega)\)</span>. By the definition of the <a href="ch-CalUn.html#sub:rv">random variable</a>, <span class="math inline">\(X(\cdot)\)</span> is a function <span class="math inline">\(X: \Omega \rightarrow \mathbb{R}\)</span>. Thus, we can imagine <span class="math inline">\(\Omega\)</span> is a deeper cause of <span class="math inline">\(X\)</span>, and hereafter, the cause of <span class="math inline">\(Y\)</span>.</span> Based on this procedure, one can construct a hierarchical “ladder” by making the state <span class="math inline">\(\Omega\)</span> visible, namely a second layer causation, but assuming another deeper invisible causal layer, and so on <a href="sub-incomplete.html#sub:beyond2">ad infinitum</a>.<label for="tufte-sn-140" class="margin-toggle sidenote-number">140</label><input type="checkbox" id="tufte-sn-140" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">140</span> In reality, this kind of mystical “hidden” power is everywhere but for most cases, it is not thought to be a higher/supremacy power; instead people treat it as some vague, almost likely a wilful agency under the name of “noise”, or “error” or “residual”, or “<span class="math inline">\(\omega\)</span>” or “<span class="math inline">\(\varepsilon\)</span>” in an event or in an experiment.</span> This is not the same type of causality as <span class="math inline">\(y=f(x)\)</span> which we have in <a href="sub-inferknow.html#determinism">deterministic model</a>.</p>
<p><a href="sub-logic.html#sub:PeanoInd">Logical deduction</a>, as building the relationship between <a href="sub-logic.html#sub:zeroth">propositions</a>, separates the logical evidence from the empirical ones. However, <a href="sub-logic.html#sub:zeroth">probability</a>, although it concerns the relationship between propositions, is epistemologically conditional on our vision of the world. It can be epistemologically true, but it does not exist in ontological reality, but in the epistemology of the mind. Unlike the heaven, or the earth, or human beings, probability does not have an existence in reality. Hume view the causal-effect relation from an empiricalist perspective. He viewed “<span class="math inline">\(X\)</span> causes <span class="math inline">\(Y\)</span>” as the meaning that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> constantly conjoined. His empiricism theory of “belief” tells that if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have been constantly conjoined in past experience, the impression of <span class="math inline">\(X\)</span> causing <span class="math inline">\(Y\)</span> constitutes the belief of such a cause. An expectation, which is a belief that is centered on the future, may or may not be realistic. We will come back to this disccusion in ch[?].</p>
<ul>
<li>
<em>Conditional expectation</em> and <em>regression</em>: The <strong>conditional expectation</strong> is defined as<label for="tufte-sn-141" class="margin-toggle sidenote-number">141</label><input type="checkbox" id="tufte-sn-141" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">141</span> The conditional expectation can be written formally as <span class="math inline">\(\mathbb{E}[Y|X]=\mathbb{E}[Y|\sigma(X)]\)</span>, namely the expectation of <span class="math inline">\(Y\)</span> given a <span class="math inline">\(\sigma\)</span>-algebra generated by the random variable <span class="math inline">\(X\)</span>. For details, see <span class="citation">Kolmogorov (<a href="bibliography.html#ref-Kolmogorov1933">1933</a>)</span>.</span> <span class="math display">\[\mathbb{E}[Y|\, X]=\int y\mathbb{P}(\mbox{d}y|X).\]</span> If there is a function <span class="math inline">\(g(\cdot)\)</span> such that <span class="math inline">\(\mathbb{E}[Y|\, X]=g(X)\)</span>, then <span class="math inline">\(g\)</span>
is called the regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>.<label for="tufte-sn-142" class="margin-toggle sidenote-number">142</label><input type="checkbox" id="tufte-sn-142" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">142</span> The function should be kind of continuous (measurable) <span class="math inline">\(g:(\mathcal{X},\sigma(\mathcal{X}))\rightarrow(\mathbb{R},\sigma(\mathbb{R}))\)</span>.</span> In this case, <span class="math inline">\(Y\)</span> is called the <em>regressand</em> and <span class="math inline">\(X\)</span> the <em>regressor</em>.</li>
</ul>
</div>
<div id="sub:consp" class="section level2">
<h2>
<span class="header-section-number">9.5</span> * Miscellaneous: Subjective Belief, Borel-Kolomogorov Paradox and Conspiracy</h2>
<p>Descartes’ famous line “Cogito, ergo sum” can be interpreted as follows: when all is in doubt, at least one thing is certain - the doubting mind exists. Descartes’ cogito made mind more certain for him than matter and led him to conclude that the two were separate and fundamentally different. The source of the exact natural <a href="sub-set-theory.html#sub:order">order</a> exist <strong>objectively</strong> and the human mind recognizes this <a href="sub-set-theory.html#sub:order">order</a> and gives an <strong>subjective</strong> interpretation. The mathematical concepts, for example, are created by the mind’s abstraction, that is, by the mental suppression of extraneous features of perceived objects so as to focus on skeletons. The seperation between <em>subjective</em> mind (or es cogitans - the “thinking thing”) and <em>objective</em> matter (or res extensa - the “extended thing”) is known as <em>Cartesian division</em>. Cartesian division has had a profound effect on Western thought. As a consequence of this division, the world was believed to be a mechanical system that could be described objectively, without ever mentioning the human observer. Such an “objective” description of nature became the ideal of all science, an ideal that was maintained until the twentieth century when the fallacy of the belief in mechanical world was exposed.</p>
<p>On the line of subjectivism, british empiricists, Berkeley and Hume stood out at the first stage.<label for="tufte-sn-143" class="margin-toggle sidenote-number">143</label><input type="checkbox" id="tufte-sn-143" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">143</span> In the east, prior to Descarte, there was a trend in a Neo-Confucian school (called school of mind) that inverstigated the function of mind. One representative, Yangming Wang, claims that objects do not exist entirely apart from the mind because the mind shapes them. He believed the mind giving reasons to the world, therefore, the mind alone shapes the world and becomes the source of all reason. Unlike Descarte, Wang argued that action (in the material world) should be in accordance with the mind to form a oneness (rather than a division).</span> Berkeley has his subjective idealist doctrine “esse est percipi” (be is to be perceived). Similar to Descarte, he asserts that everything must be either material or mental (mind). To him, mathematics as a science is ultimately concerned with objects of sense. The generality of mathematics comes from our capacity of perceiving things in a similar form. The (mental) “senses” become the skeleton of his form of empiricism: the inference to unperceived events on <a href="ch-CalUn.html#sub:conProb">causal law</a> is based on one’s sensation. Hume developed the idea of this empirical or <em>subjective causality</em>. In <span class="citation">Hume (<a href="bibliography.html#ref-Hume1772">1772</a>)</span>, he wrote “the knowledge of this relation is not, in any instance, attained by reasonings a priori, but arises entirely from experience, when we find that any particular objects are constantly conjoined with each other.” The constantly conjoined events can be viewed as the joint probability <span class="math inline">\(\mathbb{P}(Y,X)\)</span>; one’s belief of <span class="math inline">\(X\)</span> causing <span class="math inline">\(Y\)</span> is to reweight the conjoined events, namely reweighting the <a href="ch-CalUn.html#sub:conProb">joint probability</a> by the probability of the presumably condition <span class="math inline">\(\mathbb{P}(X)\)</span>. Associating these two arguments, we will end up with the <a href="ch-CalUn.html#sub:conProb">conditional probability</a> <span class="math inline">\(\mathbb{P}(Y|\, X)\)</span>.</p>
<p>The <strong>subjective</strong> view towards the <strong>objective</strong> world turns out to be critical in distinguishing epistemic truth from unjustified belief. If our epistemic cognitive ability of causality depends on a <a href="ch-CalUn.html#sub:conProb">conditional probability</a>, we need to understand the realm of admitting a conditional probability. Let <span class="math inline">\(\mathbb{P}(Y|X)\)</span> describe the probability dependence between the cause <span class="math inline">\(X\)</span> and the effect <span class="math inline">\(Y\)</span>. To have a well-defined conditional probability, by definition we need a non-trivial <a href="ch-CalUn.html#sub:conProb">condition</a>, namely <span class="math inline">\(\mathbb{P}(X)&gt;0\)</span>.</p>
<p>In pratice, it is quite hard to identify the potential issue caused by the zero conditioning probability <span class="math inline">\(\mathbb{P}(X)=0\)</span>. I list three reasons here.</p>
<ol style="list-style-type: decimal">
<li><p>Unawareness of continuous states: A switch of the focus from the discrete state to the continuous one may be an unconscious action. Just like the natural numbers can approximate all real numbers, a discrete event can be quite similar to a continuous event.</p></li>
<li><p>Ignorance of zero density: Even if we are fully aware the continuity, we may not realize that the consequence of conditioning on some zero probability density. <strong>Borel-Kolmogorov paradox</strong> will indicate this point.</p></li>
<li><p>(*) Invented zero conditioning probability: We may have surjective belief of a single conditional event, this surjective belief may take an uncompromising stand on our reasoning based on the zero conditioning probability. It may originate the <strong>conspiracy theory</strong>.</p></li>
</ol>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:NormalvsPoi"></span>
<img src="fig/Part2/NormalvsPoi.png" alt="Normal experiment vs Poisson experiment" width="100%"><!--
<p class="caption marginnote">-->Figure 9.8: Normal experiment vs Poisson experiment<!--</p>-->
<!--</div>--></span>
</p>
<p>Let’s consider these reasons one by one. For the first reason, our belief of some condition may be constructed by the isolated evidence, such as <span class="math inline">\(\mathbb{P}(X=x)\)</span> for some separated singletons <span class="math inline">\(x\)</span>s. If the underyling set <span class="math inline">\(\mathcal{X}\)</span> of the condition is represented by a <a href="ch-CalUn.html#sub:rv">continuous random variable</a>, it is known that <span class="math inline">\(\mathbb{P}(X=x)=0\)</span>, a trivial probabilistic condition, for any singleton <span class="math inline">\(x\)</span>.<label for="tufte-sn-144" class="margin-toggle sidenote-number">144</label><input type="checkbox" id="tufte-sn-144" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">144</span> For a continuous random variable, the probability of locating at a singleton <span class="math inline">\(x\)</span> is <a href="#sub:">infinitesimally</a> small.</span> In this case, the conditional probability <span class="math inline">\(\mathbb{P}(Y|X=x)\)</span> is ill-founded. Let’s consider two seperated experiments of a random variable <span class="math inline">\(Y\)</span> generated by two different probability laws <span class="math inline">\(\mathbb{P}(Y|X=x)\sim\mathcal{N}(x,\, x)\)</span> and <span class="math inline">\(\mathbb{P}(Y|X=x)\sim\mbox{Poi}(x)\)</span> for a singleton condition <span class="math inline">\(x\)</span> where <span class="math inline">\(x&gt;0\)</span>. One experiment seeting is continuous (normal) and the other is discrete (Poisson). By only looking at the observations in figure <a href="ch-CalUn.html#fig:NormalvsPoi">9.8</a>, it could be difficult to distinguish which experiment generates the continuous (in blue color) data. In pratice, we may misinterpet a continuous experiment as a discrete one, and compute a conditional probability based on a singleton condition. However, this conditional probability is ill-founded in theory.</p>
<div class="solution">
<p class="solution-begin">
R code for the plot <span id="sol-start-118" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-118', 'sol-start-118')"></span>
</p>
<div id="sol-body-118" class="solution-body" style="display: none;">
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb23-2" data-line-number="2">y =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">15</span>,<span class="dt">by =</span> <span class="dv">1</span>); normden =<span class="st"> </span><span class="kw">dnorm</span>(y,<span class="dt">sd =</span> <span class="dv">2</span>,<span class="dt">mean =</span> <span class="dv">4</span>); poiden =<span class="st"> </span><span class="kw">dpois</span>(y,<span class="dt">lambda =</span> <span class="dv">4</span>)</a>
<a class="sourceLine" id="cb23-3" data-line-number="3">pdata =<span class="st"> </span><span class="kw">data.frame</span>(y,normden,poiden)</a>
<a class="sourceLine" id="cb23-4" data-line-number="4"><span class="kw">ggplot</span>(pdata, <span class="kw">aes</span>(y,normden)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="dt">color=</span><span class="st">"skyblue"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(y,poiden),<span class="dt">color=</span><span class="st">"red"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">"Y"</span>, <span class="dt">y=</span><span class="st">"Probability"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</a></code></pre></div>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:BK"></span>
<img src="fig/Part2/BK-1.gif" alt="Borel-Kolmogorov paradox" width="100%"><!--
<p class="caption marginnote">-->Figure 9.9: Borel-Kolmogorov paradox<!--</p>-->
<!--</div>--></span>
</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:BK"></span>
<img src="fig/Part2/BK-2.gif" alt="Borel-Kolmogorov paradox" width="100%"><!--
<p class="caption marginnote">-->Figure 9.9: Borel-Kolmogorov paradox<!--</p>-->
<!--</div>--></span>
</p>
<p>The second reason is based on the <em>Borel-Kolmogorov paradox</em> given in chapter 5.2 in <span class="citation">Kolmogorov (<a href="bibliography.html#ref-Kolmogorov1933">1933</a>)</span>. <span class="citation">Kolmogorov (<a href="bibliography.html#ref-Kolmogorov1933">1933</a>)</span> considers a random variable having a uniform distribution on a unit sphere. Suppose that we have two options to construct a uniform sphere (3D) from the circle (2D), conditioning on the longitude <span class="math inline">\(\lambda\)</span> or conditioning on the latitude <span class="math inline">\(\phi\)</span>. If you have to take a uniform conditional distribution in the chosen direction, which one will you choose? You may think both direction are the same. But this is not the case because the joint density of this sphere is <span class="math inline">\(\mathbb{P}(\phi=a,\lambda=b)=\frac{1}{4\pi}\cos a\)</span>. Conditioning on longitude, the result is <span class="math inline">\(\mathbb{P}(\lambda=b|\phi=a)=1/(2\pi)\)</span>, a uniform distribution for <span class="math inline">\(b\in[0,2\pi]\)</span>, but conditioning on the latitude gives <span class="math inline">\(\mathbb{P}(\phi=a|\lambda=b)=\cos a/2\)</span> for <span class="math inline">\(a\in[-\pi,\pi]\)</span>, not a uniform distribution. From figure <a href="ch-CalUn.html#fig:BK">9.9</a>, it is easy to identify the change of the circumference along the latitude which is not the case for the longtitude.</p>
<p>We can see the problem using the <a href="ch-CalUn.html#sub:ex">Radon-Nikodym theorem</a>. The theorem says that there could be multiple probability laws consolidating with the probability <span class="math inline">\(\mathbb{P}(X\in\mathcal{A})\)</span> for an integral or an expectation.
<span class="math display">\[\begin{align*} \mathbb{P}(Y|X\in\mathcal{A})=&amp;\frac{\mathbb{P}(Y,\, X\in\mathcal{A})}{\mathbb{P}(X\in\mathcal{A})} \\ =&amp;\int_{\mathcal{A}}\left(\frac{\mathbb{P}(Y,\,\mbox{d}x)}{\mathbb{Q}(Y,\mbox{d}x)}\right)\mathbb{Q}(Y,\mbox{d}x)/\int_{\mathcal{A}}\left(\frac{\mathbb{P}(\mbox{d}x)}{\mathbb{Q}(\mbox{d}x)}\right)\mathbb{Q}(\mbox{d}x).\end{align*}\]</span>
<strong>Borel-Kolmogorov paradox</strong> basically implies that a different conditioning <a href="ch-CalUn.html#sub:conProb">(Radon-Nikodym) density</a> <span class="math inline">\(\mathbb{P}/\mathbb{Q}\)</span> may lead to a different <a href="ch-CalUn.html#sub:ex">conditional density</a> <span class="math inline">\(\mathbb{P}(Y|\mbox{d}x)\)</span>.</p>
<p>Now let <span class="math inline">\(\mathcal{A}\)</span> be a <a href="sub-incomplete.html#sub:beyond2"><span class="math inline">\(\mathbb{P}\)</span>-null set</a> (a zero-measure set <span class="math inline">\(\mathbb{P}(X\in\mathcal{A})=0\)</span>). The (Radon-Nikodym) density <span class="math inline">\(\mathbb{P}/\mathbb{Q}\)</span> remains valid, and <span class="math inline">\(\mathbb{Q}(X\in\mathcal{A})\)</span> needs not to be <span class="math inline">\(0\)</span> unless <span class="math inline">\(\mathbb{Q}\)</span> and <span class="math inline">\(\mathbb{P}\)</span> are <a href="ch-CalUn.html#sub:ex">mutual continuous</a>. For <span class="math inline">\(\mathbb{P}(X\in\mathcal{A})=0\)</span>, we know that the conditional probability <span class="math inline">\(\mathbb{P}(Y|X\in\mathcal{A})\)</span> does not exist. However, in the trick of the <a href="ch-CalUn.html#sub:ex">change of probability measures</a>, one may choose some <span class="math inline">\(\mathbb{Q}\)</span> without noticing that he may have an illed-founded <a href="ch-CalUn.html#sub:conProb">conditional density</a> on <span class="math inline">\(\mathcal{A}\)</span> as <span class="math inline">\(\mathbb{Q}(X\in\mathcal{A})\neq0\)</span>. Just like in the <strong>Borel-Kolmogorov paradox</strong>, one may wrongly assume that the conditional density is uniform.
This kind of ignorance could evoke two types of probabilitstic casuality under two different laws (or world views). In an extreme case, we can have two competing (probabilitstic) explainations.<label for="tufte-sn-145" class="margin-toggle sidenote-number">145</label><input type="checkbox" id="tufte-sn-145" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">145</span> One can model a mixture belief by <span class="math inline">\((1-\alpha)\frac{\mathbb{P}(Y,\, X)}{\mathbb{P}(X)}+\alpha\frac{\mathbb{Q}(Y,\, X)}{\mathbb{Q}(X)}\)</span> with <span class="math inline">\(\alpha\in(0,1)\)</span>.</span></p>
<p><span class="newthought">*Conspiracy theory</span></p>
<p>Now consider the third reason. In my opinion, the issue of two possible competing (probabilitstic) explainations from the <a href="ch-CalUn.html#sub:ex">change of probability measures</a> can found the conerstone of conspiracy theory.</p>
<p>The construction of a (general) conspiracy is as follows. We construct an <a href="sub-axioms.html#sub:axSet">uncountable set</a> (like the <a href="sub-incomplete.html#sub:beyond2">Cantor’s set</a>) so that some of the events in this set can only be assigned zero probability measure (due to the regularity of <span class="math inline">\(\sigma\)</span>-algebra of the <a href="sub-incomplete.html#sub:beyond2">probability axiom</a>). The <a href="sub-axioms.html#sub:axSet">uncountable set</a> is in principle computational infeasible due to infinite scales of subsets. Thus, people tend to assign zero probability to such an unusual event set. These <a href="sub-incomplete.html#sub:beyond2"><span class="math inline">\(\mathbb{P}\)</span>-null</a> events can exist at the social level, for example, they can be embedded in a common sense or be implanted in the general knowledge. Fraudence in a trustable financial system, for instance, may suppose to be a <span class="math inline">\(\mathbb{P}\)</span>-null event if <span class="math inline">\(\mathbb{P}\)</span> coincides with a sovereign law. But for the gamblers who are happen to participate in any of these <span class="math inline">\(\mathbb{P}\)</span>-null events, they may come up with different opinions <span class="math inline">\(\mathbb{Q}\)</span>. Under the gmables’ law <span class="math inline">\(\mathbb{Q}\)</span>, these unusual events are assigned by non-zero values.<label for="tufte-sn-146" class="margin-toggle sidenote-number">146</label><input type="checkbox" id="tufte-sn-146" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">146</span> Loosely speaking, any uncountable set <span class="math inline">\(\mathcal{A}\)</span> can not be measured under the <span class="math inline">\(\mathbb{Q}\)</span>-law either. But for a single event <span class="math inline">\(y\)</span> in <span class="math inline">\(\mathcal{A}\)</span>, <span class="math inline">\(\mathbb{Q}(y)\)</span> can assign a value. Here, we assume that the event <span class="math inline">\(y\)</span> is intrigued by the conspirators who hold the <span class="math inline">\(\mathbb{Q}\)</span>-law as their belief.</span> This <span class="math inline">\(\mathbb{Q}\)</span> may be not so different from <span class="math inline">\(\mathbb{P}\)</span> because when an individual forms the <span class="math inline">\(\mathbb{Q}\)</span>-law, he will make sure that <span class="math inline">\(\mathbb{P}\)</span> is continuous with respect to <span class="math inline">\(\mathbb{Q}\)</span> in order to share the casual reasoning with the system (based on the conditional probability <span class="math inline">\(\mathbb{P}(Y|X)\)</span>) on the usual events (events that are excluded from <span class="math inline">\(\mathbb{P}\)</span>-null sets).</p>
<p>From the economic perspective, one can think of the <span class="math inline">\(\mathbb{P}\)</span>-null set as those unexpected events to the general consumers and <span class="math inline">\(\mathbb{Q}\)</span>-null set as the unexpected events to the conspirators. The conspirators form the probability law <span class="math inline">\(\mathbb{Q}\)</span> to make <span class="math inline">\(\mathbb{P}\)</span> continuous with respect to <span class="math inline">\(\mathbb{Q}\)</span>, such that if <span class="math inline">\(\mathbb{Q}(\mathcal{C})=0\)</span> for any event set <span class="math inline">\(\mathcal{C}\)</span>, then <span class="math inline">\(\mathbb{P}(\mathcal{C})=0\)</span>. In addition, believers of <span class="math inline">\(\mathbb{Q}\)</span>-law can conspire an intrigue at a <span class="math inline">\(\mathbb{P}\)</span>-null set <span class="math inline">\(\mathcal{A}\)</span> so that <span class="math inline">\(\mathbb{Q}(\mathcal{A})\neq0\)</span>. One possible way to implant this intrigue is to make the system to too “complex” to analyze the event <span class="math inline">\(\mathcal{A}\)</span>. For example, <em>Ponzi scheme</em> and its derivatives all follow this strategy: particiapants who got paid in the scheme contribute to enlarge the size of the scheme until the inevitable collapse. The conspirators can construct a set <span class="math inline">\(\mathcal{A}\)</span> as the limit of some recursion (self-similar) sequences, i.e. <span class="math inline">\(\lim_{i\rightarrow\infty}\mathcal{A}_{i}=\mathcal{A}\)</span> such that <span class="math inline">\(\mathcal{A}_{1}\supset\mathcal{A}_{2}\supset\cdots\)</span> and <span class="math inline">\(\mathcal{A}_{i+1}=g(\mathcal{A}_{i})\)</span>. In each iteration <span class="math inline">\(i\)</span>, some consumers get paid by the event <span class="math inline">\(\mathcal{A}_{i}\)</span> so that they believe some similar events follow up. Indeed, for the sequential events, <span class="math inline">\(\mathcal{A}_{1},\dots,\mathcal{A}_{i}\)</span>, the scheme will make the consumers get paid. Consumers cannot foresee <span class="math inline">\(\mathcal{A}\)</span> because the game becomes more and more complex in each run (the size of measurable set <span class="math inline">\(\mathcal{A}_{i}\)</span> shrinks at each iteration), and <span class="math inline">\(\mathcal{A}\)</span> is the beyond the forward-looking scope of general participants. New consumers will make the backward-looking and will come up with a belief that the game will make them profitable.</p>
<p>I list the quantitative scheme for the <strong>conspiracy theory</strong>. This scheme aims to exploit a subjective belief pattern to accumulate a continual stream of resources. Let’s assume that the intrigue is planned at a time <span class="math inline">\(n\)</span>. For any time period <span class="math inline">\(i&lt;n\)</span>, the consumers’ probability law <span class="math inline">\(\mathbb{P}\)</span> is as follows:
<span class="math display">\[\begin{align*}
\mathbb{P}(Y=\mbox{profitable}|\, X\in\mathcal{A}_{i})  &amp;&gt;0,\\
\mathbb{P}(Y=\mbox{any far-distant event }|\, X\in\mathcal{A})  &amp;= \mbox{Undefined},\\
\mbox{Reasoning at any } i \mbox{ follows } \mathbb{P}(Y | X ) &amp;=\frac{\mathbb{P}(Y,\, X\in\mathcal{A_i})}{\mathbb{P}(X\in\mathcal{A_i})}.
\end{align*}\]</span>
The conspirators’ probability law <span class="math inline">\(\mathbb{Q}\)</span> is as follows:<span class="math display">\[\begin{align*}\mathbb{P}(Y=\mbox{profitable }|X\in\mathcal{A}_{i}) &amp;  \mbox{ is continuous with respect to }  \mathbb{Q}(Y|X) , \mbox{ for } i&lt;n\\
\mathbb{Q}(Y=\mbox{collapse }|X\in\mathcal{A}) &amp;    =1 \\
\mbox{Reasoning at any } i&lt;n &amp; \mbox{ follows } \mathbb{P}(Y | X ) = \frac{\int_{\mathcal{A}_i}\left(\frac{\mathbb{P}(Y,\,\mbox{d}x)}{\mathbb{Q}(Y,\mbox{d}x)}\right)\mathbb{Q}(Y,\mbox{d}x)}{\int_{\mathcal{A}_i}\left(\frac{\mathbb{P}(\mbox{d}x)}{\mathbb{Q}(\mbox{d}x)}\right)\mathbb{Q}(\mbox{d}x)}.
\end{align*}\]</span>
Under the <span class="math inline">\(\mathbb{Q}\)</span>-law, we can consider the <span class="math inline">\(\mathbb{P}\)</span>-null sets as <em>overflows</em>.</p>
<p>The <strong>conspiracy theory</strong> of the <span class="math inline">\(\mathbb{P}\)</span>-null set may correspond with many aspects of the economic, social, and political activities. Forbidden disclosures (crypto-communication, computer hacking), special equipments (spy-in-the-sky, big-brother monitoring), or even improper polical actions (lobbying, coup d’état), etc., are untouchable to the public or to the society. Any model that wants to cover some or all these specific features will make its complexity blow up, and this limitation is well recognized by the public and the society. Thus, the authority has to restrict the model to set the probabilities or their beliefs of those <strong>overflows</strong> to zero. But those events attract the conspirators. Once a scheme successes in one of those <span class="math inline">\(\mathbb{P}\)</span>-null sets, the authority may be forced to modify the model. But new model will awaken new <strong>overflows</strong>, and then new conspirators will arrive, and so on.<label for="tufte-sn-147" class="margin-toggle sidenote-number">147</label><input type="checkbox" id="tufte-sn-147" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">147</span> In other words, the social system can be thought of as a complete system under the authorities’ <span class="math inline">\(\mathbb{P}\)</span>-law but incomplete under conspirators’ <span class="math inline">\(\mathbb{Q}\)</span>-law.</span></p>
<p>The scheme also shows that a probability could be a totally <strong>subjective</strong> quantity. The quantity reflects a collective belief of a certain event, and the construction of such a probabilistic event invovles the actions of the followers under such belief.</p>

</div>
</div></body></html>

<p style="text-align: center;">
<a href="ch-DE.html"><button class="btn btn-default">Previous</button></a>
<a href="part-iii-emergence-of-abstract-interactions.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-11-15
</p>
</div>
</div>



</body>
</html>
