<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="12 Eigenvalues and Eigenvectors | Project XXII" />
<meta property="og:type" content="book" />





<meta name="author" content="Zhengyuan Gao" />

<meta name="date" content="2020-05-30" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="12 Eigenvalues and Eigenvectors | Project XXII">

<title>12 Eigenvalues and Eigenvectors | Project XXII</title>

<link href="libs/tufte-css/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding/datatables.js"></script>
<link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>



</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Project XXII<p><p class="author">Zhengyuan Gao</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BA; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Preface</a>
<a href="part-i-inference-based-upon-logic-and-logical-computation.html">PART I: Inference Based upon Logic and Logical Computation</a>
<a href="sub-logic.html"><span class="toc-section-number">1</span> Logic</a>
<a href="sub-set-theory.html"><span class="toc-section-number">2</span> Set Theory</a>
<a href="sub-axioms.html"><span class="toc-section-number">3</span> Axioms</a>
<a href="sub-inferknow.html"><span class="toc-section-number">4</span> Inference and Knowledge</a>
<a href="sub-incomplete.html"><span class="toc-section-number">5</span> Incompleteness</a>
<a href="part-ii-infinitesimal-changes-and-their-consequences.html">PART II: Infinitesimal Changes and their Consequences</a>
<a href="sub-continuity.html"><span class="toc-section-number">6</span> Continuity</a>
<a href="sub-calculus.html"><span class="toc-section-number">7</span> Calculus</a>
<a href="ch-DE.html"><span class="toc-section-number">8</span> Differential Equations</a>
<a href="ch-CalUn.html"><span class="toc-section-number">9</span> Calculus under Uncertainty</a>
<a href="part-iii-emergence-of-abstract-interactions.html">PART III: Emergence of Abstract Interactions</a>
<a href="ch-vecMat.html"><span class="toc-section-number">10</span> Vector and Matrix</a>
<a href="ch-MatComp.html"><span class="toc-section-number">11</span> Matrix Computation</a>
<a id="active-page" href="ch-eigen.html"><span class="toc-section-number">12</span> Eigenvalues and Eigenvectors</a><ul class="toc-sections">
<li class="toc"><a href="#sub:det"> Determinants, Characteristic Polynomials, and Complex Numbers</a></li>
</ul>
<a href="bibliography.html">Bibliography</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="ch:eigen" class="section level1">
<h1>
<span class="header-section-number">12</span> Eigenvalues and Eigenvectors</h1>
<p>If we could express the change of the milieu in a computational form, then the matrix computation of <span class="math inline">\(\mathbf{A}\mathbf{x}\)</span> could describe the generation and the transformation of such milieu. The input <span class="math inline">\(\mathbf{x}\)</span> could be stimulus and cause, and the output <span class="math inline">\(\mathbf{A}\mathbf{x}\)</span> could be reaction and effect. In other words, if the information of the current world were stored in a (huge) vector <span class="math inline">\(\mathbf{x}\)</span> in an <span class="math inline">\(n\)</span>-dimensional <a href="#sub:vecSpace">field</a>, then the (linear) dynamics of this world were conducted by an <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>Figure <a href="ch-MatComp.html#fig:MatrixTransform">11.2</a> shows the transformations of vectors in a 2-dimensional <a href="#sub:vecSpace">vector space</a>. We can see that those directions and orientations (in purple and red lines) remain consistent with the transformation. By stretching and compressing the directional arrows, we can scale their magnitudes (scalars) under which the transformation looks <strong>invariant</strong>. Generally speaking, this kind of <strong>invariance</strong> exists for any <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span>, and it is analyzable under the framework of <strong>eigenvalues</strong> and <strong>eigenvectors</strong>.<label for="tufte-sn-215" class="margin-toggle sidenote-number">215</label><input type="checkbox" id="tufte-sn-215" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">215</span> For non-square <span class="math inline">\(m\times n\)</span> matrices <span class="math inline">\(\mathbf{A}\)</span>, we can use an alternative framework of singular values and singular vectors. Roughly speaking, the singular values are the positive square roots of the nonzero eigenvalues of the corresponding matrix <span class="math inline">\(\mathbf{A}^{\top}\mathbf{A}\)</span>. The corresponding eigenvectors are called the singular vectors.</span> <strong>Eigenvalues</strong> and <strong>eigenvectors</strong> can be mathematically defined as sets of <strong>invariants</strong> within processes of transformations. As matrixs can store all sorts of data information, eigenvalues and eigenvectors can in turn represent the corresponding “script lines” with clearly directional “stages”.</p>
<p>Perhaps the infinitely embodying milieu as computations of matrices makes the symbolization impossible. But as a conceptual ideal, the embodiment of eigenvalues and eigenvectors provide a way to recognize two seperated channels of forming one’s perception, the (concrete) material channel and the (imginary) spiritual one. The logic of using eigenvalues and eigenvectors - that is, the identification, the trace and maybe the selection of its various structural invariants - can reconfigure the relation of the input and output channels of the computation. Physical and metaphysical forces from the aspects in the ontology and the epistemology, form these complemenary channels in milieux.</p>
<p>As long as a secular transformation, such as a trade or a production, can be expressed in an input-output matrix form, the <strong>eigenvalue</strong> and the <strong>eigenvector</strong> can reconfigure the form as an epistemological and ontological emergence. Maybe I can call this process the <strong>valuation</strong>. The valuation here is defined as the emergence of quantitative “being” that attaches to the transformation, and remains invariant when the structure of the transformation unchanges.</p>
<div id="sub:det" class="section level2">
<h2>
<span class="header-section-number">12.1</span> Determinants, Characteristic Polynomials, and Complex Numbers</h2>
<p>The detective novel often depicts a fragmentary plot to drive the readers in a complete daze. Once the readers were attracted by various bizarre and outrageous transformation scenes, the author, quite likely, has buried the solution clue in some unaltered remainders.</p>
<p>For mathematical transformations, in passing from one equation to the other, what was left unchanged is called an <strong>invariant</strong>. We explore the invariant properties of the linear transformations through the <strong>determinant</strong>. A <em>determinant</em> of a square matrix <span class="math inline">\(\mathbf{A}\)</span>, denoted by <span class="math inline">\(\mbox{det}(\mathbf{A})\)</span>, is a function mapping <span class="math inline">\(\mathbf{A}\)</span> to a <a href="sub-continuity.html#sub:completeness">real number</a>. The real number contains the information about the product of all <a href="ch-MatComp.html#sub:GElimination">pivots</a> of the matrix (<a href="ch-MatComp.html#sub:GElimination">echelon form</a>).</p>
<p>Consider a <span class="math inline">\(2\times2\)</span> matrix <span class="math display">\[\mathbf{A}=\left[\begin{array}{cc}
a_{11} &amp; a_{12}\\
a_{21} &amp; a_{22}
\end{array}\right].\]</span>
The pivots of <span class="math inline">\(\mathbf{A}\)</span> are <span class="math inline">\(a_{11}\)</span> and <span class="math inline">\(a_{22}-(a_{21}/a_{11})a_{12}\)</span>. The product of these pivots is <span class="math inline">\(a_{11}(a_{22}-(a_{21}/a_{11})a_{12})=a_{11}a_{22}-a_{21}a_{12}\)</span>. The <strong>determinant</strong> is sometimes written with a single straight line as left and right brackets, thus we would write the above result as <span class="math display">\[\mbox{det}(\mathbf{A})=\left|\begin{array}{cc}
a_{11} &amp; a_{12}\\
a_{21} &amp; a_{22}
\end{array}\right|=a_{11}a_{22}-a_{12}a_{21}.\]</span></p>
<p>If we think of <span class="math inline">\(\mathbf{A}\)</span> as two row vectors in the plane, i.e. <span class="math inline">\(\mathbf{A}=[\mathbf{a}_{1},\mathbf{a}_{2}]^{\top}\)</span>, then the <strong>determinant</strong> is the (signed) area of the parallelogram spanned by <span class="math inline">\(\mathbf{a}_{1}\)</span> and <span class="math inline">\(\mathbf{a}_{2}\)</span>. The sign of the <strong>determinant</strong> is positive (negative) if the two vectors are positively (negatively) oriented. Figure <a href="ch-eigen.html#fig:det">12.1</a> shows a parallelogram spanned by <span class="math inline">\(\mathbf{z}=[5,1]\)</span> and <span class="math inline">\(\mathbf{u}=[-1,3]\)</span>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:det"></span>
<img src="fig/Part3/det.png" alt="Determinant in a parallelogram" width="100%"><!--
<p class="caption marginnote">-->Figure 12.1: Determinant in a parallelogram<!--</p>-->
<!--</div>--></span>
</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:det"></span>
<img src="fig/Part3/det2.png" alt="Determinant in a parallelogram" width="100%"><!--
<p class="caption marginnote">-->Figure 12.1: Determinant in a parallelogram<!--</p>-->
<!--</div>--></span>
</p>
<p>For an <a href="ch-MatComp.html#sub:matInv">invertible</a> <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span>, its <a href="ch-MatComp.html#sub:GElimination">echelon form</a> says that all <a href="ch-MatComp.html#sub:GElimination">pivots</a> are non-zero<label for="tufte-sn-216" class="margin-toggle sidenote-number">216</label><input type="checkbox" id="tufte-sn-216" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">216</span> As the statement in section <a href="ch-MatComp.html#sub:matInv">11.2</a>, the <strong>determinant</strong> also relates the <a href="ch-MatComp.html#sub:matInv">invertibility</a> of <span class="math inline">\(\mathbf{A}\)</span>. A zero <strong>determinant</strong> induces a non-invertible/singular square matrix.</span>, so is the product of all pivots, namely <span class="math inline">\(\mbox{det}(\mathbf{A})\neq 0\)</span>. Geometrically, the absolute value of <span class="math inline">\(\mbox{det}(\mathbf{A})\)</span> equlas to the volume of the <span class="math inline">\(n\)</span>-dimensional parallelepiped, where the <span class="math inline">\(n\)</span>-dimensional parallelepiped spanned by the column or row vectors of <span class="math inline">\(\mathbf{A}\)</span>.<label for="tufte-sn-217" class="margin-toggle sidenote-number">217</label><input type="checkbox" id="tufte-sn-217" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">217</span> The <strong>determinant</strong> of <span class="math inline">\(\mathbf{A}\)</span> is positive or negative according to whether the linear mapping <span class="math inline">\(\mathbf{A}\mathbf{x}\)</span> preserves or reverses the orientation of the <span class="math inline">\(n\)</span>-dimensional <span class="math inline">\(\mathbf{x}\)</span>. The absolute value of the determinant can be thought of as a measure of how much multiplication by the matrix expands or contracts the space of <span class="math inline">\(\mathbf{x}\)</span>. If the determinant is <span class="math inline">\(0\)</span>, then space is contracted completely along at least one dimension, causing it to lose all of its volume. If the determinant is <span class="math inline">\(1\)</span>, then the transformation preserves volume.</span></p>
<p>Here are list some properties (facts) of the <strong>determinant</strong> of a square <span class="math inline">\(n\times n\)</span> matrix.</p>
<p>1). <span class="math inline">\(\mbox{det}(\mathbf{I})=1\)</span>.</p>
<p>2). <span class="math inline">\(\mbox{det}(\mathbf{A})=0\)</span>, if the matrix <span class="math inline">\(\mathbf{A}\)</span> is <a href="ch-MatComp.html#sub:matInv">singular</a> or <a href="ch-MatComp.html#sub:matInv">non-invertible</a>.</p>
<p>3). Swapping two vectors of <span class="math inline">\(\mathbf{A}\)</span> changes the sign of <span class="math inline">\(\mbox{det}(\mathbf{A})\)</span>. In other words, <span class="math inline">\(\mbox{det}(\mathbf{E}_{ij}\mathbf{A})=-\mbox{det}(\mathbf{A})\)</span> where <span class="math inline">\(\mathbf{E}_{ij}\)</span> is the <a href="ch-MatComp.html#sub:matInv">permutation elementary matrix</a>.</p>
<p>4). Scaling a row or column of <span class="math inline">\(\mathcal{A}\)</span> scales the determinant. In other words, <span class="math inline">\(\mbox{det}(\mathbf{E}_{i}(c)\mathbf{A})=c \times \mbox{det}(\mathbf{A})\)</span> where <span class="math inline">\(\mathbf{E}_{i}(c)\)</span> is the <a href="ch-MatComp.html#sub:matInv">scaling elementary matrix</a>.</p>
<p>5). For the <a href="ch-MatComp.html#sub:matInv">replacement elementary matrix</a>, <span class="math inline">\(\mbox{det}(\mathbf{E}_{ij}(s)\mathbf{A})=\mbox{det}(\mathbf{A})\)</span>.</p>
<p>Note that the <strong>determinant</strong> of triangular matrices and diagonal matrices are exactly the product of the diagonal entries. Fact 1) comes directly from the product of ones on the diagonal of <span class="math inline">\(\mathbf{I}\)</span>. Fact 2) comes from the definition. Figure <a href="ch-eigen.html#fig:det">12.1</a> gives an example of 3). For 4), figure <a href="ch-eigen.html#fig:Scalingdet">12.2</a> shows how the scaling transformation changes the determinant (the area of the parallelogram). For 5), figure <a href="ch-MatComp.html#fig:GElimination">11.1</a> shows that in the <a href="ch-MatComp.html#sub:GElimination">Gaussian elimination</a> procedures, forming the <a href="ch-MatComp.html#sub:GElimination">echelon form</a> do not affect the volume. In other words, the <a href="ch-MatComp.html#sub:matInv">replacement elementary matrices</a> simply rotate the intersections of the parallelepiped, and preserve the volume.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:Scalingdet"></span>
<img src="fig/Part3/Scalingdet.gif" alt="Determinants of the scaling transformations" width="100%"><!--
<p class="caption marginnote">-->Figure 12.2: Determinants of the scaling transformations<!--</p>-->
<!--</div>--></span>
</p>
<div class="solution">
<p class="solution-begin">
Another definition of the determinant <span id="sol-start-43" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-43', 'sol-start-43')"></span>
</p>
<div id="sol-body-43" class="solution-body" style="display: none;">
<p>The <em>determinant</em> of a square <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(\mathbf{A}=[a_{ij}]\)</span> is the scalar quantity <span class="math inline">\(\mbox{det}(\mathbf{A})\)</span> that is recursively defined by the following expression: If <span class="math inline">\(n=1\)</span>, then <span class="math inline">\(\mbox{det}\mathbf{A}=a_{11}\)</span>; otherwise, it follows<span class="math display">\[\mbox{det}\mathbf{A}=\sum_{k=1}^{n}a_{k1}(-1)^{k+1}\mbox{det}(\mathbf{A}_{(k)1})\]</span>
where <span class="math inline">\(\mbox{det}(\mathbf{A}_{(k)1})\)</span> is the <strong>determinant</strong> of the <span class="math inline">\((n-1)\times(n-1)\)</span> matrix obtained from <span class="math inline">\(\mathbf{A}\)</span> by deleting the first row and the <span class="math inline">\(k\)</span>-th coloumn of <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>The idea of this formula is to split an <span class="math inline">\(n\times n\)</span> matrix by crossing out row <span class="math inline">\(1\)</span> and column <span class="math inline">\(k\)</span> to get a (sub)matrix <span class="math inline">\(\mathbf{A}_{(k)1}\)</span> of size <span class="math inline">\(n-1\)</span>. Here is an example, <span class="math display">\[\mbox{det}\left[\begin{array}{cc}
a_{11} &amp; a_{12}\\
a_{21} &amp; a_{22}
\end{array}\right]=a_{11}\mbox{det}(\mathbf{A}_{(1)1})-a_{21}\mbox{det}(\mathbf{A}_{(2)1})=a_{11}a_{22}-a_{21}a_{12}.\]</span></p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>By the previous facts, it is possible to derive the further properties:</p>
<p>6). <span class="math inline">\(\mbox{det}(\mathbf{A}\mathbf{B})=\mbox{det}(\mathbf{A})\mbox{det}(\mathbf{B})\)</span>.</p>
<p>7). <span class="math inline">\(\mbox{det}(\mathbf{A}+\mathbf{B})=\mbox{det}(\mathbf{A})+\mbox{det}(\mathbf{B})\)</span>.</p>
<p>8). <span class="math inline">\(\mbox{det}\mathbf{A}^{\top}=\mbox{det}\mathbf{A}\)</span>.</p>
<p>9). <span class="math inline">\(\mbox{det}(\mathbf{A}^{-1})=\frac{1}{\mbox{det}(\mathbf{A})}\)</span>.</p>
<div class="solution">
<p class="solution-begin">
Sketch of the proof <span id="sol-start-44" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-44', 'sol-start-44')"></span>
</p>
<div id="sol-body-44" class="solution-body" style="display: none;">
<p>For 6), notice that <span class="math inline">\(\mbox{det}(\mathbf{E}_i (c))=c\)</span>, <span class="math inline">\(\mbox{det}(\mathbf{E}_{ij})=-1\)</span>, and <span class="math inline">\(\mbox{det}(\mathbf{E}_{ij}(s))=1\)</span>. From the results of 3),4),5), we have<br><span class="math display">\[
\begin{align*}
\mbox{det}(\mathbf{E}_{i}(c)\mathbf{A})&amp;=\mbox{det}(\mathbf{E}_{i}(c)) \mbox{det}(\mathbf{A})=c\times \mbox{det}(\mathbf{A}),\\
\mbox{det}(\mathbf{E}_{ij}\mathbf{A})&amp;=\mbox{det}(\mathbf{E}_{ij}) \mbox{det}(\mathbf{A})=-\mbox{det}(\mathbf{A}),\\
\mbox{det}(\mathbf{E}_{ij}(s)\mathbf{A})&amp;=\mbox{det}(\mathbf{E}_{ij}(s))\times \mbox{det}(\mathbf{A})=\mbox{det}.
\end{align*}
\]</span></p>
<p>Therefore, all mutiplications of the square matrix <span class="math inline">\(\mathbf{A}\)</span> by the elementary matrices <span class="math inline">\(\mathbf{E}\)</span> satisfy <span class="math inline">\(\mbox{det}(\mathbf{E}\mathbf{A})=\mbox{det}(\mathbf{E}_{i})\mbox{det}(\mathbf{A}).\)</span> Because <span class="math inline">\(\mathbf{A}\)</span> is invertible, we can express it as a product of elementary matrices (see the proof in section <a href="ch-MatComp.html#sub:matInv">11.2</a>), say <span class="math inline">\(\mathbf{A}=\mathbf{E}^{(1)}\mathbf{E}^{(2)}\cdots\mathbf{E}^{(k)}\)</span>. Then for <span class="math inline">\(\mathbf{A}\mathbf{B}\)</span>, we have
<span class="math display">\[
\begin{align*}
\mbox{det}(\mathbf{A}\mathbf{B})&amp;=\mbox{det}(\mathbf{E}^{(1)}\mathbf{E}^{(2)}\cdots\mathbf{E}^{(k)}B)\\
&amp;=\mbox{det}(\mathbf{E}^{(1)})\mbox{det}(\mathbf{E}^{(2)})\cdots\mbox{det}(\mathbf{E}^{(k)})\mbox{det}\mathbf{B}\\
&amp;=\mbox{det}(\mathbf{E}^{(1)}\cdots\mathbf{E}^{(k)})\mbox{det}(\mathbf{B})=\mbox{det}(\mathbf{A})\mbox{det}(\mathbf{B}).
\end{align*}
\]</span>
The result follows.</p>
<p>For 7), see figure <a href="ch-eigen.html#fig:detAdd">12.3</a>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:detAdd"></span>
<img src="fig/Part3/detAdd.png" alt="Determinant of the matrix addition" width="100%"><!--
<p class="caption marginnote">-->Figure 12.3: Determinant of the matrix addition<!--</p>-->
<!--</div>--></span>
</p>
<p>For 8), Similar to the proof for 6). Notice that <span class="math inline">\(\mbox{det}(\mathbf{E}^{\top}_i (c))=c\)</span>, <span class="math inline">\(\mbox{det}(\mathbf{E}^{\top}_{ij})=-1\)</span>, and <span class="math inline">\(\mbox{det}(\mathbf{E}^{\top}_{ij}(s))=1\)</span>. Because the invertible <span class="math inline">\(\mathbf{A}\)</span> can be expressed by a product of elementary matrices (see the proof in section <a href="ch-MatComp.html#sub:matInv">11.2</a>), say <span class="math inline">\(\mathbf{A}=\mathbf{E}^{(1)}\mathbf{E}^{(2)}\cdots\mathbf{E}^{(k)}\)</span>, and because <span class="math inline">\(\mathbf{A}^{\top}=(\mathbf{E}^{(1)})^{\top}(\mathbf{E}^{(2)})^{\top}\cdots(\mathbf{E}^{(k)})^{\top}\)</span>, so by the result of 6), we have
<span class="math display">\[
\begin{align*}
\mbox{det}(\mathbf{A})&amp;=\mbox{det}((\mathbf{E}^{(1)})^{\top}(\mathbf{E}^{(2)})^{\top}\cdots(\mathbf{E}^{(k)})^{\top})\\
&amp;=\mbox{det}((\mathbf{E}^{(1)})^{\top})\cdots\mbox{det}((\mathbf{E}^{(k)})^{\top})\\
&amp;=\mbox{det}(\mathbf{E}^{(1)}\cdots\mathbf{E}^{(k)})\mbox{det}=\mbox{det}(\mathbf{A}).
\end{align*}
\]</span></p>
<p>For 9), note that the identity <span class="math inline">\(\mathbf{A}\mathbf{A}^{-1}=\mathbf{I}\)</span>
gives<span class="math display">\[\mbox{det}(\mathbf{A}\mathbf{A}^{-1})=\mbox{det}(\mathbf{A})\mbox{det}(\mathbf{A}^{-1})=\mathbf{I}.\]</span>
The result follows.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Now we return to the <a href="ch-vecMat.html#sub:linearity">fixed point problem</a> in the matrix form <span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{x}\)</span> where the transformation is invariant around the solution <span class="math inline">\(\mathbf{x}^{*}\)</span>. Suppose <span class="math inline">\(\mathbf{A}\)</span> is an <span class="math inline">\(n\times n\)</span> <a href="ch-MatComp.html#sub:matInv">invertible</a> matrix, we will see that the existence of <span class="math inline">\(\mathbf{x}^{*}\)</span> can be determinated by the <strong>determinant</strong> of <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>To see the relation, notice that the fixed point problem can be presented as <span class="math inline">\(0=\mathbf{x}-\mathbf{A}\mathbf{x}\)</span> or <span class="math display">\[0=(\mathbf{I}-\mathbf{A})\mathbf{x}.\]</span> The statement 4 in section <a href="ch-MatComp.html#sub:matInv">11.2</a> says that for an invertible matrix <span class="math inline">\(\mathbf{I}-\mathbf{A}\)</span>, <span class="math inline">\((\mathbf{I}-\mathbf{A})\mathbf{x}=0\)</span> has a unique solution <span class="math inline">\(\mathbf{x}=0\)</span>. It implies that if we are looking for a non-zero solution of <span class="math inline">\(\mathbf{x}\)</span>, we should expect that <span class="math inline">\(\mathbf{(\mathbf{I}-\mathbf{A})}\)</span> to be <a href="ch-MatComp.html#sub:matInv">singular</a> or non-invertible, namely <span class="math inline">\(\mbox{det}(\mathbf{I}-\mathbf{A})=0\)</span>. In this way, the existence of a non-zero fixed point solution <span class="math inline">\(\mathbf{x}^{*}\)</span> is equivalent to a zero <strong>determinant</strong> of <span class="math inline">\(\mathbf{I}-\mathbf{A}\)</span>.</p>
<p>The linear transformation of the <a href="ch-vecMat.html#sub:linearity">fixed point problem</a> <span class="math inline">\(\mathbf{A}\mathbf{x}^{*}=1\times \mathbf{x}^{*}\)</span> preserve the invariant <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\mathbf{x}^{*}\)</span>. We can call the vector <span class="math inline">\(\mathbf{x}^{*}\)</span> the <strong>eigenvector</strong> of <span class="math inline">\(\mathbf{A}\)</span> with <strong>eigenvalue</strong> <span class="math inline">\(1\)</span>. The formal definition of <strong>eigenvector</strong> and <strong>eigenvalue</strong> is given by the <strong>characteristic polynomial</strong>.</p>
<ul>
<li>
<strong>Eigenvector</strong>, <strong>eigenvalue</strong>, <strong>characteristic polynomial</strong> : Consider an <span class="math inline">\(n\times n\)</span>
matrix <span class="math inline">\(\mathbf{A}\in\mathbb{F}^{n\times n}\)</span>, where the <a href="ch-MatComp.html#sub:vecSpaces">field</a> <span class="math inline">\(\mathbb{F}\)</span> can be <span class="math inline">\(\mathbb{C}\)</span>
or <span class="math inline">\(\mathbb{R}\)</span>. The <em>eigenvalues</em> of <span class="math inline">\(\mathbf{A}\)</span>
are the <span class="math inline">\(n\)</span> roots, <span class="math inline">\(\Lambda=\{\lambda_{1},\dots,\lambda_{n}\}\)</span>, of its <em>characteristic polynomial</em> <span class="math display">\[\mbox{det}(\lambda\mathbf{I}-\mathbf{A}).\]</span> The set of these roots is called the <em>spectrum</em> of <span class="math inline">\(\mathbf{A}\)</span>, and it can also be written as<span class="math display">\[\Lambda=\{\lambda \in {\mathbb{C}} \,:\mbox{det}(\lambda\mathbf{I}-\mathbf{A})=0\}.\]</span>
For any <span class="math inline">\(\lambda\in\Lambda\)</span>, the non-zero vector <span class="math inline">\(\mathbf{x}\in\mathbb{C}^{n}\)</span> that satisfies <span class="math inline">\(\mathbf{A}\mathbf{x}=\lambda\mathbf{x}\)</span> is an <em>eigenvector</em>.</li>
</ul>
<p>The definition says that solving the <strong>characteristic polynomial</strong> gives us the <strong>eigenvalues</strong> <span class="math inline">\(\lambda\)</span>. Given the <strong>eigenvalues</strong> <span class="math inline">\(\lambda\)</span>, the solution of <span class="math inline">\(\mathbf{A}\mathbf{x}=\lambda\mathbf{x}\)</span> gives <strong>eigenvectors</strong>. All the information of the transformation <span class="math inline">\(\mathbf{A}\mathbf{x}^{*}\)</span> is now preserved by <span class="math inline">\(\lambda \mathbf{x}^{*}\)</span>. In other words, the information of linear transformations <span class="math inline">\(\mathbf{A}\)</span> is stored by the pairs <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\mathbf{x}^{*}\)</span>.</p>
<p>Here is a straightforward property about the eigenvalues. Because <span class="math inline">\(\mbox{det}(\lambda\mathbf{I}-\mathbf{A})=0\)</span>, the determinant fact 7 implies <span class="math inline">\(\mbox{det}(\lambda\mathbf{I})=\mbox{det}(\mathbf{A})\)</span>. Notice that <span class="math inline">\((\lambda\mathbf{I}\)</span> is a diagonal matrix, so <span class="math inline">\(\mbox{det}(\lambda\mathbf{I})=\lambda_{1}\lambda_{2}\cdots\lambda_{n}\)</span>.<label for="tufte-sn-218" class="margin-toggle sidenote-number">218</label><input type="checkbox" id="tufte-sn-218" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">218</span> Also <span class="math inline">\(\mbox{tr}(\mathbf{A})=\lambda_{1}+\cdots+\lambda_{n}\)</span>.</span> We will consider more examples about eigenvalues and eigenvectors in the following sections. So far we focus on the illumination of <strong>invariant property</strong> given by the <strong>characteristic polynomial</strong> and its <strong>determinant</strong>.<label for="tufte-sn-219" class="margin-toggle sidenote-number">219</label><input type="checkbox" id="tufte-sn-219" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">219</span> Our exposition introduced matrices first and <strong>determinants</strong> later, however the historical order of discovery was the opposite: determinants were known long before matrices. Also, the recognition of an invariant as an quantitative object in its own interest emerged together with the determinants (characteristic polynomials).</span></p>
<p>Perphas, the most familiar <strong>invariant</strong> for a polynomial is the discriminant <span class="math inline">\(b^{2}-ac\)</span> from a quadratic form (a bivariate polynomial) <span class="math display">\[ax_{2}^{2}+2bx_{1}x_{2}+cx_{2}^{2}.\]</span> The discriminant indicates properties of the roots for this bivariate polynomial regardless the specific values of <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span> and <span class="math inline">\(c\)</span>. One feature about this <strong>invariant</strong> is that after any linear transformation vector <span class="math inline">\([x_{1},x_{2}]\)</span>, the discriminat <span class="math inline">\(b^{2}-ac\)</span> still works.</p>
<p>The discriminant of bivariate polynomials illuminates that searching the invariant property may receive more rewards than calculating a specific solution. The solution of the polynomial varies when the polynomial is changed by the structure (coefficients), but the discriminant is calculated under the same formula, and it also determine the new solution values. Thus, when we face various systems, it may be more worthy to find the invariant shared by these systems than to solve each system one by one.</p>
<p>In an economy of <span class="math inline">\(n\)</span>-sectors, one can model both the supply and the demand system of this economy by polynomials. That is, if we have one system, say the supply system, of <span class="math inline">\(n\)</span>-factors (variables), and we expect that there is also another corresponding but different system of the same factors.<label for="tufte-sn-220" class="margin-toggle sidenote-number">220</label><input type="checkbox" id="tufte-sn-220" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">220</span> The factors in different sides may require different quantities, so these two system cannot have the same form.</span> Suppose the factor in the demand side (output) has to be transformed from the supply side (input). An <strong>invariant</strong> of this economy means that the transformation for the factors can equate the supply system with the demand one for all <span class="math inline">\(n\)</span>-sectors. We use a quantitative model to illustrate how to identify the <strong>invariant</strong>.</p>
<div class="solution">
<p class="solution-begin">
Invariants in the multivariate polynomial system
</p>
<div class="solution-body">
<p>A general <strong>multivariate polynomial</strong> system in the <a href="ch-MatComp.html#sub:vecSpaces">vector space</a> <span class="math inline">\(\mathcal{V}\subset\mathbb{R}^n\)</span> allows an arbitrary <span class="math inline">\(n\)</span>-unknowns, namely an <span class="math inline">\(n\)</span>-length vector <span class="math inline">\(\mathbf{x}=[x_1,\dots,x_n]\)</span>. Each term of such a polynomial is called the <strong>monomial</strong><label for="tufte-sn-221" class="margin-toggle sidenote-number">221</label><input type="checkbox" id="tufte-sn-221" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">221</span> One widely used function of modelling productions and utilities is the <em>Cobb-Douglas function</em>. Its standard form is <span class="math display">\[y = c(x_1)^{\alpha_1}(x_2)^{\alpha_2}\]</span> where <span class="math inline">\(y\)</span> is the output of the function and <span class="math inline">\(\mathbf{x}\)</span> can be interpreted as the production factors, such as labor and capital, or it can be a consumption plan of two different goods.</span> <span class="math display">\[x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}\cdots x_{n}^{\alpha_{n}},\quad  \mbox{ where }  \mathbf{x}=[x_{1},\cdots,x_{n}]\in\mathcal{V}, \,\, \alpha_{i}\in\mathbb{N}^{+}.\]</span> Here <span class="math inline">\(\alpha_{i}\in\mathbb{N}^{+}\)</span> means these power indexes are positive integers. We consider a system of <strong>multivariate polynomials</strong> with <span class="math inline">\(n\)</span>-unknowns and with the total order of the monomial less than <span class="math inline">\(n\)</span>:
<span class="math display">\[\mathcal{P}_{n}(\mathcal{V})=\left\{ \left.f(\mathbf{x})=\sum_{i}c_{i}x_{1}^{\alpha_{i,1}}x_{2}^{\alpha_{i.2}}\cdots x_{n}^{\alpha_{i,n}}\right|\,\mathbf{x}\in\mathcal{V},\quad\alpha_{i,1}+\cdots+\alpha_{i,n}\leq n,\:\alpha_{i,j},\:\alpha_{i,j}\in\mathbb{N}^{+} \right\} \]</span></p>
<p>In this general <span class="math inline">\(n\)</span>-th order <strong>multivariate polynomial</strong> system, the invariants under the linear transformation of <span class="math inline">\(\mathbf{x}\)</span> relate to the determinants of the transformation. Suppose the vector <span class="math inline">\(\mathbf{y}\)</span> is linearly transformed from the vector <span class="math inline">\(\mathbf{x}\)</span> by <span class="math inline">\(\mathbf{A}\)</span>, such that <span class="math inline">\(\mathbf{y}=\mathbf{A}\mathbf{x}\)</span> with <span class="math inline">\(\mathbf{A}=[a_{ij}]_{n\times n}\)</span>. Assume that the supply function <span class="math inline">\(p(\mathbf{x})\)</span>, and the demand function <span class="math inline">\(q(\mathbf{y})\in\mathcal{P}_{n}(\mathcal{V})\)</span> are two multivariate polynomials with <span class="math inline">\(n\)</span> unknown quantities in <span class="math inline">\(n\)</span>-sectors.</p>
<p>If there is an <strong>invariant</strong> in <span class="math inline">\(p(\mathbf{x})\)</span> and <span class="math inline">\(q(\mathbf{y})\)</span> by making <span class="math inline">\(p(\mathbf{x})=q(\mathbf{y})\)</span>, then any <a href="sub-calculus.html#sub:diffInt">infinitesimal changes</a> of <span class="math inline">\(p(\mathbf{x})\)</span> and <span class="math inline">\(q(\mathbf{x})\)</span> should not violate the equality. So we expect that after taking the partial derivatives the equality still holds:
<span class="math display">\[
\begin{align*}
\frac{\partial p}{\partial y_{1}}\frac{\partial y_{1}}{\partial x_{1}}+\cdots+&amp;\frac{\partial p}{\partial y_{n}}\frac{\partial y_{n}}{\partial x_{1}}   =\frac{\partial q}{\partial x_{1}},\\
\frac{\partial p}{\partial y_{1}}\frac{\partial y_{1}}{\partial x_{2}}+\cdots+&amp;\frac{\partial p}{\partial y_{n}}\frac{\partial y_{n}}{\partial x_{2}}   =\frac{\partial q}{\partial x_{2}},\\
\vdots &amp; \qquad \quad  \vdots      \\
\frac{\partial p}{\partial y_{1}}\frac{\partial y_{1}}{\partial x_{1}}+\cdots+&amp;\frac{\partial p}{\partial y_{n}}\frac{\partial y_{n}}{\partial x_{n}}   =\frac{\partial q}{\partial x_{n}}.
\end{align*}
\]</span></p>
<p>Because <span class="math inline">\(\mathbf{y}=\mathbf{A}\mathbf{x}\)</span> or
<span class="math display">\[
\begin{align*}
a_{11}x_{1}+\cdots+a_{1n}x_{n}  &amp;=y_{1},\\
a_{21}x_{1}+\cdots+a_{2n}x_{n}  &amp;=y_{2},\\
\vdots\qquad    \quad &amp; \vdots \\
a_{m1}x_{1}+\cdots+a_{mn}x_{n}  &amp;=y_{n},
\end{align*}
\]</span></p>
<p>we can see that <span class="math inline">\(\partial y_{i}/\partial x_{j}=a_{ij}\)</span>.
So the above system can be expressed as <span class="math inline">\(\mathbf{A}\nabla p=\nabla q\)</span>, where <span class="math inline">\(\nabla p=[\partial p/\partial y_{i}]_{n\times1}\)</span> and <span class="math inline">\(\nabla q=[\partial q/\partial x_{i}]_{n\times1}\)</span>
are the <a href="ch-vecMat.html#sub:linearity">gradient vectors</a>. If the <a href="ch-vecMat.html#sub:linearity">infinitesimal changes</a> of <span class="math inline">\(q(\mathbf{x})\)</span>, namely <span class="math inline">\(\nabla q\)</span>, is zero, then we expect <span class="math inline">\(\nabla p\)</span> to be zero. In other words, the linear system <span class="math inline">\(\mathbf{A}\nabla p=0\)</span> has the solution <span class="math inline">\(\nabla p=0\)</span> if and only if <span class="math inline">\(\mbox{det}(\mathbf{A})\neq0\)</span>. (Statement 4 in section <a href="ch-MatComp.html#sub:matInv">11.2</a>.)</p>
<p>Thus the condition <span class="math inline">\(\mbox{det}(\mathbf{A})\neq0\)</span> is to ensure the existence of the <strong>invariant</strong> in this system.</p>
<p>The idea of this method, an algebra perspective of bridging the invariance and the determinant of the partially differentiable transformation (the coefficient matrix <span class="math inline">\(\mathbf{A}\)</span>), was first proposed by <span class="citation">Boole (<a href="bibliography.html#ref-Boole1841">1841</a>)</span>.<label for="tufte-sn-222" class="margin-toggle sidenote-number">222</label><input type="checkbox" id="tufte-sn-222" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">222</span> Some British mathematicians even said the study of partial differential equations was initiated by the exploration of the invariant. I am not sure how proper the statement is, but indeed many the early efforts of looking for an invariant were on the path of searching for an equivalent solution of some system of partial differential equations.</span></p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<div class="solution">
<p class="solution-begin">
Invariants of the discriminant in the bivariate polynomial <span id="sol-start-46" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-46', 'sol-start-46')"></span>
</p>
<div id="sol-body-46" class="solution-body" style="display: none;">
<p>Consider the quadratic form
<span class="math display">\[p(\mathbf{x})=ax_{2}^{2}+2bx_{1}x_{2}+cx_{2}^{2}\in\mathcal{P}_{2}(\mathcal{V}).\]</span>
Taking the derivatives of <span class="math inline">\(p(\mathbf{x})\)</span> with respect to <span class="math inline">\(x_{1}\)</span> and <span class="math inline">\(x_{2}\)</span>
and setting to zero, we have
<span class="math display">\[
\begin{align*}
\frac{\partial p}{\partial x_{1}}=2(ax_{1}+bx_{2})  &amp;=0, \\
\frac{\partial p}{\partial x_{2}}=2(bx_{1}+cx_{2})  &amp;=0.
\end{align*}
\]</span>
The <a href="ch-MatComp.html#sub:GElimination">Gaussian elimination</a> says that
<span class="math display">\[
\begin{align*}
(ab-ab)x_{1}+(b^{2}-ac)x_{2}    &amp;=0,\\
(ac-b^{2})x_{1}+(bc-bc)x_{2}    &amp;=0,
\end{align*}
\]</span>
which reaches the form of the discriminant <span class="math inline">\(b^{2}-ac=0\)</span>.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Another important implication from the definition of <strong>characteristic polynomial</strong> is that the <strong>eigenvalues</strong>, as the solutions of the <strong>characteristic polynomial</strong>, belong to the <strong>complex number field</strong> <span class="math inline">\(\mathbb{C}\)</span>, for both cases of the real valued matrix and the complex valued matrix <span class="math inline">\(\mathbf{A}\)</span>. This implication comes from the <strong>fundamental theorem of algebra</strong>.</p>
<ul>
<li>
<em>Fundamental theorem of algebra</em>: For a <span class="math inline">\(n\)</span>-th order polynomial <span class="math inline">\(f(x)\)</span>=<span class="math inline">\(a_{0}+a_{1}x+\cdots a_{n}x^{n}\)</span>
with real (or complex) number coefficients <span class="math inline">\(a_{0},\dots,a_{n}\)</span>, the polynomial equation <span class="math inline">\(f(x)=0\)</span> always has a solution in the complex field <span class="math inline">\(\mathbb{C}\)</span>.</li>
</ul>
<p>By the theorem, we can deduce that the complex number system is in the sense of a complete system such that we can solve any polynomial equation in it. The highest degree of a polynomial gives you the highest possible number of distinct complex roots for the polynomial.</p>
<p>Due to the importance of complex number system, hereby we review some basic concepts and provide some interpretations about the <em>complex number system</em> <span class="math display">\[\mathbb{C}=\{a+\mbox{i}b:\, a,b\in\mathbb{R}\}.\]</span>
For a complex number <span class="math inline">\(z\in\mathbb{C}\)</span>, <span class="math inline">\(z=a+\mbox{i}b\)</span> is the standard form. The <em>real part</em> of <span class="math inline">\(z\)</span> is written as <span class="math inline">\(\mbox{Re}(z)=a\)</span>, and the <em>imaginary part</em> is written as <span class="math inline">\(\mbox{Im}(z)=b\)</span>. The imaginary number <span class="math inline">\(\mbox{i}\)</span> satisfies the algebraic identity <span class="math inline">\(\mbox{i}^{2}=-1\)</span>. The <em>complex conjugate</em> of <span class="math inline">\(z\)</span> is defined as <span class="math inline">\(\bar{z}=a-\mbox{i}b\)</span>. Two complex numbers are equal precisely when they have the same <strong>real part</strong> and the same <strong>imaginary part</strong>.</p>
<p>One can also consider that the complex numbers as <a href="sub-set-theory.html#sub:order">ordered pairs</a> of <a href="sub-continuity.html#sub:completeness">real numbers</a> satisfying the <em>law of complex arithmetic</em>. As complex numbers behave like ordered pairs of real numbers, they can be identified with points in the plane. Real numbers go along the <span class="math inline">\(x\)</span>-axis, and imaginary numbers are on the <span class="math inline">\(y\)</span>-axis. This gives the complex plane. Arithmetic of addition and subtraction in <span class="math inline">\(\mathbb{C}\)</span> is carried out like adding and subtracting vectors in <span class="math inline">\(\mathbb{R}^{2}\)</span>. Arithmetic of multiplication in <span class="math inline">\(\mathbb{C}\)</span> and the norm, however, are different those of vectors in <span class="math inline">\(\mathbb{R}^{2}\)</span>.<label for="tufte-sn-223" class="margin-toggle sidenote-number">223</label><input type="checkbox" id="tufte-sn-223" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">223</span>  The <em>law of complex arithmetic</em> includes addition <span class="math inline">\((a+\mbox{i}b)+(c+\mbox{i}d)=(a+c)+(b+d)\mbox{i}\)</span>, multiplication <span class="math inline">\((a+\mbox{i}b)(c+\mbox{i}d)=(ac-bd)+(ad+bc)\mbox{i}\)</span>, <a href="ch-vecMat.html#sub:vec">norm</a> or absolute value <span class="math inline">\(|a+\mbox{i}b|=\sqrt{a^{2}+b^{2}}=\sqrt{(a+\mbox{i}b)(a-\mbox{i}b)}\)</span>.</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:complexPolar"></span>
<img src="fig/Part3/complexPolar.png" alt="Rotation of complex numbers" width="100%"><!--
<p class="caption marginnote">-->Figure 12.4: Rotation of complex numbers<!--</p>-->
<!--</div>--></span>
</p>
<p>For the multiplication of complex numbers, it may be more straightforward to consider under the <a href="sub-inferknow.html#sub:histgeo">polar coordinate</a>. For a complex number <span class="math inline">\(z=a+\mbox{i}b\)</span> with <span class="math inline">\(|z|=r\)</span>, its polar form is <span class="math inline">\(z=r\mbox{e}^{\mbox{i}\theta}\)</span>
or <span class="math inline">\(z=r\cos\theta+\mbox{i}r\sin\theta\)</span>, [Euler identity showes that the exponent of an imaginary number power can be turned into the sines and cosines of trigonometry via <span class="math display">\[\mbox{e}^{\mbox{i}\pi}=-1.\]</span> The identity comes from the Euler formula <span class="math inline">\(\mbox{e}^{\mbox{i}\theta}=\cos\theta+\mbox{i}\sin\theta\)</span>. Thus, any point on a plane comes with the polar coordinate pair <span class="math inline">\((r\cos\theta,r\sin\theta)\)</span>. ] where the real number <span class="math inline">\(r\)</span> represents the absolute value (norm) of <span class="math inline">\(z\)</span>, namely <span class="math inline">\(r=|z|=\sqrt{a^{2}+b^{2}}\)</span>, and <span class="math inline">\(\theta\)</span> represents the angle.</p>
<p>From figure <a href="ch-eigen.html#fig:complexPolar">12.4</a>, we can see that a rotation of the plane on <span class="math inline">\(\mathbb{R}^{2}\)</span> around the origin through angle <span class="math inline">\(\theta\)</span> is a linear transformation that sends the basis vectors <span class="math inline">\([1,0]\)</span> and <span class="math inline">\([0,1]\)</span> to <span class="math inline">\([\cos\theta,\sin\theta]\)</span> and <span class="math inline">\([-\sin\theta,\cos\theta]\)</span>, respectively. Let’s denote this transofrmation <span class="math inline">\(\mathbf{T}_{\theta}\)</span> by <span class="math display">\[\mathbf{T}_{\theta}\left(x\left[\begin{array}{c}
1\\
0
\end{array}\right]+y\left[\begin{array}{c}
0\\
1
\end{array}\right]\right)=\left[\begin{array}{c}
x\cos\theta-y\sin\theta\\
x\sin\theta+y\cos\theta
\end{array}\right].\]</span>
In fact, this linear transformation can be represented by the matrix <span class="math display">\[\mathbf{T}_{\theta}=\left[\begin{array}{cc}
\cos\theta &amp; -\sin\theta\\
\sin\theta &amp; \cos\theta
\end{array}\right].\]</span></p>
<p>We call <span class="math inline">\(\mathbf{T}_{\theta}\)</span> the <em>rotation matrix</em> with the angle <span class="math inline">\(\theta\)</span>. The multiplication of two <strong>rotation matrices</strong> <span class="math inline">\(\mathbf{T}_{\theta_{1}}\)</span> and <span class="math inline">\(\mathbf{T}_{\theta_{2}}\)</span> gives the same result as the <strong>rotation matrix</strong> <span class="math inline">\(\mathbf{T}_{\theta_{1}+\theta_{2}}\)</span>. For an arbitrary point <span class="math inline">\((x,y)\)</span> on the complex plane,<span class="math display">\[(\cos\theta+\mbox{i}\sin\theta)(x+\mbox{i}y)=x\cos\theta-y\sin\theta+\mbox{i}(x\sin\theta+y\cos\theta).\]</span>
That is, each rotation <span class="math inline">\(\mathbf{T}_{\theta}\)</span> in <span class="math inline">\(\mathbb{R}^{2}\)</span> can represent the complex number <span class="math inline">\(\cos\theta+\mbox{i}\sin\theta\)</span>.</p>
<p>By decomposing the <strong>rotation matrix</strong> <span class="math display">\[\mathbf{T}_{\theta}=\cos\theta\underset{\mathbf{B}_{1}}{\underbrace{\left[\begin{array}{cc}
1 &amp; 0\\
0 &amp; 1
\end{array}\right]}}+\sin\theta\underset{\mathbf{B}_{2}}{\underbrace{\left[\begin{array}{cc}
0 &amp; -1\\
1 &amp; 0
\end{array}\right]}},\]</span>
we represent the rotation by its basis matrices <span class="math inline">\(\mathbf{B}_1\)</span> and <span class="math inline">\(\mathbf{B}_2\)</span>. One can verify that these two matrices behave exactly the same as the numbers 1 (<span class="math inline">\(\mathbf{B}_1=\mathbf{I}\)</span>) and <span class="math inline">\(\mbox{i}\)</span> (<span class="math inline">\(\mathbf{B}_2 \mathbf{B}_2= - \mathbf{I}\)</span>). In fact, any <span class="math display">\[\left[\begin{array}{cc}
a &amp; -b\\
b &amp; a
\end{array}\right]=a\left[\begin{array}{cc}
1 &amp; 0\\
0 &amp; 1
\end{array}\right]+b\left[\begin{array}{cc}
0 &amp; -1\\
1 &amp; 0
\end{array}\right],\; a,b\in\mathbb{R}\]</span>
behaves exactly the same as the complex number <span class="math inline">\(a+\mbox{i}b\)</span> under addition and multiplication. In other words, all complex numbers can be represented by these <span class="math inline">\(2\times2\)</span> real matrices <span class="math inline">\(a\mathbf{B}_{1}+b\mathbf{B}_{2}\)</span>.</p>
<p>The <strong>determinant</strong> of real matrices <span class="math inline">\(a\mathbf{B}_{1}+b\mathbf{B}_{2}\)</span> corresponds to the squared <a href="ch-vecMat.html#sub:vec">norm</a> (squared absolute value) <span class="math display">\[\mbox{det}(a\mathbf{B}_{1}+b\mathbf{B}_{2})=|a+\mbox{i}b|^{2}=a^{2}+b^{2}\]</span>. Thus, the multiplicative property of <span class="math inline">\(|z_{1}z_{2}|=|z_{1}||z_{2}|\)</span> for <span class="math inline">\(z_{1},z_{2}\in\mathbb{C}\)</span> can be deduced by the multiplicative property of <strong>determinants</strong> (fact 6).<label for="tufte-sn-224" class="margin-toggle sidenote-number">224</label><input type="checkbox" id="tufte-sn-224" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">224</span> The inverse <span class="math inline">\(z^{-1}=(a-\mbox{i}b)/(a^{2}+b^{2})\)</span>
corresponds to the inverse matrix <span class="math display">\[\left[\begin{array}{cc}
a &amp; -b\\
b &amp; a
\end{array}\right]^{-1}=\frac{1}{a^{2}+b^{2}}\left[\begin{array}{cc}
a &amp; b\\
-b &amp; a
\end{array}\right].\]</span></span></p>
</div>
</div></body></html>

<p style="text-align: center;">
<a href="ch-MatComp.html"><button class="btn btn-default">Previous</button></a>
<a href="bibliography.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-05-30
</p>
</div>
</div>



</body>
</html>
