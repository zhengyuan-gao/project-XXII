<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="12 Eigenvalues and Eigenvectors | Project XXII" />
<meta property="og:type" content="book" />





<meta name="author" content="Zhengyuan Gao" />

<meta name="date" content="2020-06-20" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="12 Eigenvalues and Eigenvectors | Project XXII">

<title>12 Eigenvalues and Eigenvectors | Project XXII</title>

<link href="libs/tufte-css/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding/datatables.js"></script>
<link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>



</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Project XXII<p><p class="author">Zhengyuan Gao</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BA; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Preface</a>
<a href="part-i-inference-based-upon-logic-and-logical-computation.html">PART I: Inference Based upon Logic and Logical Computation</a>
<a href="sub-logic.html"><span class="toc-section-number">1</span> Logic</a>
<a href="sub-set-theory.html"><span class="toc-section-number">2</span> Set Theory</a>
<a href="sub-axioms.html"><span class="toc-section-number">3</span> Axioms</a>
<a href="sub-inferknow.html"><span class="toc-section-number">4</span> Inference and Knowledge</a>
<a href="sub-incomplete.html"><span class="toc-section-number">5</span> Incompleteness</a>
<a href="part-ii-infinitesimal-changes-and-their-consequences.html">PART II: Infinitesimal Changes and their Consequences</a>
<a href="sub-continuity.html"><span class="toc-section-number">6</span> Continuity</a>
<a href="sub-calculus.html"><span class="toc-section-number">7</span> Calculus</a>
<a href="ch-DE.html"><span class="toc-section-number">8</span> Differential Equations</a>
<a href="ch-CalUn.html"><span class="toc-section-number">9</span> Calculus under Uncertainty</a>
<a href="part-iii-emergence-of-abstract-interactions.html">PART III: Emergence of Abstract Interactions</a>
<a href="ch-vecMat.html"><span class="toc-section-number">10</span> Vector and Matrix</a>
<a href="ch-MatComp.html"><span class="toc-section-number">11</span> Matrix Computation</a>
<a id="active-page" href="ch-eigen.html"><span class="toc-section-number">12</span> Eigenvalues and Eigenvectors</a><ul class="toc-sections">
<li class="toc"><a href="#sub:det"> Determinants, Characteristic Polynomials, and Complex Numbers</a></li>
<li class="toc"><a href="#sub:diag"> Diagonalization in Dynamical Systems</a></li>
<li class="toc"><a href="#sub:alchemy"> * Miscellaneous: Alchemy and Values</a></li>
</ul>
<a href="bibliography.html">Bibliography</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="ch:eigen" class="section level1">
<h1>
<span class="header-section-number">12</span> Eigenvalues and Eigenvectors</h1>
<p>If we could express the change of the milieu in a computational form, then the matrix computation of <span class="math inline">\(\mathbf{A}\mathbf{x}\)</span> could describe the generation and the transformation of such milieu. The input <span class="math inline">\(\mathbf{x}\)</span> could be stimulus and cause, and the output <span class="math inline">\(\mathbf{A}\mathbf{x}\)</span> could be reaction and effect. In other words, if the information of the current world were stored in a (huge) vector <span class="math inline">\(\mathbf{x}\)</span> in an <span class="math inline">\(n\)</span>-dimensional <a href="#sub:vecSpace">field</a>, then the (linear) dynamics of this world were conducted by an <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>Figure <a href="ch-MatComp.html#fig:MatrixTransform">11.2</a> shows the transformations of vectors in a 2-dimensional <a href="#sub:vecSpace">vector space</a>. We can see that those directions and orientations (in purple and red lines) remain consistent with the transformation. By stretching and compressing the directional arrows, we can scale their magnitudes (scalars) under which the transformation looks <strong>invariant</strong>. Generally speaking, this kind of <strong>invariance</strong> exists for any <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span>, and it is analyzable under the framework of <strong>eigenvalues</strong> and <strong>eigenvectors</strong>.<label for="tufte-sn-215" class="margin-toggle sidenote-number">215</label><input type="checkbox" id="tufte-sn-215" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">215</span> For non-square <span class="math inline">\(m\times n\)</span> matrices <span class="math inline">\(\mathbf{A}\)</span>, we can use an alternative framework of singular values and singular vectors. Roughly speaking, the singular values are the positive square roots of the nonzero eigenvalues of the corresponding matrix <span class="math inline">\(\mathbf{A}^{\top}\mathbf{A}\)</span>. The corresponding eigenvectors are called the singular vectors.</span> <strong>Eigenvalues</strong> and <strong>eigenvectors</strong> can be mathematically defined as sets of <strong>invariants</strong> within processes of transformations. As matrixs can store all sorts of data information, eigenvalues and eigenvectors can in turn represent the corresponding “script lines” with clearly directional “stages”.</p>
<p>Perhaps the infinitely embodying milieu as computations of matrices makes the symbolization impossible. But as a conceptual ideal, the embodiment of eigenvalues and eigenvectors provide a way to recognize two seperated channels of forming one’s perception, the (concrete) material channel and the (imginary) spiritual one. The logic of using eigenvalues and eigenvectors - that is, the identification, the trace and maybe the selection of its various structural invariants - can reconfigure the relation of the input and output channels of the computation. Physical and metaphysical forces from the aspects in the ontology and the epistemology, form these complemenary channels in milieux.</p>
<p>As long as a secular transformation, such as a trade or a production, can be expressed in an input-output matrix form, the <strong>eigenvalue</strong> and the <strong>eigenvector</strong> can reconfigure the form as an epistemological and ontological emergence. Maybe I can call this process the <strong>valuation</strong>. The valuation here is defined as the emergence of quantitative “being” that attaches to the transformation, and remains invariant when the structure of the transformation unchanges.</p>
<div id="sub:det" class="section level2">
<h2>
<span class="header-section-number">12.1</span> Determinants, Characteristic Polynomials, and Complex Numbers</h2>
<p>The detective novel often depicts a fragmentary plot to drive the readers in a complete daze. When the readers were attracted by various bizarre and outrageous transforming scenes, the author, quite likely, had buried the solution clue in some unaltered remainders.</p>
<p>For mathematical transformations, in passing from one equation to the other, what was left unchanged is called an <strong>invariant</strong>. We will explore the invariant properties of the linear transformations through the <strong>determinant</strong>. A <em>determinant</em> of a square matrix <span class="math inline">\(\mathbf{A}\)</span>, denoted by <span class="math inline">\(\mbox{det}(\mathbf{A})\)</span>, is a function mapping <span class="math inline">\(\mathbf{A}\)</span> to a <a href="sub-continuity.html#sub:completeness">real number</a>. This real number contains the information about the product of all <a href="ch-MatComp.html#sub:GElimination">pivots</a> of the <a href="ch-MatComp.html#sub:GElimination">echelon form</a> of <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>Consider a <span class="math inline">\(2\times2\)</span> matrix <span class="math display">\[\mathbf{A}=\left[\begin{array}{cc}
a_{11} &amp; a_{12}\\
a_{21} &amp; a_{22}
\end{array}\right].\]</span>
The pivots of <span class="math inline">\(\mathbf{A}\)</span> are <span class="math inline">\(a_{11}\)</span> and <span class="math inline">\(a_{22}-(a_{21}/a_{11})a_{12}\)</span>. The product of these pivots is <span class="math inline">\(a_{11}(a_{22}-(a_{21}/a_{11})a_{12})=a_{11}a_{22}-a_{21}a_{12}\)</span>. The <strong>determinant</strong> is sometimes written with a single straight line as left and right brackets. So we would write the above result as <span class="math display">\[\mbox{det}(\mathbf{A})=\left|\begin{array}{cc}
a_{11} &amp; a_{12}\\
a_{21} &amp; a_{22}
\end{array}\right|=a_{11}a_{22}-a_{12}a_{21}.\]</span></p>
<p>If we think of <span class="math inline">\(\mathbf{A}\)</span> as two row vectors in the plane, i.e. <span class="math inline">\(\mathbf{A}=[\mathbf{a}_{1},\mathbf{a}_{2}]^{\top}\)</span>, then the <strong>determinant</strong> is the (signed) area of the parallelogram spanned by <span class="math inline">\(\mathbf{a}_{1}\)</span> and <span class="math inline">\(\mathbf{a}_{2}\)</span>. The sign of the <strong>determinant</strong> is positive (negative) if the two vectors are positively (negatively) oriented. Figure <a href="ch-eigen.html#fig:det">12.1</a> shows a parallelogram spanned by <span class="math inline">\(\mathbf{z}=[5,1]\)</span> and <span class="math inline">\(\mathbf{u}=[-1,3]\)</span>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:det"></span>
<img src="fig/Part3/det.png" alt="Determinant in a parallelogram" width="100%"><!--
<p class="caption marginnote">-->Figure 12.1: Determinant in a parallelogram<!--</p>-->
<!--</div>--></span>
</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:det"></span>
<img src="fig/Part3/det2.png" alt="Determinant in a parallelogram" width="100%"><!--
<p class="caption marginnote">-->Figure 12.1: Determinant in a parallelogram<!--</p>-->
<!--</div>--></span>
</p>
<p>For an <a href="ch-MatComp.html#sub:matInv">invertible</a> <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span>, its <a href="ch-MatComp.html#sub:GElimination">echelon form</a> says that all <a href="ch-MatComp.html#sub:GElimination">pivots</a> are non-zero<label for="tufte-sn-216" class="margin-toggle sidenote-number">216</label><input type="checkbox" id="tufte-sn-216" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">216</span> As the statement in section <a href="ch-MatComp.html#sub:matInv">11.2</a>, the <strong>determinant</strong> also relates the <a href="ch-MatComp.html#sub:matInv">invertibility</a> of <span class="math inline">\(\mathbf{A}\)</span>. A zero <strong>determinant</strong> induces a non-invertible/singular square matrix.</span>, so is the product of all pivots, namely <span class="math inline">\(\mbox{det}(\mathbf{A})\neq 0\)</span>. Geometrically, the absolute value of <span class="math inline">\(\mbox{det}(\mathbf{A})\)</span> is equal to the volume of the <span class="math inline">\(n\)</span>-dimensional parallelepiped, where the <span class="math inline">\(n\)</span>-dimensional parallelepiped spanned by the column or row vectors of <span class="math inline">\(\mathbf{A}\)</span>.<label for="tufte-sn-217" class="margin-toggle sidenote-number">217</label><input type="checkbox" id="tufte-sn-217" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">217</span> The <strong>determinant</strong> of <span class="math inline">\(\mathbf{A}\)</span> is positive or negative according to whether the linear mapping <span class="math inline">\(\mathbf{A}\mathbf{x}\)</span> preserves or reverses the orientation of the <span class="math inline">\(n\)</span>-dimensional <span class="math inline">\(\mathbf{x}\)</span>. The absolute value of the determinant can be thought of as a measure of how much multiplication by the matrix expands or contracts the space of <span class="math inline">\(\mathbf{x}\)</span>. If the determinant is <span class="math inline">\(0\)</span>, then space is contracted completely along at least one dimension, causing it to lose all of its volume. If the determinant is <span class="math inline">\(1\)</span>, then the transformation preserves volume.</span></p>
<p>Here is a list of some properties (facts) of the <strong>determinant</strong> of a square <span class="math inline">\(n\times n\)</span> matrix.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mbox{det}(\mathbf{I})=1\)</span>.</p></li>
<li><p><span class="math inline">\(\mbox{det}(\mathbf{A})=0\)</span>, if the matrix <span class="math inline">\(\mathbf{A}\)</span> is <a href="ch-MatComp.html#sub:matInv">singular</a> or <a href="ch-MatComp.html#sub:matInv">non-invertible</a>.</p></li>
<li><p>Swapping two vectors of <span class="math inline">\(\mathbf{A}\)</span> changes the sign of <span class="math inline">\(\mbox{det}(\mathbf{A})\)</span>. In other words, <span class="math inline">\(\mbox{det}(\mathbf{E}_{ij}\mathbf{A})=-\mbox{det}(\mathbf{A})\)</span> where <span class="math inline">\(\mathbf{E}_{ij}\)</span> is the <a href="ch-MatComp.html#sub:matInv">permutation elementary matrix</a>.</p></li>
<li><p>Scaling a row or a column of <span class="math inline">\(\mathcal{A}\)</span> scales the determinant. In other words, <span class="math inline">\(\mbox{det}(\mathbf{E}_{i}(c)\mathbf{A})=c \times \mbox{det}(\mathbf{A})\)</span> where <span class="math inline">\(\mathbf{E}_{i}(c)\)</span> is the <a href="ch-MatComp.html#sub:matInv">scaling elementary matrix</a>.</p></li>
<li><p>For the <a href="ch-MatComp.html#sub:matInv">replacement elementary matrix</a>, <span class="math inline">\(\mbox{det}(\mathbf{E}_{ij}(s)\mathbf{A})=\mbox{det}(\mathbf{A})\)</span>.</p></li>
</ol>
<p>Note that the <strong>determinants</strong> of triangular matrices and diagonal matrices are exactly the product of the diagonal entries in those matrices. Fact 1) comes directly from the product of ones on the diagonal of <span class="math inline">\(\mathbf{I}\)</span>. Fact 2) comes from the definition. Figure <a href="ch-eigen.html#fig:det">12.1</a> gives an example of 3). For 4), figure <a href="ch-eigen.html#fig:Scalingdet">12.2</a> shows how the scaling transformation changes the determinant (the area of the parallelogram). For fact 5), figure <a href="ch-MatComp.html#fig:GElimination">11.1</a> shows that in the <a href="ch-MatComp.html#sub:GElimination">Gaussian elimination</a> procedures, forming the <a href="ch-MatComp.html#sub:GElimination">echelon form</a> do not affect the volume. In other words, the <a href="ch-MatComp.html#sub:matInv">replacement elementary matrices</a> simply rotate the intersections of the parallelepiped, and preserve the volume.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:Scalingdet"></span>
<img src="fig/Part3/Scalingdet.gif" alt="Determinants of the scaling transformations" width="100%"><!--
<p class="caption marginnote">-->Figure 12.2: Determinants of the scaling transformations<!--</p>-->
<!--</div>--></span>
</p>
<div class="solution">
<p class="solution-begin">
Another definition of the determinant <span id="sol-start-94" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-94', 'sol-start-94')"></span>
</p>
<div id="sol-body-94" class="solution-body" style="display: none;">
<p>The <em>determinant</em> of a square <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(\mathbf{A}=[a_{ij}]\)</span> is the scalar quantity <span class="math inline">\(\mbox{det}(\mathbf{A})\)</span> that is recursively defined by the following expression: If <span class="math inline">\(n=1\)</span>, then <span class="math inline">\(\mbox{det}\mathbf{A}=a_{11}\)</span>; otherwise, it follows<span class="math display">\[\mbox{det}\mathbf{A}=\sum_{k=1}^{n}a_{k1}(-1)^{k+1}\mbox{det}(\mathbf{A}_{(k)1})\]</span>
where <span class="math inline">\(\mbox{det}(\mathbf{A}_{(k)1})\)</span> is the <strong>determinant</strong> of the <span class="math inline">\((n-1)\times(n-1)\)</span> matrix obtained from <span class="math inline">\(\mathbf{A}\)</span> by deleting the first row and the <span class="math inline">\(k\)</span>-th coloumn of <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>The idea of this formula is to split an <span class="math inline">\(n\times n\)</span> matrix by crossing out row <span class="math inline">\(1\)</span> and column <span class="math inline">\(k\)</span> to get a (sub)matrix <span class="math inline">\(\mathbf{A}_{(k)1}\)</span> of size <span class="math inline">\(n-1\)</span>. Here is an example, <span class="math display">\[\mbox{det}\left[\begin{array}{cc}
a_{11} &amp; a_{12}\\
a_{21} &amp; a_{22}
\end{array}\right]=a_{11}\mbox{det}(\mathbf{A}_{(1)1})-a_{21}\mbox{det}(\mathbf{A}_{(2)1})=a_{11}a_{22}-a_{21}a_{12}.\]</span></p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>With the previous five results, it is possible to derive further properties:</p>
<ol start="6" style="list-style-type: decimal">
<li><p><span class="math inline">\(\mbox{det}(\mathbf{A}\mathbf{B})=\mbox{det}(\mathbf{A})\mbox{det}(\mathbf{B})\)</span>.</p></li>
<li><p><span class="math inline">\(\mbox{det}(\mathbf{A}+\mathbf{B})=\mbox{det}(\mathbf{A})+\mbox{det}(\mathbf{B})\)</span>.</p></li>
<li><p><span class="math inline">\(\mbox{det}\mathbf{A}^{\top}=\mbox{det}\mathbf{A}\)</span>.</p></li>
<li><p><span class="math inline">\(\mbox{det}(\mathbf{A}^{-1})=\frac{1}{\mbox{det}(\mathbf{A})}\)</span>.</p></li>
</ol>
<div class="solution">
<p class="solution-begin">
Sketch of the proof <span id="sol-start-95" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-95', 'sol-start-95')"></span>
</p>
<div id="sol-body-95" class="solution-body" style="display: none;">
<p>For 6), notice that <span class="math inline">\(\mbox{det}(\mathbf{E}_i (c))=c\)</span>, <span class="math inline">\(\mbox{det}(\mathbf{E}_{ij})=-1\)</span>, and <span class="math inline">\(\mbox{det}(\mathbf{E}_{ij}(s))=1\)</span>. From the results of 3),4),5), we have<br><span class="math display">\[
\begin{align*}
\mbox{det}(\mathbf{E}_{i}(c)\mathbf{A})&amp;=\mbox{det}(\mathbf{E}_{i}(c)) \mbox{det}(\mathbf{A})=c\times \mbox{det}(\mathbf{A}),\\
\mbox{det}(\mathbf{E}_{ij}\mathbf{A})&amp;=\mbox{det}(\mathbf{E}_{ij}) \mbox{det}(\mathbf{A})=-\mbox{det}(\mathbf{A}),\\
\mbox{det}(\mathbf{E}_{ij}(s)\mathbf{A})&amp;=\mbox{det}(\mathbf{E}_{ij}(s))\times \mbox{det}(\mathbf{A})=\mbox{det}.
\end{align*}
\]</span></p>
<p>Therefore, all mutiplications of the square matrix <span class="math inline">\(\mathbf{A}\)</span> by the elementary matrices <span class="math inline">\(\mathbf{E}\)</span> satisfy <span class="math inline">\(\mbox{det}(\mathbf{E}\mathbf{A})=\mbox{det}(\mathbf{E}_{i})\mbox{det}(\mathbf{A}).\)</span> Because <span class="math inline">\(\mathbf{A}\)</span> is invertible, we can express it as a product of elementary matrices (see the proof in section <a href="ch-MatComp.html#sub:matInv">11.2</a>), say <span class="math inline">\(\mathbf{A}=\mathbf{E}^{(1)}\mathbf{E}^{(2)}\cdots\mathbf{E}^{(k)}\)</span>. Then for <span class="math inline">\(\mathbf{A}\mathbf{B}\)</span>, we have
<span class="math display">\[
\begin{align*}
\mbox{det}(\mathbf{A}\mathbf{B})&amp;=\mbox{det}(\mathbf{E}^{(1)}\mathbf{E}^{(2)}\cdots\mathbf{E}^{(k)}B)\\
&amp;=\mbox{det}(\mathbf{E}^{(1)})\mbox{det}(\mathbf{E}^{(2)})\cdots\mbox{det}(\mathbf{E}^{(k)})\mbox{det}\mathbf{B}\\
&amp;=\mbox{det}(\mathbf{E}^{(1)}\cdots\mathbf{E}^{(k)})\mbox{det}(\mathbf{B})=\mbox{det}(\mathbf{A})\mbox{det}(\mathbf{B}).
\end{align*}
\]</span>
The result follows.</p>
<p>For 7), see figure <a href="ch-eigen.html#fig:detAdd">12.3</a>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:detAdd"></span>
<img src="fig/Part3/detAdd.png" alt="Determinant of the matrix addition" width="100%"><!--
<p class="caption marginnote">-->Figure 12.3: Determinant of the matrix addition<!--</p>-->
<!--</div>--></span>
</p>
<p>For 8), Similar to the proof for 6). Notice that <span class="math inline">\(\mbox{det}(\mathbf{E}^{\top}_i (c))=c\)</span>, <span class="math inline">\(\mbox{det}(\mathbf{E}^{\top}_{ij})=-1\)</span>, and <span class="math inline">\(\mbox{det}(\mathbf{E}^{\top}_{ij}(s))=1\)</span>. Because the invertible <span class="math inline">\(\mathbf{A}\)</span> can be expressed by a product of elementary matrices (see the proof in section <a href="ch-MatComp.html#sub:matInv">11.2</a>), say <span class="math inline">\(\mathbf{A}=\mathbf{E}^{(1)}\mathbf{E}^{(2)}\cdots\mathbf{E}^{(k)}\)</span>, and because <span class="math inline">\(\mathbf{A}^{\top}=(\mathbf{E}^{(1)})^{\top}(\mathbf{E}^{(2)})^{\top}\cdots(\mathbf{E}^{(k)})^{\top}\)</span>, so by the result of 6), we have
<span class="math display">\[
\begin{align*}
\mbox{det}(\mathbf{A})&amp;=\mbox{det}((\mathbf{E}^{(1)})^{\top}(\mathbf{E}^{(2)})^{\top}\cdots(\mathbf{E}^{(k)})^{\top})\\
&amp;=\mbox{det}((\mathbf{E}^{(1)})^{\top})\cdots\mbox{det}((\mathbf{E}^{(k)})^{\top})\\
&amp;=\mbox{det}(\mathbf{E}^{(1)}\cdots\mathbf{E}^{(k)})\mbox{det}=\mbox{det}(\mathbf{A}).
\end{align*}
\]</span></p>
<p>For 9), note that the identity <span class="math inline">\(\mathbf{A}\mathbf{A}^{-1}=\mathbf{I}\)</span>
gives<span class="math display">\[\mbox{det}(\mathbf{A}\mathbf{A}^{-1})=\mbox{det}(\mathbf{A})\mbox{det}(\mathbf{A}^{-1})=\mathbf{I}.\]</span>
The result follows.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>To see the use of determinant, let’s return to the <a href="ch-vecMat.html#sub:linearity">fixed point problem</a> in the matrix form <span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{x}\)</span> where the transformation is invariant around the solution <span class="math inline">\(\mathbf{x}^{*}\)</span>. Suppose <span class="math inline">\(\mathbf{A}\)</span> is an <span class="math inline">\(n\times n\)</span> <a href="ch-MatComp.html#sub:matInv">invertible</a> matrix, we will see that the existence of <span class="math inline">\(\mathbf{x}^{*}\)</span> can be determinated by the <strong>determinant</strong> of <span class="math inline">\(\mathbf{A}\)</span>.</p>
<p>Notice that the fixed point problem can be presented as <span class="math inline">\(0=\mathbf{x}-\mathbf{A}\mathbf{x}\)</span> or <span class="math display">\[0=(\mathbf{I}-\mathbf{A})\mathbf{x}.\]</span> The statement 4) in section <a href="ch-MatComp.html#sub:matInv">11.2</a> says that for an invertible matrix <span class="math inline">\(\mathbf{I}-\mathbf{A}\)</span>, <span class="math inline">\((\mathbf{I}-\mathbf{A})\mathbf{x}=0\)</span> has a unique solution <span class="math inline">\(\mathbf{x}^{*}=0\)</span>. This statement implies that if we are looking for a non-zero solution of <span class="math inline">\(\mathbf{x}^{*}\)</span>, we should expect that <span class="math inline">\(\mathbf{(\mathbf{I}-\mathbf{A})}\)</span> to be <a href="ch-MatComp.html#sub:matInv">singular</a> or non-invertible, namely <span class="math inline">\(\mbox{det}(\mathbf{I}-\mathbf{A})=0\)</span>. In this way, the existence of a non-zero fixed point solution <span class="math inline">\(\mathbf{x}^{*}\)</span> is equivalent to a zero <strong>determinant</strong> of <span class="math inline">\(\mathbf{I}-\mathbf{A}\)</span>.</p>
<p>The linear transformation of the <a href="ch-vecMat.html#sub:linearity">fixed point problem</a> <span class="math inline">\(\mathbf{A}\mathbf{x}^{*}=1\times \mathbf{x}^{*}\)</span> preserve the invariant <span class="math inline">\(n\)</span>-vector <span class="math inline">\(\mathbf{x}^{*}\)</span>. We can call the vector <span class="math inline">\(\mathbf{x}^{*}\)</span> the <strong>eigenvector</strong> of <span class="math inline">\(\mathbf{A}\)</span> with <strong>eigenvalue</strong> <span class="math inline">\(1\)</span>. The formal definition of <strong>eigenvector</strong> and <strong>eigenvalue</strong> is given by the <strong>characteristic polynomial</strong>.</p>
<ul>
<li>
<strong>Eigenvector</strong>, <strong>eigenvalue</strong>, <strong>characteristic polynomial</strong> : Consider an <span class="math inline">\(n\times n\)</span>
matrix <span class="math inline">\(\mathbf{A}\in\mathbb{F}^{n\times n}\)</span>, where the <a href="ch-MatComp.html#sub:vecSpaces">field</a> <span class="math inline">\(\mathbb{F}\)</span> can be <span class="math inline">\(\mathbb{C}\)</span>
or <span class="math inline">\(\mathbb{R}\)</span>. The <em>eigenvalues</em> of <span class="math inline">\(\mathbf{A}\)</span>
are the <span class="math inline">\(n\)</span> roots, <span class="math inline">\(\Lambda=\{\lambda_{1},\dots,\lambda_{n}\}\)</span>, of its <em>characteristic polynomial</em> <span class="math display">\[\mbox{det}(\lambda\mathbf{I}-\mathbf{A}).\]</span> The set of these roots is called the <em>spectrum</em> of <span class="math inline">\(\mathbf{A}\)</span>, and it can also be written as<span class="math display">\[\mathcal{S}=\{\lambda \in {\mathbb{C}} \,:\mbox{det}(\lambda\mathbf{I}-\mathbf{A})=0\}.\]</span>
For any <span class="math inline">\(\lambda\in\mathcal{S}\)</span>, the non-zero vector <span class="math inline">\(\mathbf{v}\in\mathbb{C}^{n}\)</span> that satisfies <span class="math inline">\(\mathbf{A}\mathbf{v}=\lambda\mathbf{v}\)</span> is an <em>eigenvector</em>.</li>
</ul>
<p>The definition says that solving the <strong>characteristic polynomial</strong> gives us the <strong>eigenvalues</strong> <span class="math inline">\(\lambda\)</span>. Given the <strong>eigenvalues</strong> <span class="math inline">\(\lambda\)</span>, the solution of <span class="math inline">\(\mathbf{A}\mathbf{v}=\lambda\mathbf{v}\)</span> (or as a fixed point problem: <span class="math inline">\((\mathbf{A}/\lambda)\mathbf{v}=\mathbf{v}\)</span>) gives the corresponding <strong>eigenvectors</strong>. All the information of the transformation <span class="math inline">\(\mathbf{A}\mathbf{v}\)</span> is now preserved by <span class="math inline">\(\lambda \mathbf{v}\)</span>. In other words, the information of linear transformations <span class="math inline">\(\mathbf{A}\)</span> is stored by the pairs <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\mathbf{v}\)</span>.</p>
<p>Here are some straightforward properties about the eigenvalues.</p>
<ul>
<li>Let <span class="math inline">\(\lambda\mathbf{I}\)</span> be <span class="math inline">\(\Lambda\)</span>. <span class="math inline">\(\mbox{det}(\mathbf{A})=\mbox{det}(\Lambda)=\lambda_{1}\lambda_{2}\cdots\lambda_{n}\)</span>.</li>
</ul>
<p>Because <span class="math inline">\(\mbox{det}(\lambda\mathbf{I}-\mathbf{A})=0\)</span>, the determinant fact 7) implies <span class="math inline">\(\mbox{det}(\lambda\mathbf{I})=\mbox{det}(\mathbf{A})\)</span>. Notice that <span class="math inline">\(\lambda\mathbf{I}=\Lambda\)</span> is a diagonal matrix, so <span class="math inline">\(\mbox{det}(\Lambda)=\lambda_{1}\lambda_{2}\cdots\lambda_{n}\)</span>.</p>
<p>We will consider more examples about eigenvalues and eigenvectors in the following sections. At the moment, we focus on the illumination of <strong>invariant property</strong> given by the <strong>characteristic polynomial</strong> and its <strong>determinant</strong>.<label for="tufte-sn-218" class="margin-toggle sidenote-number">218</label><input type="checkbox" id="tufte-sn-218" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">218</span> Our exposition introduced matrices first and <strong>determinants</strong> later, however the historical order of discovery was the opposite: determinants were known long before matrices. Also, the recognition of an invariant as an quantitative object in its own interest emerged together with the determinants (characteristic polynomials).</span>
Perphas, the most familiar <strong>invariant</strong> for a polynomial is the discriminant <span class="math inline">\(b^{2}-ac\)</span> from a quadratic form (a bivariate polynomial) <span class="math display">\[ax_{2}^{2}+2bx_{1}x_{2}+cx_{2}^{2}.\]</span> The discriminant indicates properties of the roots for this bivariate polynomial regardless the specific values of <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span> and <span class="math inline">\(c\)</span>. One feature about this <strong>invariant</strong> is that after any linear transformation vector <span class="math inline">\([x_{1},x_{2}]\)</span>, the discriminat <span class="math inline">\(b^{2}-ac\)</span> still works.</p>
<p>The discriminant of bivariate polynomials illuminates that searching the invariant property may receive more rewards than calculating a specific solution. The solution of the polynomial varies when the polynomial is changed by the structure (coefficients), but the discriminant formula remains unchanged. Moerover, the formula also determines the new solution values. Thus, when we face various systems, it may be more worthy to find the invariant shared by these systems rather than solve each system one by one.</p>
<p>In an economy of <span class="math inline">\(n\)</span>-sectors, one can model both the supply and the demand system of this economy by polynomials. That is, if we have one system, say the supply system, of <span class="math inline">\(n\)</span>-factors (variables), and we expect that there is another corresponding but different system of the same factors.<label for="tufte-sn-219" class="margin-toggle sidenote-number">219</label><input type="checkbox" id="tufte-sn-219" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">219</span> The factors in different sides may require different quantities, so these two system cannot have the same form.</span> Suppose the factor in the demand side (output) has to be transformed from the supply side (input). An <strong>invariant</strong> of this economy means that the transformation for the factors can equate the supply system with the demand one for all <span class="math inline">\(n\)</span>-sectors. We use a quantitative model to illustrate how to identify the <strong>invariant</strong>.</p>
<div class="solution">
<p class="solution-begin">
Invariants in the multivariate polynomial system
</p>
<div class="solution-body">
<p>So far, we only saw the univariate polynomial. A more general <strong>multivariate polynomial</strong> system in the <a href="ch-MatComp.html#sub:vecSpaces">vector space</a> <span class="math inline">\(\mathcal{V}\subset\mathbb{R}^n\)</span> allows arbitrary <span class="math inline">\(n\)</span>-unknowns, namely an <span class="math inline">\(n\)</span>-length variable vector <span class="math inline">\(\mathbf{x}=[x_1,\dots,x_n]\)</span>. Each term of such a polynomial is called the <strong>monomial</strong><label for="tufte-sn-220" class="margin-toggle sidenote-number">220</label><input type="checkbox" id="tufte-sn-220" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">220</span> One widely used function of modelling productions and utilities is the <em>Cobb-Douglas function</em>. Its standard form is <span class="math display">\[y = c(x_1)^{\alpha_1}(x_2)^{\alpha_2}\]</span> where <span class="math inline">\(y\)</span> is the output of the function and <span class="math inline">\(\mathbf{x}\)</span> can be interpreted as the production factors, such as labor and capital, or it can be a consumption plan of two different goods.</span> <span class="math display">\[x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}\cdots x_{n}^{\alpha_{n}},\quad  \mbox{ where }  \mathbf{x}=[x_{1},\cdots,x_{n}]\in\mathcal{V}, \,\, \alpha_{i}\in\mathbb{N}^{+}.\]</span> Here <span class="math inline">\(\alpha_{i}\in\mathbb{N}^{+}\)</span> means these power indexes are positive integers. We consider a system of <em>multivariate polynomials</em> with <span class="math inline">\(n\)</span>-unknowns and with the total order of the <strong>monomial</strong> less than <span class="math inline">\(n\)</span>:
<span class="math display">\[\mathcal{P}_{n}(\mathcal{V})=\left\{ \left.f(\mathbf{x})=\sum_{i}c_{i}x_{1}^{\alpha_{i,1}}x_{2}^{\alpha_{i.2}}\cdots x_{n}^{\alpha_{i,n}}\right|\,\mathbf{x}\in\mathcal{V},\quad\alpha_{i,1}+\cdots+\alpha_{i,n}\leq n,\:\alpha_{i,j},\:\alpha_{i,j}\in\mathbb{N}^{+} \right\} \]</span></p>
<p>In this general <span class="math inline">\(n\)</span>-th order <strong>multivariate polynomial</strong> system, the invariants under the linear transformation of <span class="math inline">\(\mathbf{x}\)</span> relate to the determinants of the transformation. Suppose the vector <span class="math inline">\(\mathbf{y}\)</span> is linearly transformed from the vector <span class="math inline">\(\mathbf{x}\)</span> by <span class="math inline">\(\mathbf{A}\)</span>, such that <span class="math inline">\(\mathbf{y}=\mathbf{A}\mathbf{x}\)</span> with <span class="math inline">\(\mathbf{A}=[a_{ij}]_{n\times n}\)</span>. Assume that the supply function <span class="math inline">\(p(\mathbf{x})\)</span>, and the demand function <span class="math inline">\(q(\mathbf{y})\in\mathcal{P}_{n}(\mathcal{V})\)</span> are two multivariate polynomials with <span class="math inline">\(n\)</span> unknown quantities in <span class="math inline">\(n\)</span>-sectors.</p>
<p>If there is an <strong>invariant</strong> in <span class="math inline">\(p(\mathbf{x})\)</span> and <span class="math inline">\(q(\mathbf{y})\)</span> by making <span class="math inline">\(p(\mathbf{x})=q(\mathbf{y})\)</span>, then any <a href="sub-calculus.html#sub:diffInt">infinitesimal changes</a> of <span class="math inline">\(p(\mathbf{x})\)</span> and <span class="math inline">\(q(\mathbf{x})\)</span> should not violate the equality. So we expect that after taking the partial derivatives the equality still holds:
<span class="math display">\[
\begin{align*}
\frac{\partial p}{\partial y_{1}}\frac{\partial y_{1}}{\partial x_{1}}+\cdots+&amp;\frac{\partial p}{\partial y_{n}}\frac{\partial y_{n}}{\partial x_{1}}   =\frac{\partial q}{\partial x_{1}},\\
\frac{\partial p}{\partial y_{1}}\frac{\partial y_{1}}{\partial x_{2}}+\cdots+&amp;\frac{\partial p}{\partial y_{n}}\frac{\partial y_{n}}{\partial x_{2}}   =\frac{\partial q}{\partial x_{2}},\\
\vdots &amp; \qquad \quad  \vdots      \\
\frac{\partial p}{\partial y_{1}}\frac{\partial y_{1}}{\partial x_{1}}+\cdots+&amp;\frac{\partial p}{\partial y_{n}}\frac{\partial y_{n}}{\partial x_{n}}   =\frac{\partial q}{\partial x_{n}}.
\end{align*}
\]</span></p>
<p>Because <span class="math inline">\(\mathbf{y}=\mathbf{A}\mathbf{x}\)</span> or
<span class="math display">\[
\begin{align*}
a_{11}x_{1}+\cdots+a_{1n}x_{n}  &amp;=y_{1},\\
a_{21}x_{1}+\cdots+a_{2n}x_{n}  &amp;=y_{2},\\
\vdots\qquad    \quad &amp; \vdots \\
a_{m1}x_{1}+\cdots+a_{mn}x_{n}  &amp;=y_{n},
\end{align*}
\]</span></p>
<p>we can see that <span class="math inline">\(\partial y_{i}/\partial x_{j}=a_{ij}\)</span>.
So the above system can be expressed as <span class="math inline">\(\mathbf{A}\nabla p=\nabla q\)</span>, where <span class="math inline">\(\nabla p=[\partial p/\partial y_{i}]_{n\times1}\)</span> and <span class="math inline">\(\nabla q=[\partial q/\partial x_{i}]_{n\times1}\)</span>
are the <a href="ch-vecMat.html#sub:linearity">gradient vectors</a>. If the <a href="ch-vecMat.html#sub:linearity">infinitesimal changes</a> of <span class="math inline">\(q(\mathbf{x})\)</span>, namely <span class="math inline">\(\nabla q\)</span>, is zero, then we expect <span class="math inline">\(\nabla p\)</span> to be zero. In other words, the linear system <span class="math inline">\(\mathbf{A}\nabla p=0\)</span> has the solution <span class="math inline">\(\nabla p=0\)</span> if and only if <span class="math inline">\(\mbox{det}(\mathbf{A})\neq0\)</span>. (Statement 4 in section <a href="ch-MatComp.html#sub:matInv">11.2</a>.)</p>
<p>Thus the condition <span class="math inline">\(\mbox{det}(\mathbf{A})\neq0\)</span> is to ensure the existence of the <strong>invariant</strong> in this system.
Then <a href="ch-MatComp.html#sub:GElimination">Gaussian elimination</a> can reduce the linear system <span class="math inline">\(\mathbf{A}\nabla p= 0\)</span> to an expression of the invariant.</p>
<p>The idea of this method, an algebra perspective of bridging the invariance and the determinant of the partially differentiable transformation (the coefficient matrix <span class="math inline">\(\mathbf{A}\)</span>), was first proposed by <span class="citation">Boole (<a href="bibliography.html#ref-Boole1841">1841</a>)</span>.<label for="tufte-sn-221" class="margin-toggle sidenote-number">221</label><input type="checkbox" id="tufte-sn-221" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">221</span> Some British mathematicians even said the study of partial differential equations was initiated by the exploration of the invariant. I am not sure how proper the statement is, but indeed many early efforts of looking for an invariant were on the path of searching for an equivalent solution of some system of partial differential equations.</span></p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<div class="solution">
<p class="solution-begin">
Invariants of the discriminant in the bivariate polynomial <span id="sol-start-97" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-97', 'sol-start-97')"></span>
</p>
<div id="sol-body-97" class="solution-body" style="display: none;">
<p>Consider the quadratic form
<span class="math display">\[p(\mathbf{x})=ax_{2}^{2}+2bx_{1}x_{2}+cx_{2}^{2}\in\mathcal{P}_{2}(\mathcal{V}).\]</span>
Taking the derivatives of <span class="math inline">\(p(\mathbf{x})\)</span> with respect to <span class="math inline">\(x_{1}\)</span> and <span class="math inline">\(x_{2}\)</span>
and setting to zero, we have
<span class="math display">\[
\begin{align*}
\frac{\partial p}{\partial x_{1}}=2(ax_{1}+bx_{2})  &amp;=0, \\
\frac{\partial p}{\partial x_{2}}=2(bx_{1}+cx_{2})  &amp;=0.
\end{align*}
\]</span>
The <a href="ch-MatComp.html#sub:GElimination">Gaussian elimination</a> says that
<span class="math display">\[
\begin{align*}
(ab-ab)x_{1}+(b^{2}-ac)x_{2}    &amp;=0,\\
(ac-b^{2})x_{1}+(bc-bc)x_{2}    &amp;=0,
\end{align*}
\]</span>
which reaches the form of the discriminant <span class="math inline">\(b^{2}-ac=0\)</span>.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Another important implication from the definition of <strong>characteristic polynomial</strong> is that the <strong>eigenvalues</strong>, as the solutions of the <strong>characteristic polynomial</strong>, belong to the <strong>complex number field</strong> <span class="math inline">\(\mathbb{C}\)</span>. This is the case for both the real valued and the complex valued matrix <span class="math inline">\(\mathbf{A}\)</span> in <span class="math inline">\(\mathbf{A}\mathbf{v}=\lambda\mathbf{v}\)</span>. This implication comes from the <strong>fundamental theorem of algebra</strong>.</p>
<ul>
<li>
<em>Fundamental theorem of algebra</em>: For a <span class="math inline">\(n\)</span>-th order polynomial <span class="math inline">\(f(x)\)</span>=<span class="math inline">\(a_{0}+a_{1}x+\cdots a_{n}x^{n}\)</span>
with real (or complex) number coefficients <span class="math inline">\(a_{0},\dots,a_{n}\)</span>, the polynomial equation <span class="math inline">\(f(x)=0\)</span> always has a solution in the complex field <span class="math inline">\(\mathbb{C}\)</span>.</li>
</ul>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:FTA"></span>
<img src="fig/Part3/FTA.gif" alt="Fundamental theorem of algebra" width="100%"><!--
<p class="caption marginnote">-->Figure 12.4: Fundamental theorem of algebra<!--</p>-->
<!--</div>--></span>
</p>
<p>The proof of theorem is beyond our concern. But figure <a href="ch-eigen.html#fig:FTA">12.4</a> gives a rough idea why the theorem works. Suppose we restrict the domain of a polynomial <span class="math inline">\(f(z)\)</span> to a circle <span class="math inline">\(|z|\leq r\)</span> where any <span class="math inline">\(z\in\mathbb{C}\)</span> is a complex number. Then by enlarging the radius <span class="math inline">\(r\)</span> of the circle, the image (cross-hatching area) <span class="math inline">\(\{f(z): \, |z|\leq r \}\)</span> will sooner or later cover the zero origin, namely <span class="math inline">\(f(z)=0\)</span> has a solution <span class="math inline">\(z^*\)</span> where <span class="math inline">\(|z^*|\leq r\)</span>. This may not be the case if we restrict the domain to a real line of the complex plane.</p>
<p>By the theorem, we can deduce that the complex number system is in the sense of a complete system such that we can solve any polynomial equation in it. The highest degree of a polynomial gives you the highest possible number of distinct complex roots for the polynomial.</p>
<p>Due to the importance of complex number system, hereby we review some basic concepts, and provide some interpretations about the <em>complex number system</em> <span class="math display">\[\mathbb{C}=\{a+\mbox{i}b:\, a,b\in\mathbb{R}\}.\]</span>
For a complex number <span class="math inline">\(z\in\mathbb{C}\)</span>, <span class="math inline">\(z=a+\mbox{i}b\)</span> is the standard form. The <em>real part</em> of <span class="math inline">\(z\)</span> is written as <span class="math inline">\(\mbox{Re}(z)=a\)</span>, and the <em>imaginary part</em> is written as <span class="math inline">\(\mbox{Im}(z)=b\)</span>. The imaginary number <span class="math inline">\(\mbox{i}\)</span> satisfies the algebraic identity <span class="math inline">\(\mbox{i}^{2}=-1\)</span>. The <em>complex conjugate</em> of <span class="math inline">\(z\)</span> is defined as <span class="math inline">\(\bar{z}=a-\mbox{i}b\)</span>. Two complex numbers are equal precisely when they have the same <strong>real part</strong> and the same <strong>imaginary part</strong>.</p>
<p>One can also consider that the complex numbers as <a href="sub-set-theory.html#sub:order">ordered pairs</a> of <a href="sub-continuity.html#sub:completeness">real numbers</a> satisfying the <em>law of complex arithmetic</em>. As complex numbers behave like ordered pairs of real numbers, they can be identified with points in the plane. Real numbers go along the <span class="math inline">\(x\)</span>-axis, and imaginary numbers are on the <span class="math inline">\(y\)</span>-axis. This gives the complex plane. Arithmetic of addition and subtraction in <span class="math inline">\(\mathbb{C}\)</span> is carried out like adding and subtracting vectors in <span class="math inline">\(\mathbb{R}^{2}\)</span>. Arithmetic of multiplication in <span class="math inline">\(\mathbb{C}\)</span> and the norm, however, are different those of vectors in <span class="math inline">\(\mathbb{R}^{2}\)</span>.<label for="tufte-sn-222" class="margin-toggle sidenote-number">222</label><input type="checkbox" id="tufte-sn-222" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">222</span> The <em>law of complex arithmetic</em> includes addition <span class="math inline">\((a+\mbox{i}b)+(c+\mbox{i}d)=(a+c)+(b+d)\mbox{i}\)</span>, multiplication <span class="math inline">\((a+\mbox{i}b)(c+\mbox{i}d)=(ac-bd)+(ad+bc)\mbox{i}\)</span>, absolute value (or square root of the <a href="ch-vecMat.html#sub:vec">norm</a>) <span class="math inline">\(|a+\mbox{i}b|= \sqrt{a^{2}+b^{2}}=\sqrt{(a+\mbox{i}b)(a-\mbox{i}b)}\)</span>.</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:complexPolar"></span>
<img src="fig/Part3/complexPolar.png" alt="Rotation of complex numbers" width="100%"><!--
<p class="caption marginnote">-->Figure 12.5: Rotation of complex numbers<!--</p>-->
<!--</div>--></span>
</p>
<p>For the multiplication of complex numbers, it may be more straightforward to consider under the <a href="sub-inferknow.html#sub:histgeo">polar coordinate</a>. For a complex number <span class="math inline">\(z=a+\mbox{i}b\)</span> with <span class="math inline">\(|z|=r\)</span>, its polar form is <span class="math inline">\(z=r\mbox{e}^{\mbox{i}\theta}\)</span>
or <span class="math inline">\(z=r\cos\theta+\mbox{i}r\sin\theta\)</span>.<label for="tufte-sn-223" class="margin-toggle sidenote-number">223</label><input type="checkbox" id="tufte-sn-223" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">223</span> Euler identity showes that the exponent of an imaginary number power can be turned into the sines and cosines of trigonometry via <span class="math display">\[\mbox{e}^{\mbox{i}\pi}=-1.\]</span> The identity comes from the Euler formula <span class="math inline">\(\mbox{e}^{\mbox{i}\theta}=\cos\theta+\mbox{i}\sin\theta\)</span>. Thus, any point on a plane comes with the polar coordinate pair <span class="math inline">\((r\cos\theta,r\sin\theta)\)</span>. </span> where the real number <span class="math inline">\(r\)</span> represents the absolute value of <span class="math inline">\(z\)</span>, namely <span class="math inline">\(r=|z|=\sqrt{a^{2}+b^{2}}\)</span>, and <span class="math inline">\(\theta\)</span> represents the angle. By using the property of the exponential function, the polar form can easily tell us that <span class="math inline">\(z^{n}=r^{n}\mbox{e}^{\mbox{i}n\theta}=r^{n}(\cos n\theta+\mbox{i}\sin n\theta)\)</span>, where the absolute value <span class="math inline">\(|z^{n}|=r^{n}\)</span> and the angle is <span class="math inline">\(n\times \theta\)</span>. Also, for the complex multiplication of <span class="math inline">\(z_1=r_1\mbox{e}^{\mbox{i}\theta_1}\)</span> and <span class="math inline">\(z_2=r_2\mbox{e}^{\mbox{i}\theta_2}\)</span>, it is all about rescaling the absolute value to <span class="math inline">\(r_1\times r_2\)</span> and rotating the angle to <span class="math inline">\(\theta_1+\theta_2\)</span>.</p>
<p>The complex multiplication on a unit circle also relates to a linear transformation.
From figure <a href="ch-eigen.html#fig:complexPolar">12.5</a>, we can see that a rotation of the plane on <span class="math inline">\(\mathbb{R}^{2}\)</span> around the origin through angle <span class="math inline">\(\theta\)</span> is a linear transformation that sends the basis vectors <span class="math inline">\([1,0]\)</span> and <span class="math inline">\([0,1]\)</span> to <span class="math inline">\([\cos\theta,\sin\theta]\)</span> and <span class="math inline">\([-\sin\theta,\cos\theta]\)</span>, respectively. Let’s denote this transofrmation <span class="math inline">\(\mathbf{T}_{\theta}\)</span> by <span class="math display">\[\mathbf{T}_{\theta}\left(x\left[\begin{array}{c}
1\\
0
\end{array}\right]+y\left[\begin{array}{c}
0\\
1
\end{array}\right]\right)=\left[\begin{array}{c}
x\cos\theta-y\sin\theta\\
x\sin\theta+y\cos\theta
\end{array}\right].\]</span>
In fact, this linear transformation can be represented by the matrix <span class="math display">\[\mathbf{T}_{\theta}=\left[\begin{array}{cc}
\cos\theta &amp; -\sin\theta\\
\sin\theta &amp; \cos\theta
\end{array}\right].\]</span>
We call <span class="math inline">\(\mathbf{T}_{\theta}\)</span> the <em>rotation matrix</em> with the angle <span class="math inline">\(\theta\)</span>.
Notice that for an arbitrary point <span class="math inline">\((x,y)\)</span> on the complex plane, the rotation of <span class="math inline">\(\theta\)</span> angle simply says <span class="math display">\[(\cos\theta+\mbox{i}\sin\theta)(x+\mbox{i}y)=x\cos\theta-y\sin\theta+\mbox{i}(x\sin\theta+y\cos\theta).\]</span>
That is, each rotation <span class="math inline">\(\mathbf{T}_{\theta}\)</span> in <span class="math inline">\(\mathbb{R}^{2}\)</span> can represent the complex number <span class="math inline">\(\cos\theta+\mbox{i}\sin\theta\)</span>.</p>
<p>By decomposing the <strong>rotation matrix</strong> <span class="math display">\[\mathbf{T}_{\theta}=\cos\theta\underset{\mathbf{B}_{1}}{\underbrace{\left[\begin{array}{cc}
1 &amp; 0\\
0 &amp; 1
\end{array}\right]}}+\sin\theta\underset{\mathbf{B}_{2}}{\underbrace{\left[\begin{array}{cc}
0 &amp; -1\\
1 &amp; 0
\end{array}\right]}},\]</span>
we represent the rotation by the matrices <span class="math inline">\(\mathbf{B}_1\)</span> and <span class="math inline">\(\mathbf{B}_2\)</span>. One can verify that these two matrices behave exactly the same as the numbers <span class="math inline">\(1\)</span> (<span class="math inline">\(\mathbf{B}_1=\mathbf{I}\)</span>) and <span class="math inline">\(\mbox{i}\)</span> (<span class="math inline">\(\mathbf{B}_2 \mathbf{B}_2= - \mathbf{I}\)</span>). In fact, any <span class="math display">\[\left[\begin{array}{cc}
a &amp; -b\\
b &amp; a
\end{array}\right]=a\left[\begin{array}{cc}
1 &amp; 0\\
0 &amp; 1
\end{array}\right]+b\left[\begin{array}{cc}
0 &amp; -1\\
1 &amp; 0
\end{array}\right],\; a,b\in\mathbb{R}\]</span>
behaves exactly the same as the complex number <span class="math inline">\(a+\mbox{i}b\)</span> under addition and multiplication. In other words, all complex numbers can be represented by these <span class="math inline">\(2\times2\)</span> real matrices <span class="math inline">\(a\mathbf{B}_{1}+b\mathbf{B}_{2}\)</span>.</p>
<p>The <strong>determinant</strong> of real matrices <span class="math inline">\(a\mathbf{B}_{1}+b\mathbf{B}_{2}\)</span> corresponds to the squared <a href="ch-vecMat.html#sub:vec">norm</a> (squared absolute value) <span class="math display">\[\mbox{det}(a\mathbf{B}_{1}+b\mathbf{B}_{2})=|a+\mbox{i}b|^{2}=a^{2}+b^{2}.\]</span> Thus, the multiplicative property of <span class="math inline">\(|z_{1}z_{2}|=|z_{1}||z_{2}|\)</span> for <span class="math inline">\(z_{1},z_{2}\in\mathbb{C}\)</span> can be deduced by the multiplicative property of <strong>determinants</strong> (fact 6).<label for="tufte-sn-224" class="margin-toggle sidenote-number">224</label><input type="checkbox" id="tufte-sn-224" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">224</span> The inverse <span class="math inline">\(z^{-1}=(a-\mbox{i}b)/(a^{2}+b^{2})\)</span>
corresponds to the inverse matrix <span class="math display">\[\left[\begin{array}{cc}
a &amp; -b\\
b &amp; a
\end{array}\right]^{-1}=\frac{1}{a^{2}+b^{2}}\left[\begin{array}{cc}
a &amp; b\\
-b &amp; a
\end{array}\right].\]</span></span></p>
<p>As we have seen the <strong>eigenvector and eigenvalues</strong> can present a linear transformation as a scalar multiplication, we may not be surprise by the fact that the polar form <span class="math inline">\(\cos\theta+\mbox{i}\sin\theta\)</span> is an eigenvalue of the rotation matrix <span class="math inline">\(\mathbf{T}_{\theta}\)</span>. The determinant of <span class="math inline">\(\mathbf{T}_{\theta}\)</span> is
<span class="math display">\[
\begin{align*}
\mbox{det}\left(\mathbf{T}_{\theta}-\lambda\mathbf{I}\right)    &amp;=\mbox{det}\left[\begin{array}{cc}
\cos\theta-\lambda &amp; -\sin\theta\\
\sin\theta &amp; \cos\theta-\lambda
\end{array}\right] \\
=(\cos\theta-\lambda)^{2}+(\sin\theta)^{2} &amp; =\lambda^{2}-2\lambda\cos\theta+1.
\end{align*}
\]</span>
By using the discriminant, we have <span class="math inline">\(cos\theta+\mbox{i}\sin\theta\)</span> as one of the conjugate solutions.<label for="tufte-sn-225" class="margin-toggle sidenote-number">225</label><input type="checkbox" id="tufte-sn-225" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">225</span> 
<span class="math display">\[
\begin{align*}
\lambda &amp;=\cos\theta\pm\sqrt{(\cos\theta)^{2}-1}\\
&amp;=\cos\theta\pm\sqrt{-(\sin\theta)^{2}}\\
&amp;=cos\theta\pm\mbox{i}\sin\theta.
\end{align*}
\]</span></span></p>
<div class="solution">
<p class="solution-begin">
Eigenvectors of the rotation <span id="sol-start-98" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-98', 'sol-start-98')"></span>
</p>
<div id="sol-body-98" class="solution-body" style="display: none;">
<p>Two <strong>eigenvalues</strong> are <span class="math inline">\(\lambda_{1}=\cos\theta+\mbox{i}\sin\theta\)</span> and <span class="math inline">\(\lambda_{2}=\cos\theta-\mbox{i}\sin\theta\)</span>.
Their corresponding eigenvectors are <span class="math inline">\([1,-\mbox{i}]^{\top}\)</span> and <span class="math inline">\([1,\mbox{i}]^{\top}\)</span>. To see this result, notice that the eigenvalue and eigenvector <span class="math inline">\(\mathbf{T}_{\theta}\mathbf{v}_{1}=\lambda_{1}\mathbf{v}_{1}\)</span>, namely <span class="math display">\[\left[\begin{array}{cc}
\cos\theta &amp; -\sin\theta\\
\sin\theta &amp; \cos\theta
\end{array}\right]\left[\begin{array}{c}
v_{1}\\
v_{2}
\end{array}\right]=(\cos\theta+\mbox{i}\sin\theta)\left[\begin{array}{c}
v_{1}\\
v_{2}
\end{array}\right]\]</span>
which is equivalent to the following system of equations:
<span class="math display">\[
\begin{align*}
v_{1}\cos\theta-v_{2}\sin\theta &amp;=v_{1}\cos\theta+\mbox{i}(v_{2}\sin\theta),\\
v_{1}\sin\theta+v_{2}\cos\theta &amp;=v_{1}\cos\theta+\mbox{i}(v_{2}\sin\theta).
\end{align*} 
\]</span>
Two sides of equations are complex numbers. They are equal only if they have the same <strong>real parts</strong> and the same <strong>imaginary parts</strong>. It is easy to see that <span class="math inline">\(v_{2}=-\mbox{i}v_{1}\)</span> gives the (infinite) solutions of this equation. We can simply set <span class="math inline">\(v_{1}=1\)</span> so that <span class="math inline">\(v_{2}=-\mbox{i}\)</span>.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
</div>
<div id="sub:diag" class="section level2">
<h2>
<span class="header-section-number">12.2</span> Diagonalization in Dynamical Systems</h2>
<p>A dynamical system of which the solution vector <span class="math inline">\(\mathbf{x}(t)\)</span>
(continuous time) or <span class="math inline">\(\mathbf{x}_{t}\)</span> (discrete time) is changing with the time <span class="math inline">\(t\)</span> is not easy to be solved by the <a href="ch-MatComp.html#sub:GElimination">Gaussian elimination</a>. In this type of problems, almost all vectors change “directions” over time, meanwhile the changes couple the vectors to evoke further variations. This is not a pleasant news to any attempt of exploring the unchanged property, say a dynamical law, in pratice. To a proper observer, the law should neither change with the <strong>coordinate system</strong> of the measurements nor evolve with any emerging tendency of the coupling system. Thus, a natural attempt of alleviating the difficulty is to model the dynamical law by a linear transformation<label for="tufte-sn-226" class="margin-toggle sidenote-number">226</label><input type="checkbox" id="tufte-sn-226" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">226</span> For non-linear events, one can linearize them locally. See Ch[?].</span>, and then to represent the linear transformation by the <a href="ch-eigen.html#sub:det">invariants</a>. <a href="ch-eigen.html#sub:det">Eigen values-vectors</a> can - as long as the <a href="ch-eigen.html#sub:det">eigen values-vectors</a> are <strong>diagonalizable</strong> - provide us a perspective under which the vectors evolves seperately and the evolutions are along with their eigen-directions.</p>
<p>Let’s recall some properties of a <a href="l#sub:matInv">diagonal matrix</a>. Assume that <span class="math inline">\(\mathbf{A}\)</span> is a diagonal matrix such that <span class="math inline">\(\mathbf{A}=[a_{ij}]_{n\times n}=\mbox{diag}\{a_{11},\dots,a_{nn}\}\)</span>. It is easy to determine the output <span class="math inline">\(\mathbf{b}\)</span> of the transformation <span class="math inline">\(\mathbf{A}\mathbf{x}\)</span> because the transformation <span class="math inline">\(\mathbf{A}\)</span> acts like <span class="math inline">\(n\)</span> scalar multiplications by each <span class="math inline">\(a_{ii}\)</span> along the direction of the <span class="math inline">\(i\)</span>-th entry of <span class="math inline">\(\mathbf{x}\)</span>. Normally, <span class="math inline">\(\mathbf{A}\)</span> is not a diagonal matrix, so each transformed entry (output) <span class="math inline">\(b_{i}\)</span> depends on some <a href="ch-vecMat.html#sub:vec">linear combination</a> of the other entries. However, by using <a href="ch-eigen.html#sub:det">eigenvector and eigenvalue</a>, we can find out under some eigenvector <span class="math inline">\(\mathbf{x}\)</span>, the transformation <span class="math inline">\(\mathbf{A}\mathbf{x}=\lambda\mathbf{x}\)</span> act exactly as a scalar multiplication. (See figure <a href="ch-eigen.html#fig:diagMat">12.6</a>.)</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:diagMat"></span>
<img src="fig/Part3/diagMat.png" alt="Eigenvalue of a diagonal matrix" width="100%"><!--
<p class="caption marginnote">-->Figure 12.6: Eigenvalue of a diagonal matrix<!--</p>-->
<!--</div>--></span>
</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:eigenval"></span>
<img src="fig/Part3/eigenval.gif" alt="Eigenvalues of non-diagonal matrices" width="100%"><!--
<p class="caption marginnote">-->Figure 12.7: Eigenvalues of non-diagonal matrices<!--</p>-->
<!--</div>--></span>
</p>
<p>Suppose that we find <span class="math inline">\(n\)</span> distinct <a href="ch-eigen.html#sub:det">eigenvectors</a> <span class="math inline">\(\{\mathbf{v}_{i}\}_{i=1,\dots,n}\)</span> with <span class="math inline">\(n\)</span> distinct real-valued eigenvalues, then the transformation <span class="math inline">\(\mathbf{A}\)</span> is decomposable into <span class="math inline">\(n\)</span> one-dimensional transformations acting along the <span class="math inline">\(\mathbf{v}_{1}, \dots, \mathbf{v}_{n}\)</span> axes: <span class="math display">\[\mathbf{A}\mathbf{v}_{1}=\lambda_{1}\mathbf{v}_{1},\;\dots,\;\mathbf{A}\mathbf{v}_{n}=\lambda_{n}\mathbf{v}_{n}.\]</span>
This result implies that given an indecomposable non-diagonal matrix <span class="math inline">\(\mathbf{A}\)</span>, it is possible to find new axes, along which the matrix behaves as diagonalizable one. The new axes, namely the <a href="ch-eigen.html#sub:det">eigenvectors</a> <span class="math inline">\(\{\mathbf{v}_{1},\dots,\mathbf{v}_{n}\}\)</span> constructs a new coordinate system.<label for="tufte-sn-227" class="margin-toggle sidenote-number">227</label><input type="checkbox" id="tufte-sn-227" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">227</span> Formally speaking, <span class="math inline">\(\mathbf{v}_{1}, \dots, \mathbf{v}_{n}\)</span> will construct a <a href="ch-MatComp.html#sub:vecSpaces">basis</a> in a finite dimensional vector space <span class="math inline">\(\mathcal{V}\)</span>. Any point in <span class="math inline">\(\mathcal{V}\)</span> can be uniquely represented by a <a href="ch-vecMat.html#sub:vec">linear combination</a> of <span class="math inline">\(\mathbf{v}_{1}, \dots, \mathbf{v}_{n}\)</span>. The coefficients of the linear combination are referred to as <em>coordinates</em>. And the basis is said to construct a <em>coordinate system</em>.</span> The transformation <span class="math inline">\(\mathbf{A}\)</span> can be fully interpreted under the new <strong>coordinate system</strong>.</p>
<p>The <em>standard coordinate system</em> is constructed by the <a href="ch-vecMat.html#sub:vec">standard unit vectors</a>, i.e. <span class="math inline">\(\{\mathbf{e}_{1},\mathbf{e}_{2}\}\)</span> in <span class="math inline">\(\mathbb{R}^{2}\)</span>, where <span class="math inline">\(\mathbf{e}_{1}=[1,0]^{\top}\)</span> and <span class="math inline">\(\mathbf{e}_{2}=[0,1]^{\top}\)</span>. In the <strong>standard coordinate system</strong>, to get to any point of <span class="math inline">\(\mathbb{R}^{2}\)</span> in this, one can choose to go a certain distance along <span class="math inline">\(\mathbf{e}_{1}\)</span> (horizontally) and then a certain distance along <span class="math inline">\(\mathbf{e}_{2}\)</span> (vertically). Under the coordinate system of the <a href="ch-vecMat.html#sub:vec">eigenvectors</a> <span class="math inline">\(\{\mathbf{v}_{1},\mathbf{v}_{2}\}\)</span>, one can get to any point in <span class="math inline">\(\mathbb{R}^{2}\)</span> by moving along the <span class="math inline">\(\mathbf{v}_{1}\)</span> and <span class="math inline">\(\mathbf{v}_{1}\)</span>-direction.</p>
<p>Consider the following example of the matrix <span class="math inline">\(\mathbf{A}\)</span>: <span class="math display">\[\mathbf{A}=\left[\begin{array}{cc}
1 &amp; 2\\
4 &amp; 3
\end{array}\right].\]</span>
The eigenvalues of this matrix is given by
<span class="math display">\[
\begin{align*}
\mbox{det}(\lambda\mathbf{I}-\mathbf{A})=&amp;\mbox{det}\left[\begin{array}{cc}
\lambda-1 &amp; -2\\
-4 &amp; \lambda-3
\end{array}\right]\\=(\lambda-1)(\lambda-3)-8=&amp;\lambda^{2}-4\lambda-5=(\lambda-5)(\lambda+1)
\end{align*}
\]</span>
with the solutions <span class="math inline">\(\lambda_{1}=5\)</span> and <span class="math inline">\(\lambda_{2}=-1\)</span>. Then one can solve the two fixed point problems <span class="math inline">\((\mathbf{A}/5)\mathbf{v}_{1}=\mathbf{v}_{1}\)</span> and <span class="math inline">\((\mathbf{A}/-1)\mathbf{v}_{2}=\mathbf{v}_{2}\)</span> to find out the corresponding eigenvectors <span class="math inline">\(\mathbf{v}_{1}=[1,2]^{\top}\)</span> and <span class="math inline">\(\mathbf{v}_{2}=[1,-1]^{\top}\)</span>.<label for="tufte-sn-228" class="margin-toggle sidenote-number">228</label><input type="checkbox" id="tufte-sn-228" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">228</span>  One can check that
<span class="math display">\[
\begin{align*}
\left[\begin{array}{cc}
1 &amp; 2\\
4 &amp; 3
\end{array}\right]\left[\begin{array}{c}
1\\
2
\end{array}\right]&amp;=5\left[\begin{array}{c}
1\\
2
\end{array}\right],\\ \;\left[\begin{array}{cc}
1 &amp; 2\\
4 &amp; 3
\end{array}\right]\left[\begin{array}{c}
1\\
-1
\end{array}\right]&amp;=-1\left[\begin{array}{c}
1\\
-1
\end{array}\right].
\end{align*}
\]</span></span>
The eigenvectors give a new coordinate system <span class="math inline">\(\{\mathbf{v}_{1},\mathbf{v}_{2}\}\)</span> where we can navigate any point in <span class="math inline">\(\mathbb{R}^{2}\)</span>. For example, the point <span class="math inline">\(\mathbf{x}=[3,0]^{\top}\)</span> under the <span class="math inline">\(\{\mathbf{v}_{1},\mathbf{v}_{2}\}\)</span> is moved by one unit <span class="math inline">\(\mathbf{v}_{1}\)</span>-direction and two units <span class="math inline">\(\mathbf{v}_{2}\)</span>-direction from the origin, namely <span class="math display">\[1\times\left[\begin{array}{c}
1\\
2
\end{array}\right]+2\times\left[\begin{array}{c}
1\\
-1
\end{array}\right]=\left[\begin{array}{c}
3\\
0
\end{array}\right].\]</span>
If the point is at <span class="math inline">\(\mathbf{A}\mathbf{x}=[3,12]^{\top}\)</span>, under the <span class="math inline">\(\{\mathbf{v}_{1},\mathbf{v}_{2}\}\)</span>-coordinate it can be represented by is <span class="math inline">\([\mathbf{v}_{1},\mathbf{v}_{2}]\cdot[5,-2]^{\top}=5\mathbf{v}_{1}+2\mathbf{v}_{2}\)</span>
or written as the new coordinates <span class="math inline">\([5,-2]_{\{\mathbf{v}_{1},\mathbf{v}_{2}\}}\)</span>, namely, five units <span class="math inline">\(\mathbf{v}_{1}\)</span> and negative two units <span class="math inline">\(\mathbf{v}_{2}\)</span>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:coordinate"></span>
<img src="fig/Part3/coordinate.png" alt="New coordinates" width="100%"><!--
<p class="caption marginnote">-->Figure 12.8: New coordinates<!--</p>-->
<!--</div>--></span>
</p>
<p>When the eigenvalues are complex numbers, the diagonalization still works, however it becomes difficult to represent in a 2D graph, as the transformation relating to the imaginary numbers is about the rotation, which cannot be decomposed in 1D.<label for="tufte-sn-229" class="margin-toggle sidenote-number">229</label><input type="checkbox" id="tufte-sn-229" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">229</span> As shown in section <a href="ch-eigen.html#sub:det">12.1</a>, the rotation transformation has complex eigenvalues. Two real eigenvalues mean that the function could be decomposed into two 1D expansions or contractions, or into one saddle point (one expands and one contracts), but two complex eigenvalues make the transformation in a 4D space (two real and two imaginary axes). See figure <a href="ch-eigen.html#fig:eigenDy3">12.12</a> </span> Because the complex eigenvalues of a transformation imply the existence of the rotation in this transformation, and because the eigenvectors are unchanged with the direction of the transformation, we can deduce that the associated eigenvectors will also need to be complex. That is to say, when a transformation involves rotations, then its eigenvalues and eigenvectors ought to be complex. For the <span class="math inline">\(2\times2\)</span> matrix, <strong>diagonalization</strong> implies either two distinct real eigenvalues or a pair of complex conjugate eigenvalues. We can generalize the previous arguments to any square matrix.</p>
<p>If a matrix <span class="math inline">\(\mathbf{A}\)</span> satisfied <span class="math inline">\(\mathbf{T}^{-1}\mathbf{A}\mathbf{T}=\mathbf{B}\)</span> or <span class="math inline">\(\mathbf{A}=\mathbf{T}\mathbf{B}\mathbf{T}^{-1}\)</span>, then <span class="math inline">\(\mathbf{A}\)</span> is said to be <em>similar</em> to matrix <span class="math inline">\(\mathbf{B}\)</span>. A matrix is <em>diagonalizable</em> if it is <strong>similar</strong> to a diagonal matrix.</p>
<ul>
<li>
<strong>Diagonalization</strong> and <strong>similarity</strong> : For an <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span>, if <span class="math inline">\(\mathbf{A}\)</span> has <span class="math inline">\(n\)</span>-distinct eigenvectors such that <span class="math inline">\(\mathbf{A}\mathbf{v}_{1}=\lambda_{1}\mathbf{v}_{1}, \dots, \mathbf{A}\mathbf{v}_{n}=\lambda_{n}\mathbf{v}_{n}\)</span>, and <span class="math inline">\(\lambda_i \neq \lambda_j\)</span> for any <span class="math inline">\(1\leq i,j\leq n\)</span>, then <span class="math inline">\(\mathbf{v}_{1},\dots,\dots\mathbf{v}_{n}\)</span> are <a href="ch-MatComp.html#sub:vecSpaces">linearly independent</a> and <span class="math inline">\(\mathbf{V}\)</span> is <a href="ch-MatComp.html#sub:matInv">invertible</a> where <span class="math inline">\(\mathbf{V}=[\mathbf{v}_{1},\dots,\dots\mathbf{v}_{n}]\)</span>. Let <span class="math inline">\(\Lambda=\mbox{diag}\{\lambda_{1},\dots,\lambda_{n}\}\)</span>,
the relation <span class="math inline">\(\mathbf{A}\mathbf{V}=\mathbf{V}\Lambda\)</span> of <a href="ch-eigen.html#sub:det">eigenvectors and eigenvalues</a> implies <span class="math display">\[ \mathbf{V}^{-1}\mathbf{A}\mathbf{V}=\Lambda,\]</span> which says that <span class="math inline">\(\mathbf{A}\)</span> is <strong>similar</strong> to the diagonal matrix <span class="math inline">\(\Lambda\)</span>, namely <span class="math inline">\(\mathbf{A}\)</span> is <strong>diagonalizable</strong>.</li>
</ul>
<p>Any two similar matrices have the same <a href="ch-eigen.html#sub:det">eigenvalues</a> and the same <a href="ch-eigen.html#sub:det">characteristic polynomial</a>.<label for="tufte-sn-230" class="margin-toggle sidenote-number">230</label><input type="checkbox" id="tufte-sn-230" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">230</span> In chapter <a href="sub-set-theory.html#sub:crypto">2.2</a>, the cryptosystem is based on some kind of conjugate operation (to construct two similar system). The idea of this operation exactly follows the fact that encrypted system and the decrypted system share the same eigenvalues (the crypto information). </span> Thus, if <span class="math inline">\(\mathbf{A}\)</span> is <strong>diagonalizable</strong>, and <span class="math inline">\(\mathbf{A}\)</span> is <strong>similar</strong> to <span class="math inline">\(\mathbf{B}\)</span>, one can infer that <span class="math inline">\(\mathbf{B}\)</span> is also <strong>diagonalizable</strong>.</p>
<div class="solution">
<p class="solution-begin">
Proof <span id="sol-start-99" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-99', 'sol-start-99')"></span>
</p>
<div id="sol-body-99" class="solution-body" style="display: none;">
<p>First, we prove that for <span class="math inline">\(n\)</span>-distinct eigenvectors, the columns of <span class="math inline">\(\mathbf{V}\)</span> are <a href="#sub:vecSpace">linearly independent</a> (a <a href="#sub:vecSpace">basis</a>).</p>
<p>Suppose the columns of <span class="math inline">\(\mathbf{V}\)</span> are linearly dependent. Then there must exist a <a href="ch-vecMat.html#sub:vec">linear combination</a> <span class="math display">\[c_{1}\mathbf{v}_{1}+c_{2}\mathbf{v}_{2}+\cdots+c_{k}\mathbf{v}_{k}=0\]</span>
with each <span class="math inline">\(c_{i}\neq0\)</span>. Mutiplying this linear combination by <span class="math inline">\(\lambda_{1}\)</span>, we have
<span class="math display">\[c_{1}\lambda_{1}\mathbf{v}_{1}+c_{2}\lambda_{1}\mathbf{v}_{2}+\cdots+c_{k}\lambda_{1}\mathbf{v}_{k}=0.\]</span>
Similarly, mutiplying the linear combination by <span class="math inline">\(\mathbf{A}\)</span>, we have
<span class="math display">\[c_{1}\mathbf{A}\mathbf{v}_{1}+c_{2}\mathbf{A}\mathbf{v}_{2}+\cdots+c_{k}\mathbf{A}\mathbf{v}_{k}=0\]</span>
or say <span class="math display">\[c_{1}\lambda_{1}\mathbf{v}_{1}+c_{2}\lambda_{2}\mathbf{v}_{2}+\cdots+c_{k}\lambda_{k}\mathbf{v}_{k}=0.\]</span>
Subtract these two equations, we have<span class="math display">\[0\mathbf{v}_{1}+c_{2}(\lambda_{1}-\lambda_{2})\mathbf{v}_{2}+\cdots+c_{k}(\lambda_{1}-\lambda_{k})\mathbf{v}_{k}=0\]</span> which is another linear combination but with fewer terms. This contradicts with the first linear combination.
Thus if there are <span class="math inline">\(n\)</span>-distinct eigenvectors, these eigenvectors must be <a href="#sub:vecSpace">linearly independent</a>.</p>
<p>Secondly, the columns of <span class="math inline">\(\mathbf{V}\)</span> are linearly independent, <span class="math inline">\(\mathbf{V}\)</span> is a full rank matrix and so it is invertible. We can have <span class="math inline">\(\mathbf{V}^{-1}\mathbf{A}\mathbf{V}=\Lambda\)</span>.</p>
<p>Finally, if <span class="math inline">\(\mathbf{A}\)</span> is <strong>similar</strong> to <span class="math inline">\(\mathbf{B}\)</span>, say <span class="math inline">\(\mathbf{T}^{-1}\mathbf{A}\mathbf{T}=\mathbf{B}\)</span>, by definition and the trick <span class="math inline">\(\mathbf{T}^{-1}\mathbf{I}\mathbf{T}=\mathbf{I}\)</span>,
we have
<span class="math display">\[
\begin{align*}
\mbox{det}(\lambda\mathbf{I}-\mathbf{B})=&amp;\mbox{det}(\lambda\mathbf{T}^{-1}\mathbf{I}\mathbf{T}-\mathbf{T}^{-1}\mathbf{A}\mathbf{T})\\
=&amp;\mbox{det}(\mathbf{T}^{-1}(\lambda\mathbf{I}-\mathbf{A})\mathbf{T})\\
=&amp;\mbox{det}(\mathbf{T}^{-1})\mbox{det}(\lambda\mathbf{I}-\mathbf{A})\mbox{det}(\mathbf{T})\\
=&amp;\mbox{det}(\lambda\mathbf{I}-\mathbf{A}).
\end{align*}
\]</span>
The result follows.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>To see the use of <strong>diagonalization</strong> in dynamical systems, let’s recall that a matrix-vector multiplication can represent a dynamics of multiple inputs (see chapter <a href="ch-vecMat.html#sub:linearSys">10.4</a>). In the <a href="ch-vecMat.html#sub:linearSys">discrete linear dynamical system</a>, with an initial vector <span class="math inline">\(\mathbf{x}_{0}\)</span>, the linear transformation <span class="math inline">\(\mathbf{A}\mathbf{x}_{1}=\mathbf{x}_{0}\)</span> evokes the system. The iteration <span class="math inline">\(\mathbf{A}\mathbf{x}_{t}=\mathbf{x}_{t-1}\)</span> can simulate the full set of dynamics of this system.</p>
<p>The computation of <span class="math inline">\(\mathbf{x}_{t}\)</span> based on the initial vector <span class="math inline">\(\mathbf{x}_{0}\)</span> requires <span class="math inline">\(t\)</span> times iterations, namely <span class="math display">\[\mathbf{x}_{t}=\mathbf{A}\mathbf{x}_{t-1}=\mathbf{A}(\mathbf{A}\mathbf{x}_{t-2})=\cdots=\mathbf{A}^{t}\mathbf{x}_{0}.\]</span>
To compute the power of matrices is not as simple as to compute the power of scalars.<label for="tufte-sn-231" class="margin-toggle sidenote-number">231</label><input type="checkbox" id="tufte-sn-231" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">231</span>  The issue is more serious if the power of matrices is used for linearization where a small perturbation may propogate unexpected chaotic consequences which makes the whole computational scheme fail. We will come to the topic of chaos in CH[?].</span>. However, to compute the power of diagonal matrices is simple. For example:
<span class="math display">\[
\begin{align*}
\Lambda^{k}&amp;=   \left[\begin{array}{ccc}
\lambda_{1} &amp; 0 &amp; 0\\
 &amp; \ddots &amp; 0\\
0 &amp; 0 &amp; \lambda_{n}
\end{array}\right]\times\left[\begin{array}{ccc}
\lambda_{1} &amp; 0 &amp; 0\\
 &amp; \ddots &amp; 0\\
0 &amp; 0 &amp; \lambda_{n}
\end{array}\right]\cdots\times\left[\begin{array}{ccc}
\lambda_{1} &amp; 0 &amp; 0\\
 &amp; \ddots &amp; 0\\
0 &amp; 0 &amp; \lambda_{n}
\end{array}\right]\\
&amp;=  \left[\begin{array}{ccc}
\lambda_{1}^{k} &amp; 0 &amp; 0\\
 &amp; \ddots &amp; 0\\
0 &amp; 0 &amp; \lambda_{n}^{k}
\end{array}\right]=\mbox{diag}\{\lambda_{1}^{k},\dots,\lambda_{n}^{k}\}.
\end{align*}
\]</span>
Thus, if the matrix is <strong>diagonalizable</strong> with the <a href="ch-eigen.html#sub:det">eigenvector matrix</a> <span class="math inline">\(\mathbf{V}\)</span> and the <a href="ch-eigen.html#sub:det">eigenvalue matrix</a> <span class="math inline">\(\Lambda\)</span>, we may expect to have <span class="math display">\[\mathbf{A}^n = (\mathbf{V}^{-1}\Lambda\mathbf{V})^n=(\mathbf{V}^{-1}\Lambda\mathbf{V}\mathbf{V}^{-1}\Lambda\mathbf{V}\cdots\mathbf{V}^{-1}\Lambda\mathbf{V})=\mathbf{V}^{-1}\Lambda^n\mathbf{V}.\]</span> In this case, the advantage is clear: we need to calculate and apply the eigenvector matrix <span class="math inline">\(\mathbf{V}\)</span> and <span class="math inline">\(\mathbf{V}^{-1}\)</span> only once, and the rest of the iteration process is simply applying the <span class="math inline">\(n\)</span>-th power of the diagonal matrix <span class="math inline">\(\Lambda\)</span>.</p>
<p>Apart from the computational advantage, the <strong>diagonalization</strong> also assists us to disentangle some complex dynamical patterns of the <strong>coupled systems</strong>. In chapter <a href="sub-inferknow.html#sub:dyn">4.3</a> and <a href="ch-DE.html#sub:EulerScheme">8.2</a>, we have seen what a univariate (1D) time series is. The four possible dynmaical patterns of <span class="math inline">\(x_t = \lambda x_{t-1}\)</span> are summarized in figure <a href="ch-eigen.html#fig:ts">12.9</a>.<label for="tufte-sn-232" class="margin-toggle sidenote-number">232</label><input type="checkbox" id="tufte-sn-232" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">232</span> The figure is designed for <span class="math inline">\(\mathbf{x}_{1,t}=0.8\mathbf{x}_{1,t-1}\)</span>, <span class="math inline">\(\mathbf{x}_{2,t}=1.02\mathbf{x}_{2,t-1}\)</span>, <span class="math inline">\(\mathbf{x}_{3,t}=-0.8\mathbf{x}_{3,t-1}\)</span> and <span class="math inline">\(\mathbf{x}_{4,t}=-1.02\mathbf{x}_{4,t-1}\)</span>.</span> We can see that the series converges if <span class="math inline">\(|\lambda|&lt;1\)</span> and diverges if <span class="math inline">\(|\lambda|&gt;1\)</span>. For negative <span class="math inline">\(\lambda\)</span>, the trends oscillate; for positive <span class="math inline">\(\lambda\)</span>, the trends are monotonic.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:ts"></span>
<img src="fig/Part3/ts.png" alt="Dynamical patterns" width="100%"><!--
<p class="caption marginnote">-->Figure 12.9: Dynamical patterns<!--</p>-->
<!--</div>--></span>
</p>
<div class="solution">
<p class="solution-begin">
Code <span id="sol-start-100" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-100', 'sol-start-100')"></span>
</p>
<div id="sol-body-100" class="solution-body" style="display: none;">
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb29-1" data-line-number="1">time =<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">20</span></a>
<a class="sourceLine" id="cb29-2" data-line-number="2"><span class="co"># Case 1: 0&lt;lambda&lt;1</span></a>
<a class="sourceLine" id="cb29-3" data-line-number="3">x1 =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">20</span>); x1[<span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span> <span class="co"># At t=0, give x_t an initial value 1 </span></a>
<a class="sourceLine" id="cb29-4" data-line-number="4"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">19</span>){ x1[t<span class="op">+</span><span class="dv">1</span>] =<span class="st"> </span><span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span>x1[t]}</a>
<a class="sourceLine" id="cb29-5" data-line-number="5"></a>
<a class="sourceLine" id="cb29-6" data-line-number="6"><span class="co"># Case 2: lambda&gt;1</span></a>
<a class="sourceLine" id="cb29-7" data-line-number="7">x2 =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">20</span>); x2[<span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span> </a>
<a class="sourceLine" id="cb29-8" data-line-number="8"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">19</span>){ x2[t<span class="op">+</span><span class="dv">1</span>] =<span class="st"> </span><span class="fl">1.02</span> <span class="op">*</span><span class="st"> </span>x2[t]}</a>
<a class="sourceLine" id="cb29-9" data-line-number="9"></a>
<a class="sourceLine" id="cb29-10" data-line-number="10"><span class="co"># Case 3: -1&lt;lambda&lt;0</span></a>
<a class="sourceLine" id="cb29-11" data-line-number="11">x3 =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">20</span>); x3[<span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span> </a>
<a class="sourceLine" id="cb29-12" data-line-number="12"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">19</span>){ x3[t<span class="op">+</span><span class="dv">1</span>] =<span class="st"> </span><span class="fl">-0.8</span> <span class="op">*</span><span class="st"> </span>x3[t]}</a>
<a class="sourceLine" id="cb29-13" data-line-number="13"></a>
<a class="sourceLine" id="cb29-14" data-line-number="14"><span class="co"># Case 4: lambda&lt;-1</span></a>
<a class="sourceLine" id="cb29-15" data-line-number="15">x4 =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">20</span>); x4[<span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span> </a>
<a class="sourceLine" id="cb29-16" data-line-number="16"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">19</span>){ x4[t<span class="op">+</span><span class="dv">1</span>] =<span class="st"> </span><span class="fl">-1.02</span> <span class="op">*</span><span class="st"> </span>x4[t]}</a>
<a class="sourceLine" id="cb29-17" data-line-number="17"></a>
<a class="sourceLine" id="cb29-18" data-line-number="18"></a>
<a class="sourceLine" id="cb29-19" data-line-number="19"><span class="co"># Plot the equation</span></a>
<a class="sourceLine" id="cb29-20" data-line-number="20">d =<span class="st"> </span><span class="kw">data.frame</span>(time,x1,x2,x3,x4)</a>
<a class="sourceLine" id="cb29-21" data-line-number="21"><span class="kw">library</span>(ggplot2); <span class="kw">library</span>(reshape2)</a>
<a class="sourceLine" id="cb29-22" data-line-number="22"><span class="kw">ggplot</span>( <span class="kw">melt</span>(d, <span class="dt">id.vars=</span><span class="st">"time"</span>), <span class="kw">aes</span>(time, value, <span class="dt">colour=</span>variable, <span class="dt">group=</span>variable)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()   <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</a></code></pre></div>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>For a <span class="math inline">\(2\times2\)</span> diagonal matrix <span class="math inline">\(\mathbf{A}\)</span>, its linear dynamical system <span class="math inline">\(\mathbf{x}_{t}=\mathbf{A}\mathbf{x}_{t-1}\)</span> only consists of two seperated univariate time series. This kind of system is called the <em>uncoupled system</em> where the growth of one series only depends on its own historical states. On the other hand, if the off-diagonal entries of <span class="math inline">\(\mathbf{A}\)</span> are non-zero, then the dynamical system is a <em>coupled system</em>. The <strong>diagonalization</strong> allows us to analyze a <strong>coupled system</strong> in its <strong>uncoupling form</strong>.</p>
<p>Consider the following system
<span class="math display">\[
\left[\begin{array}{c}
x_{1,t}\\
x_{2,t}
\end{array}\right]=\mathbf{A}\mathbf{x}_{t-1}=\left[\begin{array}{cc}
0.65 &amp; 0.5\\
0.25 &amp; 0.9
\end{array}\right]\left[\begin{array}{c}
x_{1,t-1}\\
x_{2,t-1}
\end{array}\right]
\]</span>
The eigenvalues of this system are <span class="math inline">\(\lambda_{1}=1.15\)</span>, and <span class="math inline">\(\lambda_{2}=0.4\)</span>. The eigenvectors can be chosen to be <span class="math inline">\(\mathbf{v}_{1}=[1,1]^{\top}\)</span> and <span class="math inline">\(\mathbf{v}_{2}=[-2,1]^{\top}\)</span>. Notice that the eigenvalues are real numbers and <span class="math inline">\(|\lambda_{1}|&gt;1\)</span> and <span class="math inline">\(\lambda_{2}|&lt;1\)</span>. So for the <strong>uncoupled system</strong> <span class="math inline">\((\mathbf{V}\mathbf{x}_{t})=\Lambda(\mathbf{V}\mathbf{x}_{t-1})\)</span>, we expect the behavior of the system must expand along the <span class="math inline">\(\mathbf{v}_{1}\)</span>-coordinate and contract along the <span class="math inline">\(\mathbf{v}_{2}\)</span>-coordinate. Figure <a href="ch-eigen.html#fig:eigenDy1">12.10</a> shows that the initial vector <span class="math inline">\(\mathbf{x}_{0}= [10,50]^{\top}\)</span> grows along the <span class="math inline">\(\mathbf{v}_{1}\)</span>-coordinate in five periods.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:eigenDy1"></span>
<img src="fig/Part3/eigenDy1.png" alt="Linear dynamical system 1" width="100%"><!--
<p class="caption marginnote">-->Figure 12.10: Linear dynamical system 1<!--</p>-->
<!--</div>--></span>
</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:eigenDy2"></span>
<img src="fig/Part3/eigenDy2.gif" alt="Linear dynamical system 2" width="100%"><!--
<p class="caption marginnote">-->Figure 12.11: Linear dynamical system 2<!--</p>-->
<!--</div>--></span>
</p>
<p>Consider another system
<span class="math display">\[
\left[\begin{array}{c}
x_{1,t}\\
x_{2,t}
\end{array}\right]=\mathbf{A}\mathbf{x}_{t-1}=\left[\begin{array}{cc}
0.1 &amp; 1.4\\
0.4 &amp; 0.2
\end{array}\right]\left[\begin{array}{c}
x_{1,t-1}\\
x_{2,t-1}
\end{array}\right]
\]</span></p>
<p>The eigenvalues of this system are <span class="math inline">\(\lambda_{1}=0.9\)</span>, and <span class="math inline">\(\lambda_{2}=-0.6\)</span>. The eigenvectors can be chosen to be <span class="math inline">\(\mathbf{v}_{1}=[1,1]^{\top}\)</span> and <span class="math inline">\(\mathbf{v}_{2}=[-2,1]^{\top}\)</span>. Figure <a href="ch-eigen.html#fig:eigenDy2">12.11</a> shows that the trend shrinks along <span class="math inline">\(\mathbf{v}_{1}\)</span> but collapse in an oscillating manner along the <span class="math inline">\(\mathbf{v}_{2}\)</span>-axis. The presence of oscillation is due to the negative eigenvalue which means that the matrix will flip the state point back and forth on either side of the <span class="math inline">\(\mathbf{v}_{1}\)</span>-axis. The oscillation is diminishing because the largest absolute eigenvalue is less than one (otherwise, the oscillation will departure).</p>
<div class="solution">
<p class="solution-begin">
Derivations <span id="sol-start-101" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-101', 'sol-start-101')"></span>
</p>
<div id="sol-body-101" class="solution-body" style="display: none;">
<p>System 1:</p>
<p>The characteristic equation is
<span class="math display">\[
\begin{align*}
\mbox{det}\left[\begin{array}{cc}
0.65-\lambda &amp; 0.5\\
0.25 &amp; 0.9-\lambda
\end{array}\right]=(0.65-\lambda)(0.9-\lambda)-(0.5)(0.25)\\
=\lambda^{2}-(0.65+0.9)\lambda-(0.65\times0.9-0.25\times0.5).
\end{align*}
\]</span>
By using the discriminant of this equation, we have <span class="math inline">\(\lambda=\frac{(0.65+0.9)\pm\sqrt{(0.65+0.9)^{2}-4(0.65\times0.9-0.25\times0.5)}}{2}=\frac{1.55\pm0.75}{2},\)</span>
namely <span class="math inline">\(\lambda=[1.15,\,0.4]^{\top}\)</span>. For <span class="math inline">\(\lambda_{1}=1.15\)</span>,
<span class="math display">\[\mathbf{A}\mathbf{v}_{1}=\left[\begin{array}{cc}
0.65 &amp; 0.5\\
0.25 &amp; 0.9
\end{array}\right]\left[\begin{array}{c}
v_{1}\\
v_{2}
\end{array}\right]=1.15\left[\begin{array}{c}
v_{1}\\
v_{2}
\end{array}\right].\]</span>
The simplified version is
<span class="math display">\[
\begin{align*}
 0.65v_{1}+0.5v_{2} &amp;=1.15v_{1}\\
0.25v_{1}+0.9v_{2}  &amp;=1.15v_{2}
\end{align*}
\]</span>
Gaussian elimination gives the answer <span class="math inline">\(v_{1}=v_{2}\)</span>. We can choose any vector on the line <span class="math inline">\(v_{1}=v_{2}\)</span> to represent this eigenvector, for example <span class="math inline">\([1,1]^{\top}\)</span>.</p>
<p>For <span class="math inline">\(\lambda_{2}=0.4\)</span>, the system of equations becomes</p>
<p><span class="math display">\[
\begin{align*}
0.65v_{1}+0.5v_{2}  &amp;=0.4v_{1}\\
0.25v_{1}+0.9v_{2}  &amp;=0.4v_{2}
\end{align*}
\]</span>
which gives the answer <span class="math inline">\(v_{2}=-0.5v_{1}\)</span>. So we can choose the vector <span class="math inline">\([-2,1]\)</span> to represent the line
<span class="math inline">\(v_{1}+2v_{2}=0\)</span>.</p>
<p>System 2:</p>
<p>The characteristic equation gives <span class="math display">\[\lambda=\frac{(0.1+0.2)\pm\sqrt{(0.1+0.2)^{2}-4(0.1\times0.2-0.4\times1.4)}}{2}=\frac{0.3\pm1.5}{2},\]</span>
namely <span class="math inline">\(\lambda=[0.9,\,-0.6]^{\top}\)</span>. For <span class="math inline">\(\lambda_{1}=0.9\)</span>, the system of equations of eigenvectors is
<span class="math display">\[
\begin{align*}
0.1v_{1}+1.4v_{2}   &amp;=0.9v_{1}\\
0.4v_{1}+0.2v_{2}   &amp;=0.9v_{2}
\end{align*}
\]</span>
Gaussian elimination gives the answer <span class="math inline">\(1.75v_{1}=v_{2}\)</span>. We can choose any vector on the line <span class="math inline">\(1.75v_{1}-v_{2}=0\)</span> to represent this eigenvector, for example <span class="math inline">\([4,7]^{\top}\)</span>.
For <span class="math inline">\(\lambda_{2}=-0.6\)</span>, the system of equations becomes
<span class="math display">\[
\begin{align*}
0.1v_{1}+1.4v_{2}   &amp;=-0.6v_{1}\\
0.4v_{1}+0.2v_{2}   &amp;=-0.6v_{2}
\end{align*}
\]</span>
which gives the answer <span class="math inline">\(v_{2}=-0.5v_{1}\)</span>. So we can choose the vector <span class="math inline">\([2,-1]\)</span> to represent the line <span class="math inline">\(v_{1}+2v_{2}=0\)</span>.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:eigenDy3"></span>
<img src="fig/Part3/eigenDy3.gif" alt="Linear dynamical system 3" width="100%"><!--
<p class="caption marginnote">-->Figure 12.12: Linear dynamical system 3<!--</p>-->
<!--</div>--></span>
</p>
<p>The final case is that <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span> are a pair of <a href="ch-eigen.html#sub:det">complex conjugate</a> eigenvalues, and then the action of the matrix was a rotation. Recall the <a href="ch-eigen.html#sub:det">rotation matrix</a> in section <a href="ch-eigen.html#sub:det">12.1</a>. If we set the angle of the rotation to <span class="math inline">\(120^{\circ}\)</span>, the matrix gives the following system:
<span class="math display">\[
\left[\begin{array}{c}
x_{1,t}\\
x_{2,t}
\end{array}\right]=\mathbf{A}\mathbf{x}_{t-1}=\left[\begin{array}{cc}
0 &amp; 1\\
-1 &amp; -1
\end{array}\right]\left[\begin{array}{c}
x_{1,t-1}\\
x_{2,t-1}
\end{array}\right]
\]</span>
with the eigenvalues <span class="math inline">\(\lambda = \cos120^{\circ} \pm \mbox{i}\sin120^{\circ}= -\frac{1}{2} \pm \mbox{i}\frac{\sqrt{3}}{2}\)</span>, and the corresponding eigenvectors <span class="math inline">\(\mathbf{v}=[1,\pm\mbox{i}]^\top\)</span>. In figure <a href="ch-eigen.html#fig:eigenDy3">12.12</a> we apply this transformation with an initial vector <span class="math inline">\([5,0]^{\top}\)</span>, the complex eigenvalues induces rotations to the system. Because the absolute value of <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(\sqrt{\sin^2120^{\circ}+\cos^{2}120^{\circ}}=1\)</span>, these rotations neither expand nor shrink the sizes of the system.
In other words, <span class="math inline">\(\mbox{e}^{\mbox{i}120^{\circ}} \mathbf{v}\)</span> is to rotate <span class="math inline">\(120^{\circ}\)</span> degrees along the “imaginary” coordinates <span class="math inline">\(\mathbf{v}=[1,\pm\mbox{i}]^\top\)</span> that cannot be visualized in a 2D graph.</p>
<p>With above three examples, we can almost infer the following properties about the linear dynamical systems <span class="math inline">\(\mathbf{x}_t= \mathbf{A} \mathbf{x}_{t-1}\)</span>. Given the eigenvalue-eigenvector decomposition <span class="math inline">\(\mathbf{A}\mathbf{v}=\lambda\mathbf{v}\)</span> If an eigenvalue of <span class="math inline">\(\mathbf{A}\)</span> has absolute value less than <span class="math inline">\(1\)</span>, the matrix shrinks vectors lying along that eigenvector <span class="math inline">\(\mathbf{v}\)</span>. If the eigenvalue has absolute value greater than <span class="math inline">\(1\)</span>, the matrix expands vectors lying along that eigenvector. If the eigenvalue is negative, the action of the matrix is to flip back and forth between negative and positive values along that eigenvector. If the eigenvalue is a complex numbers <span class="math inline">\(z\)</span>, the <a href="ch-eigen.html#sub:det">polar form</a> <span class="math inline">\(z=r\mbox{e}^{\mbox{i}\theta}\)</span> seperates the rotation <span class="math inline">\(\mbox{e}^{\mbox{i}\theta}\)</span> from the absolute size <span class="math inline">\(r\)</span>. In this case, the absolute value of <span class="math inline">\(r\)</span> will indicate whether the matrix will shrink or will expand vectors, and <span class="math inline">\(\mbox{e}^{\mbox{i}\theta}\)</span> will rotate vectors around the axis of the complex-valued eigenvector.</p>
<p>Let’s define the <em>principal eigenvector</em> as the eigenvector whose eigenvalue has the largest absolute value. In the linear dynamical system, the state will evolve until its motion lies along the <strong>principal eigenvector</strong>. The largest eigenvalue, as the <em>principal eigenvalue</em> will determine the long-term behavior of the iteration, i.e. expansion, contraction, rotation, etc.</p>
<p>Recall that for a single <a href="#">differential equation</a> <span class="math inline">\(\mbox{d}x(t)/\mbox{d}t=\lambda x(t)\)</span> with an initial value <span class="math inline">\(x(0)\)</span>, the solution is <span class="math inline">\(x(t)=x(0)\mbox{e}^{\lambda t}\)</span>. Similarly, for a linear system of <a href="ch-DE.html#sub:ode">differential equations</a>, the diagonal matrix uncouples the system. Let <span class="math display">\[\left[\begin{array}{c}
\mbox{d}v_{1}(t)/\mbox{d}t\\
\mbox{d}v_{2}(t)/\mbox{d}t
\end{array}\right]=\Lambda\mathbf{v}(t)=\left[\begin{array}{cc}
\lambda_{1} &amp; 0\\
0 &amp; \lambda_{2}
\end{array}\right]\left[\begin{array}{c}
v_{1}(t)\\
v_{2}(t)
\end{array}\right].\]</span>
Suppose the initial vector is <span class="math inline">\([v_{1}(0),v_{2}(0)]^{\top}\)</span>, the above system can be treated as two seperated differential equations whose solution vector is <span class="math display">\[\left[\begin{array}{c}
v_{1}(t)\\
v_{2}(t)
\end{array}\right]=\left[\begin{array}{c}
v_{1}(0)\mbox{e}^{\lambda_{1}t}\\
v_{2}(0)\mbox{e}^{\lambda_{2}t}
\end{array}\right],\]</span> where <span class="math inline">\(\lambda_{1}\)</span> and <span class="math inline">\(\lambda_{2}\)</span> play the roles of <a href="ch-DE.html#sub:ode">growth rates</a> for these differential equations.</p>
<p>Now we consider the third example of the linear dynamical system but under the continuous time setting. Let the matrix <span class="math inline">\(\mathbf{A}\)</span> be the <span class="math inline">\(120^{\circ}\)</span>-rotation matrix so that the system becomes:
<span class="math display">\[
\left[\begin{array}{c}
\mbox{d}x_{1}(t)/\mbox{d}t\\
\mbox{d}x_{2}(t)/\mbox{d}t
\end{array}\right]=\left[\begin{array}{c}
x_{2}(t)\\
-x_{1}(t)-x_{2}(t)
\end{array}\right]=\left[\begin{array}{cc}
0 &amp; 1\\
-1 &amp; -1
\end{array}\right]\left[\begin{array}{c}
x_{1}(t)\\
x_{2}(t)
\end{array}\right].
\]</span>
As we already knew the eigenvalues of this matrix are <span class="math inline">\(\lambda = -\frac{1}{2} \pm \mbox{i}\frac{\sqrt{3}}{2}\)</span>, we can infer the trends of this dynamical system. Notice that unlike in the discrete time setting, the solution vector contains <span class="math inline">\(\mbox{e}^{\lambda t}\)</span>, the exponential function of complex eigenvalues <span class="math inline">\(\lambda\)</span>.<label for="tufte-sn-233" class="margin-toggle sidenote-number">233</label><input type="checkbox" id="tufte-sn-233" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">233</span> This difference is due to the differential operator. In fact, we can extend the idea to general linear mappings or operators. Take the differential operator <span class="math inline">\(\frac{\mbox{d}}{\mbox{d}x}\)</span> (the operation of taking the derivative of a function) as an example. As the operation is linear, one may also expect there exist some “eigenvector” (or formally <em>eigenfunction</em>) of the operator. Let <span class="math inline">\(v(x)=\mbox{e}^{\lambda x}\)</span>. We know that <span class="math display">\[\frac{\mbox{d}}{\mbox{d}x}v(x)=\lambda v(x).\]</span> In other words, the exponential function <span class="math inline">\(\mbox{e}^{\lambda x}\)</span> is an eigenvector to the derivative-operator with eigenvalue <span class="math inline">\(\lambda\)</span>. We will come back to this point in Ch[?].
.</span> The exponential function can be decomposed into two parts <span class="math display">\[\mbox{e}^{\lambda t}=\mbox{e}^{-\frac{t}{2}} \mbox{e}^{\pm \mbox{i}\frac{\sqrt{3}}{2} t},\]</span>
a real exponential function and a pair of exponential functions with conjugate imaginary numbers. According to the previous discussion on convergences of differential equations in chapter <a href="ch-DE.html#sub:ode">8.1</a>, the real negative (positive) growth rate indicates the convergent (divergent) trend. Because the real parts of the eigenvalues are less than zero, we expect the system to converge. Because of the imaginary term <span class="math inline">\(\mbox{e}^{\pm \mbox{i}\frac{\sqrt{3}}{2} t}\)</span>, we also expect some rotations of <span class="math inline">\(x_1(t)\)</span> and <span class="math inline">\(x_2(t)\)</span> occuring in the process of convergence. In figure <a href="ch-eigen.html#fig:odeEuler">12.13</a>, we can find that both solutions converge to zero. The convergence accompanies with some oscillationary movements across time <span class="math inline">\(t\)</span>. The oscillation is caused by the rotation. Because the rotation (of a 2D circle) means the loss or the gain in one direction is partially compensated by the gain or the loss from the other. This rotation together with the convergent treand appears as a stable spiral in figure <a href="ch-eigen.html#fig:odeSys">12.14</a>. Figure <a href="ch-eigen.html#fig:odeSys">12.14</a> provides a coordinate plane with axes being the values of two state variables <span class="math inline">\(x_1(t)\)</span> and <span class="math inline">\(x_2(t)\)</span>. This kind of plots is called the <em>phase plot</em>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:odeEuler"></span>
<img src="fig/Part3/ode.png" alt="Solutions" width="100%"><!--
<p class="caption marginnote">-->Figure 12.13: Solutions<!--</p>-->
<!--</div>--></span>
</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:odeSys"></span>
<img src="fig/Part3/odeSys.gif" alt="Phase plot of the solutions" width="100%"><!--
<p class="caption marginnote">-->Figure 12.14: Phase plot of the solutions<!--</p>-->
<!--</div>--></span>
</p>
<div class="solution">
<p class="solution-begin">
Code of solving the simple ODE system <span id="sol-start-102" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-102', 'sol-start-102')"></span>
</p>
<div class="solution-body" id="sol-body-102" style="display: none;">




<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb30-1" data-line-number="1">times =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dt">by=</span><span class="fl">0.01</span>) <span class="co"># Time interval </span></a>
<a class="sourceLine" id="cb30-2" data-line-number="2"></a>
<a class="sourceLine" id="cb30-3" data-line-number="3"><span class="co"># ODE system</span></a>
<a class="sourceLine" id="cb30-4" data-line-number="4">odeSys =<span class="st"> </span><span class="cf">function</span>(t,init, parms){</a>
<a class="sourceLine" id="cb30-5" data-line-number="5">  x1 =<span class="st"> </span>init[<span class="dv">1</span>]; </a>
<a class="sourceLine" id="cb30-6" data-line-number="6">  x2 =<span class="st"> </span>init[<span class="dv">2</span>];</a>
<a class="sourceLine" id="cb30-7" data-line-number="7">  dx1 =<span class="st">  </span>x2; </a>
<a class="sourceLine" id="cb30-8" data-line-number="8">  dx2 =<span class="st"> </span><span class="op">-</span>x1<span class="op">-</span>x2;</a>
<a class="sourceLine" id="cb30-9" data-line-number="9">  <span class="kw">list</span>(<span class="kw">c</span>(dx1,dx2))</a>
<a class="sourceLine" id="cb30-10" data-line-number="10">  }</a>
<a class="sourceLine" id="cb30-11" data-line-number="11"></a>
<a class="sourceLine" id="cb30-12" data-line-number="12"><span class="co"># Solve System of ODE by Euler's method</span></a>
<a class="sourceLine" id="cb30-13" data-line-number="13"><span class="kw">library</span>(deSolve)</a>
<a class="sourceLine" id="cb30-14" data-line-number="14">cinit =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>); <span class="co"># initial vector </span></a>
<a class="sourceLine" id="cb30-15" data-line-number="15">parms =<span class="st"> </span><span class="dv">0</span>; <span class="co"># no parameter uses in this example</span></a>
<a class="sourceLine" id="cb30-16" data-line-number="16">xsolution =<span class="st"> </span><span class="kw">ode</span>(cinit, times, odeSys, parms, <span class="dt">method=</span><span class="st">"euler"</span>) <span class="co"># ODE solver</span></a>
<a class="sourceLine" id="cb30-17" data-line-number="17"></a>
<a class="sourceLine" id="cb30-18" data-line-number="18"></a>
<a class="sourceLine" id="cb30-19" data-line-number="19"></a>
<a class="sourceLine" id="cb30-20" data-line-number="20"><span class="co"># Plot the solution</span></a>
<a class="sourceLine" id="cb30-21" data-line-number="21">d =<span class="st"> </span><span class="kw">data.frame</span>(xsolution)</a>
<a class="sourceLine" id="cb30-22" data-line-number="22"><span class="kw">library</span>(ggplot2); <span class="kw">library</span>(reshape2)</a>
<a class="sourceLine" id="cb30-23" data-line-number="23"><span class="kw">ggplot</span>( <span class="kw">melt</span>(d, <span class="dt">id.vars=</span><span class="st">"time"</span>), <span class="kw">aes</span>(time, value, <span class="dt">colour=</span>variable, <span class="dt">group=</span>variable)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()   <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</a>
<a class="sourceLine" id="cb30-24" data-line-number="24"></a>
<a class="sourceLine" id="cb30-25" data-line-number="25"><span class="kw">library</span>(gganimate)</a>
<a class="sourceLine" id="cb30-26" data-line-number="26">fig =<span class="st"> </span><span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">x =</span> X1, <span class="dt">y =</span> X2)) </a>
<a class="sourceLine" id="cb30-27" data-line-number="27">fig <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="dt">color=</span><span class="st">"blue"</span>) <span class="op">+</span><span class="st">  </span><span class="kw">transition_time</span>(<span class="dt">time=</span>time) <span class="op">+</span><span class="st"> </span><span class="kw">shadow_mark</span>() <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</a></code></pre></div>
</div>
<div id="sub:alchemy" class="section level2">
<h2>
<span class="header-section-number">12.3</span> * Miscellaneous: Alchemy and Values</h2>
<p>Alchemy is about an idea of a common metallic matter subject to interchangeable sets of qualities. The common metallic matter in the west was called Philosophers’ stone, and in the east was called Golden elixir. They both reveal an ideal capability of transforming substances. While Philosophers’ stone is supposed to transform the substances into better ones, the Golden exlixir is supposed to transform the substances into immortal ones. In history, legions of alchemists sought for these stones to transform coloring metals, stones, and cloth into precious (or apparently precious) objects.</p>
<p>Transformation, as an alchemical theme here, obeys a cosmic view of the world that all substances are composed of the same fundamental one. If a single material that serves as the fundamental for all substances, one may expect to turn one thing into another because at the deepest level they are really the same thing. In Greek philosophy, this idea is the <em>ladder of nature</em>: everything in existence, from inanimate matter to the transcendent one, is linked in a hierarchical chain. In Chinese philosophy, this idea is called the <em>correlative cosmology</em> (天人合一): all entities and phenomena ultimately originate from the Dao (道) in a hierarchical manner.<label for="tufte-sn-234" class="margin-toggle sidenote-number">234</label><input type="checkbox" id="tufte-sn-234" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">234</span> This idea later was developped into a doctrine called <em>interactions between heaven and mankind</em> (天人感应) by the confucian politician Dong Zhongshu. The doctrine approached within the ace of the Christian faith where the omniscience of God implied that a uniform, harmonious, interfunctioning world, namely a highly ordered and self-consistent whole. It not only served as a basis for deciding the legitimacy of Chinese monarch but also provided governing guidances on a reigning monarch.</span> These principles undergirded the idea of alchemical transmutation, and encourged alchmists to explore the theoretical possibility of transforming unessential substances into more valuable ones.</p>
<p>From my current point of view, the literature about alchemy consists of a forbidding tangle of intentional secrecy, bizarre ideas, and strange images plunging the readers into a maze of conflicting claims and contradictory assertions. Thus, my motivation is not to provide any serious knowledge about the secrets, the processes or the privileges of alchemy, as I think the comprehensive root of the subject has been merged into modern chemistry. Instead, I will treat the alchemy as a kind of exploration of the nature of matter under the themes of purification and improvement. And by giving a brief retrospective description of this exploration, I will provide a coarse outline of defining the value (of transformations) from an alchemical perspective.</p>
<p>Alchemy, like other sceintific pursuits, has a skeleton of theory that provides intellectual fundamentals, that instructs the practical works, and illuminates the perspicuous pathways of future developments. Ancient Greek philosophers (Empedocles and later Aristotle) attributed the origin of natural substances and their transformations to four “roots” of things which are fire, air, earth, and water. Similarly, ancient Chinese philosophers attributed the essence, namely the set of properties constructing substances, to five elements which are earth, metal, water, wood, and fire. Both schools of thoughts endeavored to explain matter’s hidden nature and to account for its unending transformations into new forms. Most of the following ideas point to the belief that some sort of an invariant hidden beneath the constantly changing appearances of things. The invariant of Thales was water, of Aristotle was the first matter, of Empedocles were four elements, of Daoists and Confucianists were five elements.</p>
<p>Just like any current empirical research activities, alchemists relied on those theoretical principles to guide the practical work. The practice of transformative process could involve special psychic states and incorporeal agents. The history of alchemy were simultaneously chemical, spiritual, and psychological, but that the main purpose of those pratical works looked like to unite the several constituents of consciousness and to develop a trascending state of mankind. At this point, we can see a clear bifurcation of two schools.</p>
<ul>
<li><p>Judaism-Christian alchemy: The Philosophers’ stone as a perfect metal represents the state of quintessence for transforming corrodible base metals into incorruptible gold.<label for="tufte-sn-235" class="margin-toggle sidenote-number">235</label><input type="checkbox" id="tufte-sn-235" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">235</span> The metals were divided into different categories. By the standards of the intrinsic beauty and the ability to resist corrosion, noble metals (such as gold and silver) are different from the base metals (such as copper, iron, tin, lead, and mercury).</span></p></li>
<li><p>Chinese alchemy: The Golden elixir as a immortal pellet represents the state of constancy and immutability beyond the change and transiency.<label for="tufte-sn-236" class="margin-toggle sidenote-number">236</label><input type="checkbox" id="tufte-sn-236" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">236</span> Sulphur and nitre were both used for medicinal purposes to extend the life of a Chinese, and for military purposes to extend the life of Chinese culture.</span></p></li>
</ul>
<p>Even though the ultimal goals looked different, if we interpret the chemical processes declared by the alchemists as metaphorical or analogical manifestations of the need for enhancing knowledge to achieve salvation, of the need for distilling human’s inner being to archieve divinity, we may consider what all these alchemists sought were the same, a purely self-transformative, meditative, psychic process.<label for="tufte-sn-237" class="margin-toggle sidenote-number">237</label><input type="checkbox" id="tufte-sn-237" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">237</span> <span class="citation">Jung (<a href="bibliography.html#ref-Jung2008">2008</a>)</span> asserted that the real object of alchemy was the transformation of the psyche, and considered Alchemy’s “true root” was to be found not so much in philosophical ideas and outlooks but rather in “experiences of projection of the individual researcher.” <span class="citation">Jung (<a href="bibliography.html#ref-Jung2008">2008</a>)</span> say,
“a peculiar attitude of the mind—the concentration of attention on a single thing. The result of this state of concentration is that the mind is absorbed to the exclusion of other things, and to such a degree insensible that the way is opened for auto- matic actions; and these actions, becoming more complicated, as in the preceding case, may assume a psychic character and constitute intelligences of a parasitic kind, existing side by side with the normal personality, which is not aware of them.” Since the psyche could project its contents onto any sort of matter, the actual substances employed by the alchemist were not crucial. Jung believe that Alchemy’s allegorical language were expressions of a collective unconscious.</span></p>
<p>Transformation, in a alchemical metaphor, could be about the changes of the non-volatile body and the volatile spirit. The body seems to be the same for all substances; so the identity is dependent on its spirit.</p>
<p>The Judaism-Christian alchemy focused on a transformation from a state of nature to a state of grace by releasing the imprisoned material body. Themes such as purification and distillation readily lend themselves as moral or spiritual symbols. Knowledge, in this case, was necessary to overcome one’s ignorance, to liberate oneself from the material world and the evil forces.</p>
<p>The Chinese alchemy focused on a transformation from a fleeting state to an eternal state by collecting the seperated spirits. Metaphorical sublimation and volatilization seperated the spirits from the various bodies. Joining the separated spirits formed a unifed consciousness, and established the desired collective knowledge.</p>
</div>
</div>
</div>
</div></body></html>

<p style="text-align: center;">
<a href="ch-MatComp.html"><button class="btn btn-default">Previous</button></a>
<a href="bibliography.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-06-20
</p>
</div>
</div>



</body>
</html>
