<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="16 Randomization | Project XXII" />
<meta property="og:type" content="book" />





<meta name="author" content="Zhengyuan Gao" />

<meta name="date" content="2021-03-10" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="16 Randomization | Project XXII">

<title>16 Randomization | Project XXII</title>

<link href="libs/tufte-css/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding/datatables.js"></script>
<link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>



</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Project XXII<p><p class="author">Zhengyuan Gao</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BA; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Preface</a>
<a href="part-i-inference-based-upon-logic-and-logical-computation.html">PART I: Inference Based upon Logic and Logical Computation</a>
<a href="sub-logic.html"><span class="toc-section-number">1</span> Logic</a>
<a href="sub-set-theory.html"><span class="toc-section-number">2</span> Set Theory</a>
<a href="sub-axioms.html"><span class="toc-section-number">3</span> Axioms</a>
<a href="sub-inferknow.html"><span class="toc-section-number">4</span> Inference and Knowledge</a>
<a href="sub-incomplete.html"><span class="toc-section-number">5</span> Incompleteness</a>
<a href="part-ii-infinitesimal-changes-and-their-consequences.html">PART II: Infinitesimal Changes and their Consequences</a>
<a href="sub-continuity.html"><span class="toc-section-number">6</span> Continuity</a>
<a href="sub-calculus.html"><span class="toc-section-number">7</span> Calculus</a>
<a href="ch-DE.html"><span class="toc-section-number">8</span> Differential Equations</a>
<a href="ch-CalUn.html"><span class="toc-section-number">9</span> Calculus under Uncertainty</a>
<a href="part-iii-emergence-of-abstract-interactions.html">PART III: Emergence of Abstract Interactions</a>
<a href="ch-vecMat.html"><span class="toc-section-number">10</span> Vector and Matrix</a>
<a href="ch-MatComp.html"><span class="toc-section-number">11</span> Matrix Computation</a>
<a href="ch-eigen.html"><span class="toc-section-number">12</span> Eigenvalues and Eigenvectors</a>
<a href="ch-UnMulti.html"><span class="toc-section-number">13</span> Uncertainty in Multiple Dimensions</a>
<a href="part-iv-three-masons-to-illuminate-the-dual-world.html">PART IV: Three Masons to Illuminate the Dual World</a>
<a href="ch-representation.html"><span class="toc-section-number">14</span> Representation</a>
<a href="ch-optApp.html"><span class="toc-section-number">15</span> Optimum Approximation</a>
<a id="active-page" href="ch-randomization.html"><span class="toc-section-number">16</span> Randomization</a><ul class="toc-sections">
<li class="toc"><a href="#sub:RHilbert"> Randomized Hilbert Space</a></li>
<li class="toc"><a href="#sub:SumDecomp"> Direct Sums and Decompositions</a></li>
</ul>
<a href="bibliography.html">Bibliography</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="ch:randomization" class="section level1">
<h1>
<span class="header-section-number">16</span> Randomization</h1>
<p>Plato, in his work <em><em>the Republic</em></em>, describes a cave in which the residents are chained to a wall so they can’t see the real world; the best they can perceive are shadows reflected on the wall of the cave by some light outside. The residents have to make up their own interpretations of the outside world according to these shadows. All residents lack the omniscient knowledge of the real world, so they perceive the world differently based upon what is going on inside their minds and their “mental” projections of the shadows. That’s to say, facing the same situation, different people may consciously react differently; the different actions consecutively create different life paths. An observer from outside may find the evolutions of these paths somehow <a href="ch-CalUn.html#sub:rv">random</a>.</p>
<p>The <a href="ch-CalUn.html#sub:rv">randomness</a> emerges when the residents construct various elaborate ideas towards what reality is. The variety of these elaborate “mental” projections breaks down the unitarity and generates random outcomes. Such a construction is called <em>randomization</em>, a process of endowing unpredictable patterns to the events.<label for="tufte-sn-421" class="margin-toggle sidenote-number">421</label><input type="checkbox" id="tufte-sn-421" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">421</span> <strong>Randomization</strong> is particularly important in designing a game. Nobody wants to play a deterministic scissors-paper-stone game. This game is interesting because of the random feature - no one knows the opponent’s action. Similarly, suppose that many residents are characters immersing inside some kind of giant, massively “game platform” that is so well rendered that none of them can detect the artificiality; the way of keeping the game continue, I guess, is to create enough <a href="ch-CalUn.html#sub:rv">randomness</a> to prevent awakening the players. (Although attributing few random events a “non-random” feature to some players would make the game more “addictive,” this trick is non-applicable to all the events and all the players.)</span></p>
<div id="sub:RHilbert" class="section level2">
<h2>
<span class="header-section-number">16.1</span> Randomized Hilbert Space</h2>
<p>One way to think about <a href="ch-randomization.html#ch:randomization">randomization</a> is to consider the physical process of heating the crystals. The cooling crystals of a solid locate statically on some perfect lattice. During the heating process, the free energy accumulates so that the solid begins to melt, and the previous static crystals start to move randomly.<label for="tufte-sn-422" class="margin-toggle sidenote-number">422</label><input type="checkbox" id="tufte-sn-422" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">422</span> The inverse of this physical process, namely cooling down a melting solid, is called annealing.</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:annealing"></span>
<img src="fig/Part4/annealing.gif" alt="Heating the particles" width="100%"><!--
<p class="caption marginnote">-->Figure 16.1: Heating the particles<!--</p>-->
<!--</div>--></span>
</p>
<p>In a technical sense, if we restrict our attention to some objects in a <a href="ch-representation.html#sub:conjugacy">Hilbert space</a>, <span class="math inline">\(\mathcal{H}\)</span>, we can define the <a href="ch-randomization.html#ch:randomization">randomization</a> as mapping these objects into a <strong>Hilbert space</strong> associated with a certain <a href="sub-incomplete.html#sub:beyond2">probability space</a>.</p>
<ul>
<li>
<strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space, and mean square completeness</strong> : Let <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> be a <a href="sub-incomplete.html#sub:beyond2">probability space</a>, and let <span class="math inline">\(\mathcal{H}\)</span> be a (non-random) Hilbert space with the <a href="ch-vecMat.html#sub:vec">inner product</a> <span class="math inline">\(\left\langle \cdot,\cdot\right\rangle\)</span>. The <a href="ch-randomization.html#ch:randomization">randomization</a> of this Hilbert space on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> gives some measurable maps <span class="math inline">\(X(\omega):\Omega\rightarrow\mathcal{H}\)</span>, <span class="math inline">\(Y(\omega):\Omega\rightarrow\mathcal{H}\)</span>, and the <em><span class="math inline">\(\mathbb{P}\)</span>-inner product</em> <span class="math inline">\(\left\langle \cdot,\cdot\right\rangle _{\mathbb{P}}\)</span> such that:
<span class="math display">\[\left\langle X(\omega),Y(\omega)\right\rangle _{\mathbb{P}}=\int\left\langle X(\omega),Y(\omega)\right\rangle \mathbb{P}(\mbox{d}\omega)=\mathbb{E}\left[\left\langle X(\omega),Y(\omega)\right\rangle \right].\]</span>
In particular, any random element <span class="math inline">\(X(\omega)\)</span> in this randomized Hilbert space has the finite second moment, e.g., <span class="math display">\[\mathbb{E}\left[\left\langle X(\omega),X(\omega)\right\rangle \right]^{2}=\mathbb{E}[\|X(\omega)\|^{2}]&lt;\infty.\]</span>
These random elements are known as <em>mean-square integrable <span class="math inline">\(\mathcal{H}\)</span>-value random variables</em>. The space is denoted by <span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>.
A sequence <span class="math inline">\(\{X_{n}(\omega)\}_{n\in\mathbb{N}}\)</span> of the <em><span class="math inline">\(\mathcal{H}\)</span>-valued random variables</em> is said to converge to <span class="math inline">\(X(\omega)\)</span> in <em>mean square</em> if
<span class="math display">\[\lim_{n\rightarrow\infty}\mathbb{E}\left[\left|X_{n}(\omega)-X(\omega)\right|^{2}\right]=0\]</span> with <span class="math inline">\(\mathbb{E}[|X_{n}(\omega)|^{2}]&lt;\infty\)</span> for all <span class="math inline">\(n\)</span>.
The <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span> space</strong> is <a href="ch-representation.html#sub:conjugacy">complete</a> with respect to the <strong>mean square</strong>.<label for="tufte-sn-423" class="margin-toggle sidenote-number">423</label><input type="checkbox" id="tufte-sn-423" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">423</span> Recall that the Hilbert space is complete with respect to the <a href="ch-representation.html#sub:innerProd"><span class="math inline">\(L_{2}\)</span>-norm</a>.</span>
</li>
</ul>
<p>Here are three examples of the <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span> space</strong>.
First, suppose that the <a href="ch-representation.html#sub:conjugacy">Hilbert space</a> <span class="math inline">\(\mathcal{H}\)</span> is just a real-valued <a href="ch-MatComp.html#sub:vecSpaces">vector space</a> in the finite dimension, i.e., <span class="math inline">\(\mathcal{H}=\mathbb{R}^{n}\)</span>, the <strong><span class="math inline">\(\mathcal{H}\)</span>-valued random variables</strong> are standard random vectors, i.e., <span class="math display">\[\begin{align*}\mathbf{\mathbf{X}}(\omega)=\left[\begin{array}{c}
X_{1}(\omega)\\
\vdots\\
X_{n}(\omega)
\end{array}\right]&amp;,\: \mathbf{\mathbf{Y}}(\omega)=\left[\begin{array}{c}
Y_{1}(\omega)\\
\vdots\\
Y_{n}(\omega)
\end{array}\right],\\
\left\langle \mathbf{X}(\omega),\mathbf{Y}(\omega)\right\rangle _{\mathbb{P}}=&amp;\mathbb{E}\left[\sum_{i=1}^{n}X_i(\omega)Y_i(\omega)\right],\end{align*}\]</span>
where the <span class="math inline">\(\mathbb{P}\)</span>-inner product <span class="math inline">\(\left\langle \mathbf{X}(\omega),\mathbf{Y}(\omega)\right\rangle _{\mathbb{P}}\)</span> can also be written as
<span class="math display">\[\left\langle \mathbf{X}(\omega),\mathbf{Y}(\omega)\right\rangle _{\mathbb{P}} = \int_{\mathbb{R}^{n}}\int_{\mathbb{R}^{n}}\langle \mathbf{x},\mathbf{y}\rangle p(\mathbf{x},\mathbf{y})\mbox{d}\mathbf{x}\mbox{d}\mathbf{y}=\int\int  (\mathbf{x}^\top\mathbf{y} p(\mathbf{x},\mathbf{y}))\mbox{d}\mathbf{x}\mbox{d}\mathbf{y},\]</span>
where <span class="math inline">\(p(\cdot,\cdot)\)</span> is the <a href="ch-CalUn.html#sub:conProb">joint probability</a> density function of <span class="math inline">\(\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}\)</span>.<label for="tufte-sn-424" class="margin-toggle sidenote-number">424</label><input type="checkbox" id="tufte-sn-424" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">424</span> If we assume that the set <span class="math inline">\(\Omega\)</span> in the probability space <span class="math inline">\((\Omega, \mathcal{F}, \mathbf{P})\)</span> only contains <a href="ch-CalUn.html#sub:rv">discrete states</a>, the <span class="math inline">\(\mathbb{P}\)</span>-inner product becomes
<span class="math display">\[\begin{align*}\left\langle \mathbf{X}(\omega),\mathbf{Y}(\omega)\right\rangle _{\mathbb{P}}&amp;= \mathbb{E}[X(\omega)Y(\omega)]\\&amp;=\sum_{i=1}^{n}x_{i}y_{i}p(x_{i},y_{i}).\end{align*}\]</span></span></p>
<p>Second, suppose that the <a href="ch-representation.html#sub:conjugacy">Hilbert space</a> <span class="math inline">\(\mathcal{H}\)</span> is an infinite-dimensional space of real-valued functions on the domain <span class="math inline">\(t\in[0,T]\)</span>, i.e., <span class="math inline">\(\mathcal{H}=L_2[0,T]\)</span>, then the <strong><span class="math inline">\(\mathcal{H}\)</span>-valued random variables</strong> are <a href="ch-UnMulti.html#sub:Markov">stochastic processes</a>, i.e., <span class="math inline">\(X(t,\omega)\)</span> and <span class="math inline">\(Y(t,\omega)\)</span>. Informally, we can think to extend the previous example to the infinite dimension.<label for="tufte-sn-425" class="margin-toggle sidenote-number">425</label><input type="checkbox" id="tufte-sn-425" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">425</span> A <a href="ch-UnMulti.html#sub:Markov">stochastic processes</a> <span class="math inline">\(X(t,\omega)\)</span> can be thought of as a collection of infinite many random variables at infinite time points <span class="math inline">\(t_1, t_2,\dots\)</span>. So we have an infinite-length vector <span class="math inline">\(X_{t_1}(\omega),\dots X_{t_n}(\omega)\dots\)</span>. Also, the probability space of the <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong> is a <a href="ch-UnMulti.html#sub:Markov">filtered space</a> <span class="math inline">\((\Omega,\mathcal{F},\{\mathcal{F}_{t}\}_{t&gt;0},\mathbb{P})\)</span> where <span class="math inline">\(\mathcal{F}_1\)</span> is for <span class="math inline">\(X_{t_1}\)</span>, etc.</span> That is, given a fixed <span class="math inline">\(\omega\in\Omega\)</span> (the sample path), the <strong><span class="math inline">\(\mathcal{H}\)</span>-valued</strong> <span class="math inline">\(\{X(\cdot,\omega):t\in[0,T]\}\)</span> is a deterministic function, say <span class="math inline">\(X(\cdot,\omega):[0,t]\rightarrow L_{2}[0,T]\)</span>.
The <span class="math inline">\(\mathbb{P}\)</span>-inner product of these stochastic processes is given by
<span class="math display">\[\begin{align*}&amp; \left\langle X(t,\omega),Y(t,\omega)\right\rangle _{\mathbb{P}}=\int\int X(s,\omega)Y(s,\omega)\mbox{d}s\mathbb{P}(\mbox{d}\omega)\\&amp;=\mathbb{E}\left[\left\langle X(s,\omega),Y(s,\omega)\right\rangle \right]=\mathbb{E}\left[\int_{0}^{t}X(s,\omega)Y(s,\omega)\mbox{d}s\right].\end{align*}\]</span></p>
<p>Third, one fundamental <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> in <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong> is the <em>Gaussian Hilbert space</em>, a <a href="ch-CalUn.html#sub:divRV">complete</a> space consisting of zero-mean <a href="ch-CalUn.html#sub:divRV">Gaussian random variables</a> from the probability space <span class="math inline">\((\Omega,\mathcal{F},\mathcal{N})\)</span> where <span class="math inline">\(\mathcal{N}\)</span> stands for the <a href="ch-CalUn.html#sub:divRV">Gaussian probability law</a>. For example, for any <span class="math inline">\(n\)</span> independent identical <span class="math inline">\(\varepsilon_i \sim \mathcal{N}(0,\sigma^2)\)</span>, the <a href="ch-MatComp.html#sub:vecSpaces">span</a> of these <a href="ch-CalUn.html#sub:divRV">Gaussian random variables</a> <span class="math display">\[\mbox{span}\left\{\sum_{i=1}^{n} c_i\varepsilon_i\,:\, \sum_{i=1}^n|c_i|^2\leq \infty\right\}\]</span> is a <strong>Gaussian Hilbert space</strong>.</p>
<p>In the <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong>, we can formalize the randomness for infinite-dimensional objects.
One example is to randomize the (deterministic) <a href="sub-inferknow.html#sub:dyn">dynamics</a>. Consider the following <a href="ch-DE.html#sub:ode">differential equation</a>
<span class="math display">\[\frac{\mbox{d}x(t)}{\mbox{d}t} =f(x(t)),  \,\, \mbox{with }
x(0) =x_{0}.\]</span>
If the solution of the system exists, say <span class="math inline">\(x(t)\in\mathcal{H}\)</span>, it must be a function <span class="math inline">\(x(\cdot):[0,T]\rightarrow\mathbb{R}\)</span>.<label for="tufte-sn-426" class="margin-toggle sidenote-number">426</label><input type="checkbox" id="tufte-sn-426" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">426</span> Suppose the function <span class="math inline">\(f(\cdot)\)</span> is <a href="sub-continuity.html#sub:continuity">Lipschitz continuous</a>; the solution exists and is unique.</span> <a href="ch-randomization.html#ch:randomization">Randomization</a> is to parameterize the function <span class="math inline">\(f(\cdot)\)</span> by a random variable <span class="math inline">\(\varepsilon_{t}\)</span> at time <span class="math inline">\(t\)</span> with <span class="math inline">\(\mathbb{E}[|\varepsilon_t|^2]\leq\infty\)</span>. The system then becomes <span class="math display">\[
\frac{\mbox{d}X(t,\omega)}{\mbox{d}t} =f\left(X(t,\omega),\varepsilon_t\right),\,\, \mbox{with }
X(0,\omega) =X_{0},\]</span>
whose solution, if it exists, is a <a href="ch-UnMulti.html#sub:Markov">stochastic process</a> <span class="math display">\[X(\cdot,\cdot):[0,T]\times(\Omega,\mathcal{F},\{\mathcal{F}_{t}\}_{t\in[0,T]},\mathbb{P})\rightarrow\mathbb{R}.\]</span> Given <span class="math inline">\(\omega\)</span>, the deterministic function <span class="math inline">\(X(\cdot,\omega)\)</span> is an <span class="math inline">\(\mathcal{H}\)</span>-valued element.</p>
<p>The above differentiation notation of the stochastic process <span class="math inline">\(X(t,\omega)\)</span> may look awkward because the stochastic processes often have zigzags and because we have seen the “non-differentiability” natural of zigzags in chapter <a href="sub-calculus.html#sub:noDiff">7.2</a>. The fact is that the differentiation <span class="math inline">\(\mbox{d}X(t,\omega)/\mbox{d}t\)</span> refers to a “differentiation” in the <strong>mean square</strong> sense. For the <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space’s</strong> processes, we can introduce this new differentiation by some standard calculus conditions on the <a href="ch-CalUn.html#sub:ex">means</a> and the <a href="ch-UnMulti.html#sub:MultiVar">covariances</a>.<label for="tufte-sn-427" class="margin-toggle sidenote-number">427</label><input type="checkbox" id="tufte-sn-427" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">427</span> Notice that for a <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong> stochastic process, the first- and second-order densities (if they exist) can answer most (if not all) important probabilistic questions about the process. In other words, all the probability laws of the process are given by the first- and second-order densities. So it would be quite straightforward to consider the parameters associated with these densities, namely the <a href="ch-CalUn.html#sub:ex">means</a> and the <a href="ch-UnMulti.html#sub:MultiVar">covariance</a>.</span></p>
<p>One can define an equivalence class of stochastic processes having prescribed first and second-order <a href="ch-CalUn.html#sub:ex">moments</a>. This class is normally labeled as the <em>second-order process</em>. Let <span class="math inline">\(\{X(t,\omega)\}_{t\in\mathbb{R}^{+}}\)</span> be a stochastic process and let <span class="math inline">\(\{X_t(\omega)\}_{t\in\mathbb{N}}\)</span> be a time series.</p>
<ul>
<li>
<em>Mean value function</em> of the process or the time series:
<span class="math display">\[\mu_X(t) =\mathbb{E}[X(t,\omega)] = \mathbb{E}[X_t(\omega)].\]</span>
</li>
<li>
<em>(Auto)-covariance function</em> of the process or the time series:
<span class="math display">\[\begin{align*}\mbox{Cov}_X (t,s) &amp;=\mathbb{E}\left[(X(t,\omega)- \mu_{X}(t))(X(s,\omega)- \mu_{X}(s))\right], 
\\&amp;=\mathbb{E}[X(t,\omega)X(s,\omega)]- \mu_{X}(t)\mu_{X}(s);
\end{align*}\]</span>
</li>
</ul>
<p>Many interesting models can be characterized by the <strong>mean and covariance functions</strong>.
For example, consider the <a href="ch-eigen.html#sub:matNorms">AR(1)</a> model <span class="math inline">\(X_{t+1}=\phi X_{t}+\varepsilon_{t}\)</span> where <span class="math inline">\(\varepsilon_{t}\sim\mathcal{N}(0,\sigma_{\epsilon}^{2})\)</span> is <a href="ch-CalUn.html#sub:divRV">independent</a> of <span class="math inline">\(X_t\)</span>. The mean and covariance functions follow the dynamic law:<label for="tufte-sn-428" class="margin-toggle sidenote-number">428</label><input type="checkbox" id="tufte-sn-428" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">428</span> The law comes from
<span class="math display">\[\begin{align*}
\mu_{X}(t+1)&amp;=\mathbb{E}[X_{t+1}]\\&amp;=\mathbb{E}[\phi X_{t}+\varepsilon_{t}]\\
&amp;=\phi\mu_{X}(t)\\
\mbox{Cov}_{X}(t+1,t)   &amp;=\mbox{Cov}(X_{t+1},X_{t})\\
&amp;=\mbox{Cov}(\phi X_{t}+\epsilon_{t},X_{t})
    \\&amp;=\phi\mbox{Cov}(X_{t},X_{t})+\mbox{Cov}(\epsilon_{t},X_{t})
    \\&amp;=\phi\mbox{Cov}_{X}(t,t)\end{align*}\]</span>
Sometimes people are interested in AR models with invariant variances, namely <span class="math display">\[\mbox{Var}(X_t(\omega))=\cdots=\mbox{Var}(X_1(\omega))=\sigma^2.\]</span><br>
Then there is
<span class="math display">\[\sigma^{2} = \phi^{2} \sigma^{2} + \sigma_{\epsilon}^{2}\]</span> which implies <span class="math inline">\(\sigma=\sigma_{\epsilon}^{2}/(1-\phi)\)</span>. Thus the <strong>covariance function</strong> of this AR(1) model can be simplified into <span class="math display">\[\mbox{Cov}_{X}(t+1,t)=\frac{\sigma_{\epsilon}^{2}\phi}{1-\phi}.\]</span></span>
<span class="math display">\[\mu_{X}(t+1)=\phi\mu_{X}(t),\,\,\mbox{Cov}_{X}(t+1,t)=\phi\mbox{Cov}_{X}(t,t).\]</span></p>
<p>Another essential sub-class of the <strong>second-order processes</strong> is the <em>Gaussian process</em>, whose probability law is uniquely determined by the following specification <span class="math display">\[X(\cdot,\omega)\sim \mathcal{N}(\mu_X(\cdot),\mbox{Cov}_X(\cdot,\cdot)).\]</span> To understand the above specification, let’s consider a real-valued random vector <span class="math inline">\(\mathbf{X}(\omega)\in\mathbb{R}^{n}\)</span> with <a href="ch-CalUn.html#sub:divRV">Gaussian distribution</a> <span class="math inline">\(\mathbf{X}(\omega)\sim \mathcal{N}(\mathbf{\mu},\Sigma)\)</span>.
The first-order information is given by the mean vector <span class="math inline">\(\mathbb{E}[\mathbf{X}(\omega)]=\mathbf{\mu}\)</span>; the finite second-order information is contained in the <a href="ch-UnMulti.html#sub:MultiVar">covariance matrix</a> <span class="math inline">\(\mbox{Var}(\mathbf{X}(\omega))=\Sigma\)</span>. Now let’s turn to a <strong>Gaussian process</strong> <span class="math inline">\(X(t,\omega)\in\mathbb{R}\)</span> with mean function <span class="math inline">\(\mu_X(\cdot)\)</span> and <span class="math inline">\(\mbox{Cov}_X(\cdot,\cdot)\)</span>. If we visualize the process at any <span class="math inline">\(n\)</span> time points <span class="math inline">\((t_1,\dots,t_n)\)</span>, then the process looks like a random vector of the time series <span class="math inline">\(X_{t_1}(\omega),\dots,X_{t_n}(\omega)\)</span>:
<span class="math display">\[\underset{\mathbf{X}(\omega)}{\underbrace{\left[\begin{array}{c}
X(t_{1},\omega)\\
\vdots\\
X(t_{n},\omega)
\end{array}\right]}}\sim\mathcal{N}\left(\underset{\mathbf{\mu}}{\underbrace{\left[\begin{array}{c}
\mu_{X}(t_{1})\\
\vdots\\
\mu_{X}(t_{n})
\end{array}\right]}},\underset{\Sigma}{\underbrace{\left[\begin{array}{ccc}
\mbox{Cov}_{X}(t_{1},t_{1}) &amp; \cdots &amp; \mbox{Cov}_{X}(t_{1},t_{n})\\
\vdots &amp; \ddots &amp; \vdots\\
\mbox{Cov}_{X}(t_{n},t_{1}) &amp; \cdots &amp; \mbox{Cov}_{X}(t_{n},t_{n})
\end{array}\right]}}\right).\]</span>
Extending this <span class="math inline">\(n\)</span>-length random vector to infinity, we are supposed to have the <strong>Gaussian process</strong>:<label for="tufte-sn-429" class="margin-toggle sidenote-number">429</label><input type="checkbox" id="tufte-sn-429" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">429</span> The discretization of a continuous time <strong>Gaussian process</strong> has an interesting implication. The <a href="ch-UnMulti.html#sub:MultiVar">covariance matrices</a> as the <a href="ch-UnMulti.html#sub:MultiVar">positive (semi)-definite matrices</a> may have their corresponding underlying functions. Moreover, these functions may also have corresponding underlying <a href="ch-MatComp.html#sub:vecSpaces">operators</a> (the functions’ infinite-dimensional counterparts). In practice, the discretized <strong>Gaussian process</strong>’s <a href="ch-UnMulti.html#sub:MultiVar">covariance matrices</a> come from some “kernels” or called “positive operators.” We will come back to these concepts in sec[?].</span> <span class="math display">\[X(\cdot,\omega)\sim \mathcal{N}(\mu_X(\cdot),\mbox{Cov}_X(\cdot,\cdot)).\]</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:GP"></span>
<img src="fig/Part4/GP.png" alt="Gaussian processes with zero mean" width="100%"><!--
<p class="caption marginnote">-->Figure 16.2: Gaussian processes with zero mean<!--</p>-->
<!--</div>--></span>
</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:GPKern"></span>
<img src="fig/Part4/GPKern.png" alt="The corresponding covariance matrces" width="100%"><!--
<p class="caption marginnote">-->Figure 16.3: The corresponding covariance matrces<!--</p>-->
<!--</div>--></span>
</p>
<div class="solution">
<p class="solution-begin">
Simulation of Gaussian processes <span id="sol-start-203" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-203', 'sol-start-203')"></span>
</p>
<div id="sol-body-203" class="solution-body" style="display: none;">
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">20210228</span>)</a>
<a class="sourceLine" id="cb122-2" data-line-number="2"><span class="kw">library</span>(MASS) <span class="co"># call for multivariate normal sampling function</span></a>
<a class="sourceLine" id="cb122-3" data-line-number="3">n =<span class="st"> </span><span class="dv">100</span>;</a>
<a class="sourceLine" id="cb122-4" data-line-number="4">CovMatrix1 =<span class="st"> </span><span class="cf">function</span>(s, t) {<span class="kw">min</span>(s, t)}; <span class="co"># Brownian motion</span></a>
<a class="sourceLine" id="cb122-5" data-line-number="5">CovMatrix2 =<span class="st"> </span><span class="cf">function</span>(s,t){ <span class="kw">exp</span>(<span class="op">-</span><span class="dv">10</span><span class="op">*</span><span class="kw">abs</span>(s<span class="op">-</span>t))}; <span class="co"># OU</span></a>
<a class="sourceLine" id="cb122-6" data-line-number="6">CovMatrix3 =<span class="st"> </span><span class="cf">function</span>(s,t){ <span class="kw">as.numeric</span>(s<span class="op">==</span>t)}; <span class="co"># Gaussian white noise</span></a>
<a class="sourceLine" id="cb122-7" data-line-number="7">t =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">0</span>, <span class="dt">to =</span> <span class="dv">1</span>, <span class="dt">length.out =</span> n); <span class="co"># t_1=0,..., t_n=1</span></a>
<a class="sourceLine" id="cb122-8" data-line-number="8"></a>
<a class="sourceLine" id="cb122-9" data-line-number="9"><span class="co"># Fill in entities of the covariance matrix</span></a>
<a class="sourceLine" id="cb122-10" data-line-number="10">CovMatrix1 =<span class="st"> </span><span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">1</span>){<span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">2</span>){</a>
<a class="sourceLine" id="cb122-11" data-line-number="11">     <span class="kw">CovMatrix1</span>(argument_<span class="dv">1</span>, argument_<span class="dv">2</span>)})})</a>
<a class="sourceLine" id="cb122-12" data-line-number="12">CovMatrix2 =<span class="st"> </span><span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">1</span>){<span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">2</span>){</a>
<a class="sourceLine" id="cb122-13" data-line-number="13">     <span class="kw">CovMatrix2</span>(argument_<span class="dv">1</span>, argument_<span class="dv">2</span>)})})</a>
<a class="sourceLine" id="cb122-14" data-line-number="14">CovMatrix3 =<span class="st"> </span><span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">1</span>){<span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">2</span>){</a>
<a class="sourceLine" id="cb122-15" data-line-number="15">     <span class="kw">CovMatrix3</span>(argument_<span class="dv">1</span>, argument_<span class="dv">2</span>)})})</a>
<a class="sourceLine" id="cb122-16" data-line-number="16"><span class="co"># Sampling</span></a>
<a class="sourceLine" id="cb122-17" data-line-number="17">samplePath1 =<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dt">mu =</span> <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dt">times =</span> n), <span class="dt">Sigma =</span> CovMatrix1);</a>
<a class="sourceLine" id="cb122-18" data-line-number="18">samplePath2 =<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dt">mu =</span> <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dt">times =</span> n), <span class="dt">Sigma =</span> CovMatrix2);</a>
<a class="sourceLine" id="cb122-19" data-line-number="19">samplePath3 =<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dt">mu =</span> <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dt">times =</span> n), <span class="dt">Sigma =</span> CovMatrix3);</a>
<a class="sourceLine" id="cb122-20" data-line-number="20"></a>
<a class="sourceLine" id="cb122-21" data-line-number="21"><span class="co"># plot</span></a>
<a class="sourceLine" id="cb122-22" data-line-number="22"><span class="kw">library</span>(ggplot2); <span class="kw">library</span>(reshape2); <span class="kw">library</span>(expm)</a>
<a class="sourceLine" id="cb122-23" data-line-number="23">dat =<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">c</span>(t,samplePath1, samplePath2, samplePath3), <span class="dt">ncol=</span><span class="dv">4</span>))</a>
<a class="sourceLine" id="cb122-24" data-line-number="24"><span class="kw">names</span>(dat)=<span class="kw">c</span>(<span class="st">"time"</span>,<span class="st">"Brownian motion"</span>,<span class="st">"Ornstein-Uhlenbeck"</span>,<span class="st">"Gaussian white noise"</span>);</a>
<a class="sourceLine" id="cb122-25" data-line-number="25"><span class="kw">ggplot</span>( <span class="kw">melt</span>(dat, <span class="dt">id.vars=</span><span class="st">"time"</span>), <span class="kw">aes</span>(time, value, <span class="dt">group=</span>variable)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()   <span class="op">+</span></a>
<a class="sourceLine" id="cb122-26" data-line-number="26"><span class="op">+</span><span class="st">   </span><span class="kw">facet_grid</span>(.<span class="op">~</span>variable)<span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</a>
<a class="sourceLine" id="cb122-27" data-line-number="27">m1=<span class="kw">Matrix</span>(CovMatrix1, <span class="dt">sparse=</span><span class="ot">TRUE</span>); m2=<span class="kw">Matrix</span>(CovMatrix2, <span class="dt">sparse=</span><span class="ot">TRUE</span>); m3=<span class="kw">Matrix</span>(CovMatrix3, <span class="dt">sparse=</span><span class="ot">TRUE</span>);</a>
<a class="sourceLine" id="cb122-28" data-line-number="28"><span class="kw">print</span>(<span class="kw">image</span>(m1, <span class="dt">main=</span><span class="st">"Brownian motion"</span>), <span class="dt">split=</span><span class="kw">c</span>(<span class="dt">x=</span><span class="dv">1</span>,<span class="dt">y=</span><span class="dv">1</span>,<span class="dt">nx=</span><span class="dv">3</span>, <span class="dt">ny=</span><span class="dv">1</span>), <span class="dt">more=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb122-29" data-line-number="29"><span class="kw">print</span>(<span class="kw">image</span>(m2, <span class="dt">main=</span><span class="st">"Ornstein-Uhlenbeck"</span>), <span class="dt">split=</span><span class="kw">c</span>(<span class="dt">x=</span><span class="dv">2</span>,<span class="dt">y=</span><span class="dv">1</span>,<span class="dt">nx=</span><span class="dv">3</span>, <span class="dt">ny=</span><span class="dv">1</span>), <span class="dt">more=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb122-30" data-line-number="30"><span class="kw">print</span>(<span class="kw">image</span>(m3, <span class="dt">main=</span><span class="st">"Gaussian white noise"</span>, <span class="dt">Imult=</span><span class="fl">0.1</span>), <span class="dt">split=</span><span class="kw">c</span>(<span class="dt">x=</span><span class="dv">3</span>,<span class="dt">y=</span><span class="dv">1</span>,<span class="dt">nx=</span><span class="dv">3</span>, <span class="dt">ny=</span><span class="dv">1</span>))</a></code></pre></div>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Now, we come back to the discussion of the differentiation issue for the stochastic process <span class="math inline">\(X(t,\omega)\)</span>. The issue is resolvable with defining a meaningful calculus towards the randomness.
The following definition gives the <em>mean square calculus</em> for the processes in <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong>.</p>
<ul>
<li><p><strong>Mean square continuity</strong>: When <span class="math display">\[\lim_{h\rightarrow0}\mathbb{E}\left[\left|X(t+h, \omega)-X(t, \omega)\right|^{2}\right]=0,\]</span>
the random variable <span class="math inline">\(X(t,\omega)\)</span> is <em>mean square continous</em> at <span class="math inline">\(t\)</span>.</p></li>
<li><p><strong>Mean square differentiability</strong> : When the second order derivative <span class="math display">\[\frac{\partial^{2}\mathbb{E}[X(t,\omega)X(s,\omega)]}{\partial t\partial s}\]</span>
exists for any <span class="math inline">\(t,s \in [a,b]\)</span>, <span class="math inline">\(X(t,\omega)\)</span> is <em>mean square differentiable</em> such that <span class="math display">\[\lim_{h\rightarrow0}\mathbb{E}\left[\left|\frac{X(t+h,\omega)-X(t,\omega)}{h}-\frac{\mbox{d}X(t,\omega)}{\mbox{d}t}\right|^{2}\right]=0.\]</span></p></li>
<li><p><strong>Mean square integrability</strong> : When the (Riemann) <a href="sub-calculus.html#sub:diffInt">integral</a> <span class="math display">\[\int_{a}^{b}\int_{a}^{b}\mathbb{E}[X(t,\omega)X(s,\omega)]\mbox{d}t\mbox{d}s\]</span> is well defined over the interval <span class="math inline">\([a,b]\)</span>, then <span class="math inline">\(X(t,\omega)\)</span> is <em>mean square integrable</em> over <span class="math inline">\([a,b]\)</span> with the <em>mean square integral</em> <span class="math inline">\(\int_{a}^{b} X(t,\omega) \mbox{d}t\)</span>.<label for="tufte-sn-430" class="margin-toggle sidenote-number">430</label><input type="checkbox" id="tufte-sn-430" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">430</span> Here is a more formal definition of the <em>mean square integral</em>. For an interval <span class="math inline">\([a,b]\)</span>, we make a partition <span class="math inline">\(a=t_{0}&lt;t_{1}&lt;\cdots&lt;t_{n}=b\)</span> and
let <span class="math inline">\(\kappa=\max_{i}(t_{i+1}-t_{i})\)</span>, <span class="math inline">\(t_{i}\leq m_{i}\leq t_{i+1}\)</span>. The <strong>mean square integral</strong> of <span class="math inline">\(X(t,\omega)\)</span> is given by the <strong>mean square limit</strong> of <span class="math inline">\(\sum_{i=0}^{n-1}X_{m_{i}}(\omega)(t_{i+1}-t_{i})\)</span> that is
<span class="math display">\[\begin{align*}\lim_{\kappa\rightarrow0, n\rightarrow\infty}\mathbb{E}\left[\left|\sum_{i=0}^{n-1}X_{m_{i}}(\omega)(t_{i+1}-t_{i})-\\
\int_{a}^{b}X(t,\omega)\mbox{d}t\right|^{2}\right]=0.\end{align*}\]</span></span></p></li>
<li><p><em>Fundamental theorem of mean square calculus</em> : <span class="math display">\[\Pr\left\{ \left|X(t,\omega)-X(a,\omega)=\int_{a}^{t}\left(\frac{\mbox{d}X(\tau,\omega)}{\mbox{d}\tau}\right)\mbox{d}\tau\right|\right\} =1.\]</span></p></li>
</ul>
<p>The term <span class="math inline">\(\mathbb{E}[X(t,\omega)X(s,\omega)]\)</span> connects to the <strong>mean square</strong> criterion:
<span class="math display">\[\begin{align*}\mathbb{E}\left[\left|X(t+h)-X(t)\right|^{2}\right]&amp;=\mathbb{E}\left[X(t+h)X(t+h)\right]-\mathbb{E}\left[X(t+h)X(t)\right]\\
&amp;-\mathbb{E}\left[X(t)X(t+h)\right]+\mathbb{E}\left[X(t)X(t)\right].\end{align*}\]</span>
That is to say, the <strong>mean square continuity</strong> is all about continuity of <span class="math inline">\(\mathbb{E}[X(t,\omega)X(s,\omega)]\)</span> for any <span class="math inline">\(t,s\)</span>. In addition, both <strong>mean square differentiability and integrability</strong> relate to the calculus of <span class="math inline">\(\mathbb{E}[X(t,\omega)X(s,\omega)]\)</span>.<label for="tufte-sn-431" class="margin-toggle sidenote-number">431</label><input type="checkbox" id="tufte-sn-431" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">431</span> To have a vague idea about this relationship, recall that the definitions of <a href="sub-calculus.html#sub:diffInt">differentiation</a> and <a href="sub-calculus.html#sub:diffInt">integration</a> are both based on the continuity of some criteria of <a href="sub-continuity.html#sub:rational">infinitesimal changes</a>. Thus, the <strong>mean square differentiability and integrability</strong> must be based on the continuity of some <strong>mean square</strong> criteria of <a href="sub-continuity.html#sub:rational">infinitesimal changes</a>. As these <strong>mean square</strong> criteria all relate to the terms of <span class="math inline">\(\mathbb{E}[X(t,\omega)X(s,\omega)]\)</span>, we can see that the essence of defining <strong>mean square differentiability and integrability</strong> is to define the differentiability and integrability of <span class="math inline">\(\mathbb{E}[X(t,\omega)X(s,\omega)]\)</span> for any <span class="math inline">\(t,s\)</span>.</span>
By the equality
<span class="math display">\[\mathbb{E}[X(t,\omega)X(s,\omega)]=\mbox{Cov}_X (t,s)+\mu_{X}(t)\mu_{X}(s),\]</span> one can induce that the <strong>mean square continuity, differentiability, and integrability</strong> actually relate to the continuity and the calculus of <strong>mean and covariance functions</strong>.</p>
<div class="solution">
<p class="solution-begin">
Some remark about the mean square criterion <span id="sol-start-204" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-204', 'sol-start-204')"></span>
</p>
<div id="sol-body-204" class="solution-body" style="display: none;">
<p>Let’s discretize the stochastic process <span class="math inline">\(X(t,\omega)\)</span> into a stochastic sequence <span class="math inline">\(\{X_{n}(\omega)\}_{n\in\mathbb{N}}\)</span>.</p>
<ol style="list-style-type: decimal">
<li>The <strong>mean square convergence</strong> implies probabilistic convergence in section <a href="ch-UnMulti.html#sub:WLLN">13.4</a>.</li>
</ol>
<p>Proof: For any <span class="math inline">\(\epsilon&gt;0\)</span> and a random variable <span class="math inline">\(Y\)</span> with a finite second moment, we have
<span class="math display">\[\begin{align*}\mathbb{E}[|Y|^{2}]=&amp;   \int|y|^{2}p(y)\mbox{d}y\\ &amp;=\int_{-\infty}^{-\epsilon}|y|^{2}p(y)\mbox{d}y+\int_{-\epsilon}^{\infty}|y|^{2}p(y)\mbox{d}y\\
&amp;\geq   \epsilon^{2}\left(\int_{-\infty}^{-\epsilon}p(y)\mbox{d}y+\int_{-\epsilon}^{\infty}p(y)\mbox{d}y\right)\\
&amp;=\epsilon^{2}\Pr\left\{ |y|\geq\epsilon\right\}.\end{align*}\]</span>
Substituting <span class="math inline">\(X-X_{n}\)</span> for <span class="math inline">\(Y\)</span>, we have
<span class="math display">\[\Pr\left\{ |X-X_{n}|\geq\epsilon\right\} \leq\frac{\mathbb{E}\left[\left|X_{n}-X\right|^{2}\right]}{\epsilon^{2}}\]</span>
by the <a href="ch-UnMulti.html#sub:WLLN">Markov inequality</a> (see the proof of <a href="ch-UnMulti.html#sub:WLLN">weak law of large numbers</a>).</p>
<ol start="2" style="list-style-type: decimal">
<li><p>The <a href="sub-continuity.html#sub:Cauchy">Cauchy criterion</a> in the <strong>mean square</strong> sense means
<span class="math display">\[\lim_{n,m\rightarrow\infty}\mathbb{E}\left[X_{n}-X_{m}\right]^{2}=0.\]</span>
If the <a href="sub-continuity.html#sub:Cauchy">Cauchy criterion</a> holds, we say that the stochastic sequence <span class="math inline">\(\{X_{n}\}\)</span> <strong>converges in mean square</strong> to some <span class="math inline">\(X\)</span>,
<span class="math display">\[\lim_{n\rightarrow\infty}\mathbb{E}\left[X_{n}-X\right]^{2}=0.\]</span></p></li>
<li><p>If <span class="math inline">\(X_n\)</span> <strong>converges in mean square</strong> to <span class="math inline">\(X\)</span>, then
<span class="math display">\[\mathbb{E}[X]=\lim_{n\rightarrow\infty}\mathbb{E}[X_n].\]</span></p></li>
</ol>
<p>Proof: Notice that the following equality <span class="math display">\[\lim_{n\rightarrow\infty}\mathbb{E}\left[\left|X_{n}-X\right|^{2}\right]=\lim_{n\rightarrow\infty}\left|\mathbb{E}[X_{n}-X]\right|^{2}=0\]</span>
can be verified by the <a href="ch-vecMat.html#sub:vec">Schwarz inequality</a>: <span class="math display">\[\left|\mathbb{E}[X_{n}-X]\right|^{2}\leq\mathbb{E}\left[\left|X_{n}-X\right|^{2}\right].\]</span>
Because <span class="math inline">\(\lim_{n\rightarrow\infty}\left|\mathbb{E}[X_{n}-X]\right|^{2}=0\)</span> means
<span class="math display">\[\lim_{n\rightarrow\infty}\mathbb{E}[X_{n}-X]=0,\]</span> the result follows.</p>
<ol start="4" style="list-style-type: decimal">
<li>If <span class="math inline">\(X_n\)</span> and <span class="math inline">\(Y_n\)</span> <strong>converge in mean square</strong> to <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, then <span class="math display">\[\mathbb{E}[XY]=\lim_{n\rightarrow\infty}\mathbb{E}[X_{n}Y_{n}].\]</span>
</li>
</ol>
<p>Proof: Note that if <span class="math inline">\(X_n\)</span> and <span class="math inline">\(Y_n\)</span> <strong>converge in mean square</strong> to <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, we have
<span class="math display">\[\lim_{n\rightarrow\infty}\mathbb{E}[X_{n}-X]=\lim_{n\rightarrow\infty}\mathbb{E}[Y_{n}-Y]=0.\]</span></p>
<p>To see this, we make the following inequality
<span class="math display">\[\begin{align*}\left|\mathbb{E}[XY]-\mathbb{E}[X_{n}Y_{n}]\right|  =\left|\mathbb{E}[(Y-Y_{n})X_{n}]+\mathbb{E}[(X-X_{t})Y_{n}]-\mathbb{E}[(X-X_{t})(Y-Y_{n})]\right|\\
\leq    \left|\mathbb{E}[(Y-Y_{n})X_{n}]\right|+\left|\mathbb{E}[(X-X_{n})Y_{n}]\right|+\left|\mathbb{E}[(X-X_{n})(Y-Y_{n})]\right|.\end{align*}\]</span>
Then we apply <a href="ch-vecMat.html#sub:vec">Schwarz inequality</a> to the last three terms and take the limit
<span class="math display">\[\begin{align*}
&amp;\lim_{n\rightarrow\infty}\left|\mathbb{E}[XY]-\mathbb{E}[X_{n}Y_{n}]\right|    \leq
\lim_{n\rightarrow\infty}   \mathbb{E}\left|Y-Y_{n}|\mathbb{E}|X_{n}\right|\\
+&amp;\mathbb{E}\left|X-X_{n}| \mathbb{E}|Y_{n}\right|+\mathbb{E}\left|X-X_{n}|\mathbb{E}|Y-Y_{n}\right|\rightarrow0.\end{align*}\]</span>
The result follows.</p>
<ol start="5" style="list-style-type: decimal">
<li>In the view of the <a href="sub-continuity.html#sub:Cauchy">Cauchy criterion</a>, the <strong>mean square differentiability</strong> is about the convergence of the infinitesimal stochastic sequence <span class="math inline">\(\left\{\frac{X_{t+h}-X_{t}}{h}\right\}\)</span> such that <span class="math display">\[\lim_{h,s\rightarrow0}\mathbb{E}\left[\left|\frac{X_{t+h}-X_{t}}{h}-\frac{X_{t+s}-X_{t}}{s}\right|^{2}\right]=0.\]</span> We can show that the convergence holds if <span class="math inline">\(\frac{\partial^{2}\mathbb{E}[X_{t}(\omega)X_{s}(\omega)]}{\partial t\partial s}\)</span> exists.</li>
</ol>
<p>Proof: Because <span class="math display">\[\begin{align*}\mathbb{E}\left[\left|\frac{X_{t+h}-X_{t}}{h}-\frac{X_{t+s}-X_{t}}{s}\right|^{2}\right]&amp;=\mathbb{E}\left[\frac{X_{t+h}-X_{t}}{h}\frac{X_{t+h}-X_{t}}{h}\right]\\-2\mathbb{E}\left[\frac{X_{t+h}-X_{t}}{h}\frac{X_{t+s}-X_{t}}{s}\right]&amp;+\mathbb{E}\left[\frac{X_{t+s}-X_{t}}{s}\frac{X_{t+s}-X_{t}}{s}\right].\end{align*}\]</span></p>
<p>Notice that <span class="math display">\[\lim_{h,s\rightarrow0}\mathbb{E}\left[\frac{X_{t+h}-X_{t}}{h}\times\frac{X_{t+s}-X_{t}}{s}\right] =\lim_{h,s\rightarrow0}\frac{1}{hs}\left[\mathbb{E}[X_{t+h}X_{t+s}]\\-\mathbb{E}[X_{t}X_{t+s}]-(\mathbb{E}[X_{t+h}X_{t}]-\mathbb{E}[X_{t}X_{t}])\right]=\frac{\partial^{2}\mathbb{E}[X_{t}X_{t}]}{\partial t\partial t}.\]</span></p>
<p>Similarly,
<span class="math display">\[\begin{align*}\lim_{h\rightarrow0}\mathbb{E}\left[\frac{X_{t+h}-X_{t}}{h}\frac{X_{t+h}-X_{t}}{h}\right]&amp;=\lim_{s\rightarrow0}\mathbb{E}\left[\frac{X_{t+h}-X_{t}}{h}\frac{X_{t+h}-X_{t}}{h}\right]\\&amp;=\frac{\partial^{2}\mathbb{E}[X_{t}X_{t}]}{\partial t\partial t}.\end{align*}\]</span>
So when the second derivatives of <span class="math inline">\(\mathbb{E}[X_{t}X_{t}]\)</span> exists, the sequence <span class="math inline">\(\left\{\frac{X_{t+h}-X_{t}}{h}\right\}\)</span> <strong>converges in mean square</strong> to <span class="math inline">\(\mbox{d}X_{t}/\mbox{d}t\)</span>.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>With the <strong>mean square calculus</strong>, we can define a <strong>white noise process</strong>, a baseline of modeling unpredictable fluctuation in the <strong>second-order processes</strong>. (See figure <a href="ch-randomization.html#fig:GP">16.2</a>.)</p>
<p>Let <span class="math inline">\(\varepsilon(t,\omega)\)</span> denote the <strong>white noise</strong>. The process has zero <strong>mean function</strong> and its <strong>covariance function</strong> is <span class="math display">\[\mbox{Cov}_{\varepsilon}(t,s)=\sigma^{2}\delta(t-s)\]</span> where <span class="math inline">\(\delta(\cdot)\)</span> is called the <em>delta function</em> such that <span class="math display">\[\begin{align*}\delta(x)=\begin{cases}
\infty, &amp; x=0,\\
0, &amp; x\neq0,
\end{cases} &amp; \; \mbox{ and }\\ 
\int_{-\infty}^{\infty}\delta(x)\mbox{d}x=1,&amp; \,\, \int_{-\infty}^{\infty}f(x)\delta(x)\mbox{d}x=f(0)
\end{align*}\]</span>
for any continuous function <span class="math inline">\(f(\cdot)\)</span>. Basically, a <strong>delta function</strong> is an infinite “spike” above a single point.<label for="tufte-sn-432" class="margin-toggle sidenote-number">432</label><input type="checkbox" id="tufte-sn-432" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">432</span> One can shift the origin to any point <span class="math inline">\(t\)</span>
by setting the argument to <span class="math inline">\(x-t\)</span> such that <span class="math display">\[\delta(x-t)=\begin{cases}
\infty, &amp; x=t,\\
0, &amp; x\neq t.
\end{cases}\]</span></span>
The <strong>delta function</strong> can be thought of as the derivative of the step function <span class="math inline">\(\mathbf{1}_{\{x&gt;0\}}(x)\)</span> that has zero everywhere and goes to infinity at <span class="math inline">\(0\)</span>.<label for="tufte-sn-433" class="margin-toggle sidenote-number">433</label><input type="checkbox" id="tufte-sn-433" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">433</span> Note that for any <span class="math inline">\(\epsilon&gt;0\)</span>, the derivative of the step function is <span class="math display">\[\begin{align*}
&amp;\lim_{\epsilon\rightarrow0}\frac{\mathbf{1}_{\{x&gt;0\}}(x+\epsilon)-\mathbf{1}_{\{x&gt;0\}}(x)}{\epsilon}=\\
&amp;\begin{cases}
\lim_{\epsilon\rightarrow0}\frac{1}{\epsilon}\rightarrow\infty &amp; x=0\\
\lim_{\epsilon\rightarrow0}\frac{0}{\epsilon}\rightarrow 0 &amp; x\neq 0
\end{cases}.\end{align*}\]</span></span></p>
<p>For a <strong>white noise process</strong>, if the probability law in the <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong> is Gaussian, then the <strong>white noise</strong> is called <em>Gaussian white noise</em>,<label for="tufte-sn-434" class="margin-toggle sidenote-number">434</label><input type="checkbox" id="tufte-sn-434" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">434</span> Notice that the inner product <span class="math inline">\(\langle f,\delta \rangle=f(0)\)</span> is well-defined. So the <strong>delta function</strong> is somehow similar to the <a href="sub-set-theory.html#sub:func">indicator function</a> at the origin as <span class="math display">\[\int_{-\infty}^{\infty}f(x)\mathbf{1}_{\{x=0\}}(x)\mbox{d}x=f(0).\]</span> In practice, one implements <span class="math inline">\(\mathbf{1}_{\{x=t\}}(x)\)</span>
instead of <span class="math inline">\(\delta(x-t)\)</span> as a (numerical) <strong>delta function</strong>, because the infinite value of the <strong>delta function</strong> is impossible for the implementation.</span> <span class="math display">\[\varepsilon(t,\omega)\sim\mathcal{N}(0,\sigma^{2}\delta(x-t)).\]</span></p>
<p>The <strong>Gaussian white noise</strong> <span class="math inline">\(\varepsilon(t,\omega)\)</span> corresponds to the <strong>mean square derivative</strong> of a <a href="ch-DE.html#sub:pde">Brownian motion</a> <span class="math inline">\(X(t,\omega)\)</span>:
<span class="math display">\[\varepsilon(t,\omega)=\frac{\mbox{d}X(t,\omega)}{\mbox{d}t}.\]</span>
To see the connection, we analyze the <strong>mean and covariance functions</strong> of both processes. The <strong>mean functions</strong> of both white noise and Brownian motion are zero. So the <strong>mean square derivative</strong> does not change anything for the mean. The <strong>covariance function</strong> of the <a href="ch-DE.html#sub:pde">Brownian motion</a> is <span class="math inline">\(\mbox{Cov}_{X}(t,s)=\sigma^{2}\min(t,s)\)</span>. According to the <strong>mean square differentiability</strong>, we should check the second derivative of this function, and we can see it equals to the <strong>covariance function</strong> of the <strong>white noise</strong><label for="tufte-sn-435" class="margin-toggle sidenote-number">435</label><input type="checkbox" id="tufte-sn-435" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">435</span> Note that<span class="math display">\[\min(t,s)=\begin{cases}
s, &amp; s&lt;t\\
t, &amp; s&gt;t
\end{cases},\\
\frac{\partial\min(t,s)}{\partial t}=\mathbf{1}_{\{s&gt;t\}}(s)=\begin{cases}
0, &amp; s&lt;t\\
1, &amp; s&gt;t
\end{cases}.\]</span>
That is, the first derivative of <span class="math inline">\(\min(t,s)\)</span> is the step function <span class="math inline">\(\mathbf{1}_{\{s&gt;t\}}(s)\)</span>. As we know, the derivative of a step function is the <strong>delta function</strong>. The result follows.</span> <span class="math display">\[\mbox{Cov}_{\varepsilon}(t,s)=\frac{\partial^{2}\mbox{Cov}_{B}(t,s)}{\partial t\partial s}=\sigma^{2}\frac{\partial^{2}\min(t,s)}{\partial t\partial s}=\sigma^{2}\delta(t-s).\]</span>
Thus, we have the result that the <strong>mean square differentiation</strong> of a <a href="ch-DE.html#sub:pde">Brownian motion</a> is a <strong>white noise process</strong>.<label for="tufte-sn-436" class="margin-toggle sidenote-number">436</label><input type="checkbox" id="tufte-sn-436" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">436</span> The discretization of the covariance functions of Brownian motion and white noise can be found in figure <a href="ch-randomization.html#fig:GPKern">16.3</a>.</span></p>
<p>Conversely, if we consider <span class="math inline">\(\int_{s}^{t}\varepsilon(\tau,\omega)\mbox{d}\tau\)</span> to be a “proper” integral, by the <strong>fundamental theorem of mean square calculus</strong>, we expect to have
<span class="math display">\[\int_{s}^{t}\varepsilon(\tau,\omega)\mbox{d}\tau=X(t,\omega)-X(s,\omega)\mbox{ with probability }1,\]</span>
where <span class="math inline">\(X(t,\omega)-X(s,\omega)\)</span> is also called the <em>increment</em> of <a href="ch-DE.html#sub:pde">Brownian motion</a>.<label for="tufte-sn-437" class="margin-toggle sidenote-number">437</label><input type="checkbox" id="tufte-sn-437" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">437</span> The “integral” <span class="math inline">\(\int_{s}^{t}\varepsilon(\tau,\omega)\mbox{d}\tau\)</span>, however, is not technically well defined over here. A precise definition of the integral is not trivial, because some quantities that are finite sums in the finite discrete case must be replaced by integrals that may not converge in the <strong>mean square</strong> sense. Thus, instead of dealing with the rigorous definition, here we only consider a vague presentation of the result.</span>
The <strong>covariance function</strong> of the <strong>increment of Brownian motion</strong> <span class="math inline">\(X(t,\omega)-X(s,\omega)\)</span> follows the probability law <span class="math inline">\(\mathcal{N}(0,(t-s)\sigma^{2})\)</span>, which is what we expect to see on the “integral” of the <strong>white noise process</strong>.<label for="tufte-sn-438" class="margin-toggle sidenote-number">438</label><input type="checkbox" id="tufte-sn-438" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">438</span> The fact is that <a href="ch-DE.html#sub:pde">Brownian motion</a> is a mathematically tractable version of the idea of the <a href="ch-UnMulti.html#sub:rw">random walk</a> in the continuous time setting. Intuitively, one can think of <a href="ch-UnMulti.html#sub:rw">random walks</a> <span class="math inline">\(X_{s}=\varepsilon_{1}+\cdots+\varepsilon_{s}\)</span>
and <span class="math inline">\(X_{t}=\varepsilon_{1}+\cdots+\varepsilon_{t}\)</span>. Then <span class="math display">\[X_{t}-X_{s}=\underset{(t-s)-\mbox{entities}}{\underbrace{\varepsilon_{t}+\varepsilon_{t-1}+\cdots+\varepsilon_{s}}}.\]</span> For each <span class="math inline">\(\varepsilon_{i}\sim\mathcal{N}(0,\sigma^{2})\)</span>, the <a href="ch-CalUn.html#sub:divRV">Gaussian property</a> tells that <span class="math display">\[\sum_{i=s}^{t}\varepsilon_{i}\sim\mathcal{N}(0,(t-s)\sigma^{2}).\]</span></span></p>
</div>
<div id="sub:SumDecomp" class="section level2">
<h2>
<span class="header-section-number">16.2</span> Direct Sums and Decompositions</h2>
<p>When we compare the <a href="ch-representation.html#ch:representation">representation</a> of a <a href="ch-UnMulti.html#sub:rw">random walk</a> <span class="math inline">\(X_{t}(\omega)\in\mathbb{R}\)</span> and that of a vector <span class="math inline">\(\mathbf{x}\in\mathbb{R}^{t}\)</span>, we can find some interesting similarities:
<span class="math display">\[\begin{align*}X_{t}(\omega) &amp;=    \varepsilon_{1}(\omega)+\cdots+\varepsilon_{t}(\omega), \, \varepsilon_{i} \sim \mathcal{N}(0,1), \\
\left\langle \varepsilon_{i}(\omega),\varepsilon_{j}(\omega)\right\rangle _{\mathbb{P}}&amp;=\mathbb{E}\left[\left\langle \varepsilon_{i}(\omega),\varepsilon_{j}(\omega)\right\rangle \right]  =\begin{cases}
0, &amp; i\neq j\\
1, &amp; i=j
\end{cases},\\
\mathbf{x} &amp;=   c_{1}\mathbf{e}_{1}+\cdots+c_{n}\mathbf{e}_{t},\\
\left\langle \mathbf{e}_{i},\mathbf{e}_{j}\right\rangle &amp;=\begin{cases}
0, &amp; i\neq j\\
1, &amp; i=j
\end{cases}.\end{align*}\]</span>
The (discrete time) <a href="ch-randomization.html#sub:RHilbert">Gaussian white noise</a> <span class="math inline">\(\varepsilon_i(\omega)\)</span> with unit variance looks like a “whitened” <a href="ch-vecMat.html#sub:vec">unit vector</a> <span class="math inline">\(\mathbf{e}_i\)</span>. Moreover, the collection of white noises seems to play the same role as the <a href="ch-UnMulti.html#sub:MultiVar">orthonormal basis</a> to the random walk <span class="math inline">\(X_{t}(\omega)\)</span> in a <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a>.</p>
<p>Given this result, a natural question is to ask whether we can represent any <a href="ch-randomization.html#sub:RHilbert">second-order</a> <span class="math inline">\(\mathcal{H}\)</span>-valued time series or stochastic process in terms of white noises? If this is the case, then we can think the <a href="ch-randomization.html#sub:RHilbert">randomization</a> of the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a> is about randomizing the Hilbert space <span class="math inline">\(\mathcal{H}\)</span> through “whitening” the <a href="ch-representation.html#sub:innerProd">basis</a> of <span class="math inline">\(\mathcal{H}\)</span>.</p>
<p>The route of our exposition will start with defining how to construct a space by many <a href="ch-MatComp.html#sub:vecSpaces">subspaces</a>; then we will study the subspaces generated by white noises and will see what kind of processes can live in the “combination” of those subspaces. The switch of the focus from white noises to the space of white noises will help us to bridge the difference between the <a href="ch-representation.html#ch:representation">representations</a> of the stochastic objects and of the deterministic ones.</p>
<p>Now we start with introducing the way of “combining” the <a href="ch-MatComp.html#sub:vecSpaces">subspaces</a>.</p>
<ul>
<li>
<strong>Direct sums, decompositions</strong> and <strong>complements</strong> : Consider two <a href="ch-MatComp.html#sub:vecSpaces">vector spaces</a> <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> over the same <a href="ch-MatComp.html#sub:vecSpaces">scalar field</a> <span class="math inline">\(\mathbb{F}\)</span>. When the two spaces can be combined to form a new vector space <span class="math inline">\(\mathcal{V}\)</span>, then <span class="math inline">\(\mathcal{V}\)</span> is called the <em>direct sum</em> of <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span>, denoted by <span class="math display">\[\mathcal{V}=\mathcal{V}_{1}+\mathcal{V}_{2}.\]</span>
If <span class="math inline">\(\mathcal{V}\)</span> is the <strong>direct sum</strong> of <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span>, then all objects in <span class="math inline">\(\mathcal{V}\)</span> can be <em>decompose</em> into those from <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> such that for any <span class="math inline">\(f\in\mathcal{V}\)</span>, there is <span class="math inline">\(f=f_{1}+f_{2}\)</span> where <span class="math inline">\(f_{1}\in\mathcal{V}_{1}\)</span> and <span class="math inline">\(f_{2}\in\mathcal{V}_{2}\)</span>, and we say <span class="math inline">\(\mathcal{V}_1\)</span> and <span class="math inline">\(\mathcal{V}_2\)</span> are <em>complements</em> of each other. In addition, if <span class="math inline">\(\mathcal{V}_{1}\)</span>
and <span class="math inline">\(\mathcal{V}_{2}\)</span> are <a href="ch-representation.html#sub:conjugacy">Hilbert spaces</a>, and they are <em>disjoint</em><label for="tufte-sn-439" class="margin-toggle sidenote-number">439</label><input type="checkbox" id="tufte-sn-439" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">439</span> The disjoint property tells that the <strong>direct sum</strong> can be uniquely <strong>decomposed</strong>. The proof is given below.</span> and <em>orthogonal</em><label for="tufte-sn-440" class="margin-toggle sidenote-number">440</label><input type="checkbox" id="tufte-sn-440" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">440</span> The orthogonality of the subspaces <span class="math inline">\(\mathcal{V}_1 \bot \mathcal{V}_2\)</span> are defined as <span class="math display">\[\langle f,g\rangle=0\]</span> for all <span class="math inline">\(f\in\mathcal{V}_1\)</span> and <span class="math inline">\(g\in\mathcal{V}_2\)</span>.</span>, we say <span class="math inline">\(\mathcal{V}\)</span> is the <em>orthogonal direct sum</em> of <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span>, denoted by <span class="math display">\[\mathcal{V}=\mathcal{V}_{1}\oplus\mathcal{V}_{2},\]</span> where <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> are <em>orthogonal complements</em> of one another.<label for="tufte-sn-441" class="margin-toggle sidenote-number">441</label><input type="checkbox" id="tufte-sn-441" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">441</span> The <strong>orthogonal complement</strong> of <span class="math inline">\(\mathcal{V}_1\subset\mathcal{V}\)</span> is denoted by <span class="math inline">\(\mathcal{V}_1^{\bot}\)</span>. If <span class="math inline">\(\mathcal{V}=\mathcal{V}_{1}\oplus\mathcal{V}_{2}\)</span>, then <span class="math inline">\(\mathcal{V}_2 = \mathcal{V}_1^{\bot}\)</span>.</span>
</li>
</ul>
<div class="solution">
<p class="solution-begin">
Proof <span id="sol-start-205" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-205', 'sol-start-205')"></span>
</p>
<div id="sol-body-205" class="solution-body" style="display: none;">
<p>For two subspaces <span class="math inline">\(\mathcal{V}_1\)</span> and <span class="math inline">\(\mathcal{V}_2\)</span>, the following two statements are equivalent:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(f\in \mathcal{V}_{1} + \mathcal{V}_{2}\)</span>, and <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> are disjoint.</p></li>
<li><p>There is a unique <span class="math inline">\(f_{1}\in\mathcal{V}_{1}\)</span> and a unique <span class="math inline">\(f_{2}\in\mathcal{V}_{2}\)</span> to <strong>decompose</strong> <span class="math inline">\(f\)</span>: <span class="math display">\[f=f_{1}+f_{2}.\]</span></p></li>
</ol>
<p>Proof:</p>
<ol style="list-style-type: decimal">
<li>Suppose that <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> are disjoint. Let <span class="math display">\[f=f_{1}+f_{2}=g_{1}+g_{2},\]</span> where <span class="math inline">\(f_{1},g_{1}\in\mathcal{V}_{1}\)</span> and <span class="math inline">\(f_{2},g_{2}\in\mathcal{V}_{2}\)</span>. So we have <span class="math inline">\(f_{1}-g_{1}=g_{2}-f_{2}\)</span>. By the <a href="ch-MatComp.html#sub:vecSpaces">subspace property</a>, we have <span class="math inline">\(f_{1}-g_{1}\in\mathcal{V}_{1}\)</span> and <span class="math inline">\(f_{2}-g_{2}\in\mathcal{V}_{2}\)</span>. By the disjoint condition <span class="math inline">\(\mathcal{V}_{1}\cap\mathcal{V}_{2}=\{0\}\)</span>, one can induce that <span class="math display">\[f_{1}-g_{1}=g_{2}-f_{2}=0,\]</span> which implies <span class="math inline">\(f_{1}=g_{1}\)</span> and <span class="math inline">\(g_{2}=f_{2}\)</span>.</li>
</ol>
<p>2: Now suppose that <span class="math inline">\(f_{1}\)</span> and <span class="math inline">\(f_{2}\)</span> are uniquely determined <span class="math inline">\(f=f_{1}+f_{2}\)</span> in <span class="math inline">\(\mathcal{V}_{1}+\mathcal{V}_{2}\)</span>, we need to show that <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> are disjoint. Suppose that <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> are not disjoint, then there exists <span class="math inline">\(g\neq0\)</span> in <span class="math inline">\(\mathcal{V}_{1}\cap\mathcal{V}_{2}\)</span>. Then for any <span class="math inline">\(f=f_{1}+f_{2}\)</span>, we have
<span class="math display">\[f=(f_{1}+cg)+(f_{2}-cg)\]</span>
for any scalar <span class="math inline">\(c\)</span>. But then <span class="math inline">\(f\)</span> is not uniquely determined, a contradiction.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Let’s consider three examples of “summing” the subspaces or “decomposing” a space.</p>
<p><span class="newthought">Range space and null space </span></p>
<p>For a non-square matrix <span class="math inline">\(\mathbf{A}\in\mathbb{F}^{n\times m}\)</span>, the <em>column space</em> (or called <em>range space</em> when we consider the operator rather than the matrix) of <span class="math inline">\(\mathbf{A}\)</span> is defined by:<label for="tufte-sn-442" class="margin-toggle sidenote-number">442</label><input type="checkbox" id="tufte-sn-442" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">442</span> Notice that the <strong>column/range space</strong> <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span> is the <a href="sub-set-theory.html#sub:func">image</a> of the linear transformation <span class="math inline">\(\mathbf{A}\)</span>.</span>
<span class="math display">\[\mbox{Range}(\mathbf{A}):=\left\{ \mathbf{y}\in\mathbb{F}^{n}:\,\mathbf{y}=\mathbf{A}\mathbf{x},\mathbf{x}\in\mathbb{F}^{m},\mathbf{A}\in\mathbb{F}^{n\times m}\right\}.\]</span>
The solution of the system <span class="math inline">\(\mathbf{b}=\mathbf{A}\mathbf{x}\)</span> exists only if <span class="math inline">\(\mathbf{b}\)</span> is in <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span>, i.e., <span class="math inline">\(\mathbf{b}\in\mbox{Range}(\mathbf{A})\)</span>. The <strong>column space</strong> <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span> contains all the <a href="ch-vecMat.html#sub:vec">linear combinations</a> of the columns of <span class="math inline">\(\mathbf{A}\)</span>. It is a <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> of <span class="math inline">\(\mathbb{F}^{n}\)</span>.</p>
<p>The <em>null space</em> is <a href="ch-vecMat.html#sub:vec">orthogonal</a> to <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span><label for="tufte-sn-443" class="margin-toggle sidenote-number">443</label><input type="checkbox" id="tufte-sn-443" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">443</span> To see the orthogonality, for any <span class="math inline">\(\mathbf{x}\in\mbox{Range}(\mathbf{A})\)</span>, there is
<span class="math display">\[\left\langle \mathbf{A}\mathbf{x},\,\mathbf{y}\right\rangle =\mathbf{x}^{\mbox{H}}\mathbf{A}^{\mbox{H}}\mathbf{y}=0\]</span> for any <span class="math inline">\(\mathbf{y}\in\mbox{Null}(\mathbf{A}^{\mbox{H}})\)</span>.</span>
<span class="math display">\[\mbox{Null}(\mathbf{A}^{\mbox{H}}):=\left\{ \mathbf{y}\in\mathbb{F}^{n}:\mathbf{A}^{\mbox{H}}\mathbf{y}=0,\,\mathbf{A}^{\mbox{H}}\in\mathbb{F}^{m\times n}\right\}\]</span> The <strong>null space</strong> of <span class="math inline">\(\mathbf{A}^{\mbox{H}}\)</span> consists of all solutions <span class="math inline">\(\mathbf{A}^{\mbox{H}}\mathbf{y}=0\)</span>, it is also a <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> of <span class="math inline">\(\mathbb{F}^{n}\)</span>.</p>
<p>The <strong>null space</strong> and <strong>column space</strong> are <strong>orthogonal complements</strong> such that:
<span class="math display">\[\mbox{Range}(\mathbf{A})\oplus\mbox{Null}(\mathbf{A}^{\mbox{H}})=\mathbb{F}^{n}.\]</span>
This result is a generalization of the <a href="ch-optApp.html#sub:Proj1">Farkas’ lemma</a>. Basically it reveals a dichotomy about the sandwich form <span class="math inline">\(\left\langle \mathbf{x},\,\mathbf{A}^{\mbox{H}}\mathbf{y}\right\rangle\)</span> for a given system <span class="math inline">\(\mathbf{y}=\mathbf{A}\mathbf{x}\)</span>: either the sandwich form is zero or non-zero.<label for="tufte-sn-444" class="margin-toggle sidenote-number">444</label><input type="checkbox" id="tufte-sn-444" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">444</span> That is, for any <span class="math inline">\(\mathbf{x},\mathbf{y}\in\mathbb{F}^{n}\)</span>, either <span class="math inline">\(\left\langle \mathbf{x},\,\mathbf{A}^{\mbox{H}}\mathbf{y}\right\rangle =0\)</span> or
<span class="math display">\[\begin{align*}\left\langle \mathbf{x},\,\mathbf{A}^{\mbox{H}}\mathbf{y}\right\rangle &amp;=\left\langle \mathbf{A}\mathbf{x},\,\mathbf{y}\right\rangle\\
&amp;=\left\langle \mathbf{y},\,\mathbf{y}\right\rangle =\|\mathbf{y}\|^{2}\neq0.\end{align*}\]</span></span> The dichotomy is also known as the <em>Fredholm alternative</em>.</p>
<div class="solution">
<p class="solution-begin">
Fredholm alternative <span id="sol-start-206" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-206', 'sol-start-206')"></span>
</p>
<div id="sol-body-206" class="solution-body" style="display: none;">
<p>The <strong>Fredholm alternative</strong> states as follows:</p>
<p>For any <span class="math inline">\(\mathbf{y}\in\mathbb{R}^{n}\)</span> such that <span class="math inline">\(\mathbf{A}^{\top}\mathbf{y}=0\)</span>, then <span class="math inline">\(\mathbf{b}=\mathbf{A}\mathbf{x}\)</span> has a solution if and only if
<span class="math display">\[\langle\mathbf{A}\mathbf{x},\mathbf{y}\rangle=\langle\mathbf{b},\mathbf{y}\rangle=0.\]</span></p>
<p>The statement can be summarized as: the system <span class="math inline">\(\mathbf{b}=\mathbf{A}\mathbf{x}\)</span> has a solution (<span class="math inline">\(\mathbf{b}\in\mbox{Range}(\mathbf{A})\)</span> if and only if <span class="math inline">\(\langle\mathbf{b},\mathbf{y}\rangle=0\)</span> for any <span class="math inline">\(\mathbf{y}\in \mbox{Null}(\mathbf{A}^{\top})\)</span> (<span class="math inline">\(\mathbf{b}\in(\mbox{Null}(\mathbf{A}^{\top}))^{\bot}\)</span>).</p>
<p>Notice that <span class="math inline">\(\mbox{Range}(\mathbf{A})=(\mbox{Null}(\mathbf{A}^{\top}))^{\bot}\)</span>.</p>
<p>Thus, the <strong>Fredholm alternative</strong> essentially tells the dichotomy: <span class="math inline">\(\mathbf{b}\in\mbox{Range}(\mathbf{A})\)</span> (has a solution) or <span class="math inline">\(\mathbf{b}\in\mbox{Null}(\mathbf{A}^{\top})\)</span> (no solution).</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>The <strong>complementary subspaces</strong> tell that the “dimension” of the <strong>column space</strong> is not only a property of <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span> but also the property of <span class="math inline">\(\mbox{Null}(\mathbf{A}^{\mbox{H}})\)</span>.<label for="tufte-sn-445" class="margin-toggle sidenote-number">445</label><input type="checkbox" id="tufte-sn-445" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">445</span> The dimension of the space refers to the cardinal number of the <a href="ch-MatComp.html#sub:vecSpaces">basis</a> of that space.</span> That is, if the “dimension” of <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span> grows, then the “size” of <span class="math inline">\(\mbox{Null}(\mathbf{A}^{\mbox{H}})\)</span> will shrink, and vice versa. In the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a>, the zero mean <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathcal{H}\)</span>-valued random variables</a> belong to the <strong>null space</strong> of a <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a>. It turns out that the size of this <strong>null space</strong> is only one dimensional lower than the (non-randomized) <span class="math inline">\(\mathcal{H}\)</span>.<label for="tufte-sn-446" class="margin-toggle sidenote-number">446</label><input type="checkbox" id="tufte-sn-446" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">446</span> For a <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathcal{H}\)</span>-valued random variable</a>, its expectation can be thought of as a <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a>, <span class="math display">\[\begin{align*}\mathbb{E}[X(\omega)]=\int_{\Omega}x\mathbb{P}(\mbox{d}x)\\=\int_{\Omega}xf(x)\mbox{d}x=\langle x,f\rangle\end{align*}\]</span>
where <span class="math inline">\(\mathbb{P}(\mbox{d}x)=f(x)\mbox{d}x\)</span> for the density function <span class="math inline">\(f(\cdot)\)</span>. For a non-trivial <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a> <span class="math inline">\(\langle \cdot,f\rangle\)</span>, i.e., <span class="math inline">\(f\neq0\)</span>, the <strong>null space</strong> <span class="math display">\[\mbox{Null}(f)=\{x\in\mathcal{H}:\langle x,f\rangle=0\}\]</span> is a subspace of <span class="math inline">\(\mathcal{H}\)</span>. In fact, the space <span class="math inline">\(\mbox{Null}(f)\)</span> is so large that it is the “maximal” subspace of <span class="math inline">\(\mathcal{H}\)</span>, i.e., one dimensional lower than <span class="math inline">\(\mathcal{H}\)</span>.</span></p>
<div class="solution">
<p class="solution-begin">
Remarks about the null space of a linear functional <span id="sol-start-207" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-207', 'sol-start-207')"></span>
</p>
<div id="sol-body-207" class="solution-body" style="display: none;">
<p>Consider the null space of a linear functional <span class="math inline">\(g:\mathcal{V}\rightarrow\mathbb{F}\)</span> <span class="math display">\[\mbox{Null}(g):=\left\{ \langle f,\, g\rangle=0,\; f\in\mathcal{V}\right\}.\]</span></p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\mbox{Null}(g)\)</span> is one dimensional lower than the original space <span class="math inline">\(\mathcal{V}\)</span>.</li>
</ol>
<p>Proof: To see that, consider a point <span class="math inline">\(f_{0}\)</span> in <span class="math inline">\(\mathcal{V}\)</span> such that <span class="math inline">\(\langle f_{0},\, g\rangle\neq0\)</span>. Then for any point <span class="math inline">\(f\in\mathcal{V}\)</span>,
we have <span class="math display">\[f_{p}=f-\frac{\langle f,\, g\rangle}{\langle f_{0},\, g\rangle}f_{0}\]</span> where <span class="math inline">\(\langle f_{p},\, g\rangle=0\)</span> because <span class="math display">\[\langle f_{p},g\rangle=\langle f,g\rangle-\frac{\langle f,\, g\rangle}{\langle f_{0},\, g\rangle}\langle f_{0},g\rangle=0.\]</span>
That means each point <span class="math inline">\(f\in\mathcal{V}\)</span> can be expressed as the <strong>direct sum</strong> of a point in <span class="math inline">\(\mbox{Null}(g)\)</span> and a point in the one-dimensional subpsace spanned by <span class="math inline">\(f_{0}\)</span>. Since <span class="math inline">\(\mbox{span}(f_{0})\cap\mbox{ker}(g)=\emptyset\)</span>, we have <span class="math display">\[\mbox{span}(f_{0})\oplus\mbox{ker}(g)=\mathcal{V}.\]</span>
The result follows.</p>
<ol start="2" style="list-style-type: decimal">
<li><p>The inverse of the above statement is also true. That is, if a subspace whose dimension of the <strong>complement</strong> is only one, then we can use a linear functional <span class="math inline">\(g\)</span> to characterize this subspace so that the <strong>orthogonal complement</strong> subspace is equivalent to <span class="math inline">\(\mbox{Null}(g)\)</span>.</p></li>
<li><p>In chapter @ref(#sub:Proj1), we have seen that a <a href="ch-optApp.html#sub:Proj1">hyperlane</a> or (a regression) is one dimensional lower than the original space. In fact, a <a href="ch-optApp.html#sub:Proj1">hyperplane</a> can be viewed as a translated subspace of the <strong>null space</strong> of the linear functional, i.e. <span class="math inline">\(\mbox{Null}(g)\)</span>. Consider the hyperplane <span class="math inline">\(\mathcal{S}(g,c)=\{f\in\mathcal{V}:\,\langle f,g\rangle=c\}\)</span> and let <span class="math inline">\(f_{0}\)</span> be an arbitrary point in <span class="math inline">\(\mathcal{S}\)</span>
. We construct another hyperplane
<span class="math display">\[\mathcal{M}(g,c)=\{f\in\mathcal{V}:\, f+f_{0}\in\mathcal{S}(g,c)\}.\]</span>
This hyperplane is the <strong>null space</strong> of the linear functional <span class="math inline">\(g\)</span>, because <span class="math inline">\(\langle f+f_{0},g\rangle=c\)</span> in <span class="math inline">\(\mathcal{M}(g,c)\)</span> and <span class="math inline">\(\langle f_{0},g\rangle=c\)</span> in <span class="math inline">\(\mathcal{S}(g,c)\)</span>. So for any <span class="math inline">\(f\in \mathcal{M}(g,c)\)</span>, we have <span class="math display">\[\langle f_{0},g\rangle=0.\]</span> That means <span class="math inline">\(\mathcal{M}(g,c)=\mbox{Null}(g)\)</span>.</p></li>
</ol>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p><span class="newthought">Future and past space </span></p>
<p>Let’s look at another example. Consider a vector space linearly generated by <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathcal{H}\)</span>-valued time series</a> <span class="math inline">\(X_{t}(\omega)\)</span> such that
<span class="math display">\[\mbox{span}\left\{ X_{t}(\omega)\right\} =\left\{ \sum_{t}\phi(t)X_{t}(\omega),\,\phi\in\mathcal{H} \right\}.\]</span>
By <em>closing</em> this space, i.e., adding all <a href="ch-randomization.html#sub:RHilbert">mean square limits</a> of all sequences of <span class="math inline">\(\sum_{t}\phi(t)X_{t}(\omega)\)</span>, we will obtain a <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a> generated by the series <span class="math inline">\(X_{t}(\omega)\)</span>, denoted by <span class="math display">\[\mathcal{H}_{X(\omega)}=\overline{\mbox{span}}\left\{ \left.X_{t}(\omega)\right|t\in(-\infty,\infty)\right\},\]</span>
where <span class="math inline">\(\overline{\mbox{span}}\)</span> says that the <a href="">span</a> is <strong>closed</strong>.<label for="tufte-sn-447" class="margin-toggle sidenote-number">447</label><input type="checkbox" id="tufte-sn-447" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">447</span> The extension of the span to the continuous time requires more technicalities and is beyond our current concerns.</span></p>
<p>The space <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span> can be decomposed into two subspaces of interests. They are representing the past and the future space generated by <span class="math inline">\(X_t(\omega)\)</span> respectively:<label for="tufte-sn-448" class="margin-toggle sidenote-number">448</label><input type="checkbox" id="tufte-sn-448" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">448</span> The convention is to include the present in the future only and not in the past.</span>
<span class="math display">\[\mathcal{H}_{X(\omega)}^{t-}=\overline{\mbox{span}}\left\{ \left.X_{s}(\omega)\right|s&lt;t\right\} ,\;\mathcal{H}_{X(\omega)}^{t+}=\overline{\mbox{span}}\left\{ \left.X_{s}(\omega)\right|s\geq t\right\}.\]</span>
Combing the past and the future gives us the whole space, namely <span class="math inline">\(\mathcal{H}_{X(\omega)}=\mathcal{H}_{X(\omega)}^{t-}+\mathcal{H}_{X(\omega)}^{t+}\)</span>. In particular, if the process <span class="math inline">\(X_t(\omega)\)</span> is <a href="ch-UnMulti.html#sub:Markov">Markovian</a>, then the future and the past are orthogonal given the present, which means given <span class="math inline">\(X_{t}(\omega)\)</span>,<label for="tufte-sn-449" class="margin-toggle sidenote-number">449</label><input type="checkbox" id="tufte-sn-449" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">449</span> The orthogonality can be justified by the conditional expectation. We will discuss it in sec[?].</span> <span class="math display">\[\mathcal{H}_{X(\omega)}=\mathcal{H}_{X(\omega)}^{t-}\oplus \mathcal{H}_{X(\omega)}^{t+}.\]</span></p>
<p><span class="newthought">Decomposition by the projection </span></p>
<p>The third example is about decomposing the space by the <a href="ch-representation.html#sub:conjugacy">projection operator</a>. Let <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}:\mathcal{H}\rightarrow \mathcal{Y}\)</span> be the <a href="ch-representation.html#sub:conjugacy">projection operator</a> defined on the Hilbert space <span class="math inline">\(\mathcal{H}\)</span>, such that <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}\mathbf{P}_{\mathcal{Y}}=\mathbf{P}_{\mathcal{Y}}\)</span>. By the property of the projection, it is straightforward to see that <span class="math inline">\(\mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span> and <span class="math inline">\(\mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span> are <strong>disjoint subspaces</strong> of <span class="math inline">\(\mathcal{H}\)</span> such that
<span class="math display">\[\mathcal{H} = \mbox{Range}(\mathbf{P}_{\mathcal{Y}}) + \mbox{Null}(\mathbf{P}_{\mathcal{Y}})=\mathcal{Y} + \mbox{Null}(\mathbf{P}_{\mathcal{Y}}).\]</span> In addition, if the projection is <a href="ch-vecMat.html#sub:vec">orthogonal</a>, then
<span class="math display">\[\mathcal{H} = \mbox{Range}(\mathbf{P})_{\mathcal{Y}} \oplus \mbox{Null}(\mathbf{P}_{\mathcal{Y}})=\mathcal{Y} \oplus \mathcal{Y}^{\bot},\]</span> and <span class="math inline">\(\|f\|^2 = \|f_1\|^2 +\|f_2\|^2\)</span> for <span class="math inline">\(f\in \mathcal{H}\)</span>, <span class="math inline">\(f_1\in \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>, <span class="math inline">\(f_2 \in \mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span>.</p>
<p>In other words, one <a href="ch-representation.html#sub:conjugacy">projection</a> can uniquely decompose the Hilbert space into two <strong>disjoint subspaces</strong>. The <strong>range space</strong> of the projection operator is simply the target subspace (<span class="math inline">\(\mathcal{Y}\)</span>) of the projection. Given a target space, there could be many projections. If we want the <strong>null space</strong> of the projection to be <a href="ch-vecMat.html#sub:vec">orthogonal</a> to the <strong>range space</strong>, then we need the <a href="ch-representation.html#sub:conjugacy">orthogonal projection</a>. Because the <strong>decomposition</strong> is unique, we expect that for any target subspace, there is one and only one <a href="ch-representation.html#sub:conjugacy">orthogonal projection</a>.<label for="tufte-sn-450" class="margin-toggle sidenote-number">450</label><input type="checkbox" id="tufte-sn-450" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">450</span> In fact, if <span class="math inline">\(\mathcal{A}\)</span> is any <strong>closed</strong> subspace of a Hilbert space <span class="math inline">\(\mathcal{H}\)</span>. Then <span class="math inline">\(\mathcal{H}=\mathcal{A}\oplus\mathcal{A}^{\bot}\)</span>. The proof is given below.</span></p>
<div class="solution">
<p class="solution-begin">
Proof <span id="sol-start-208" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-208', 'sol-start-208')"></span>
</p>
<div id="sol-body-208" class="solution-body" style="display: none;">
<p>Let <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}:\mathcal{H}\rightarrow \mathcal{Y}\)</span> be the <a href="ch-representation.html#sub:conjugacy">projection operator</a> defined on the Hilbert space <span class="math inline">\(\mathcal{H}\)</span>, such that <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}\mathbf{P}_{\mathcal{Y}}=\mathbf{P}_{\mathcal{Y}}\)</span>.</p>
<p>To show that <span class="math inline">\(\mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span> and <span class="math inline">\(\mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>, are <strong>disjoint subspaces</strong>, let <span class="math inline">\(f\in \mbox{Null}(\mathbf{P}_{\mathcal{Y}}) \cap \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>. Because <span class="math inline">\(f\in \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span> then there must be some <span class="math inline">\(g \in \mathcal{H}\)</span> such that <span class="math inline">\(\mathbf{P}_{\mathcal{Y}} (g) =f\)</span>. By the projection property, we know that <span class="math display">\[\mathbf{P}_{\mathcal{Y}}\mathbf{P}_{\mathcal{Y}} (g) = \mathbf{P}_{\mathcal{Y}} f =f.\]</span> Also, because <span class="math inline">\(f\in \mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span>, <span class="math inline">\(\mathbf{P}_{\mathcal{Y}} f=0\)</span>. That tells <span class="math inline">\(f=\mathbf{P}_{\mathcal{Y}} f=0\)</span>. Since <span class="math inline">\(f\)</span> is any object in <span class="math inline">\(\mbox{Null}(\mathbf{P}_{\mathcal{Y}}) \cap \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>, we have <span class="math inline">\(\mbox{Null}(\mathbf{P}_{\mathcal{Y}}) \cap \mbox{Range}(\mathbf{P}_{\mathcal{Y}})=\{0\}\)</span>.</p>
<p>Now consider the orthogonal projection.
Firstly, note that <span class="math inline">\(\mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span> and <span class="math inline">\(\mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span> are <strong>null spaces</strong> of <span class="math inline">\(\mathbf{I}-\mathbf{P}_{\mathcal{Y}}\)</span> and <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}\)</span> where <span class="math inline">\(\mathbf{I}\)</span> is an identical operator/matrix. That says
<span class="math display">\[\langle \mathbf{P}_{\mathcal{Y}}, \mathbf{I}-\mathbf{P}_{\mathcal{Y}}\rangle =0.\]</span> Thus<br><span class="math display">\[\mbox{Range}(\mathbf{P}_{\mathcal{Y}}) = \mbox{Null}(\mathbf{I}-\mathbf{P}_{\mathcal{Y}}) = \mbox{Null}(\mathbf{P}_{\mathcal{Y}})^{\bot}.\]</span>
Secondly, note that any <span class="math inline">\(f\in\mathcal{H}\)</span> can be uniquely decompose as <span class="math display">\[f=f_1 + f_2\]</span> for <span class="math inline">\(f_1\in \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>, <span class="math inline">\(f_2 \in \mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span>. Since <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}\)</span> is orthogonal, we have <span class="math inline">\(\langle f_1, f_2\rangle =0\)</span> for any <span class="math inline">\(f_1\in \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>, <span class="math inline">\(f_2 \in \mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span>. By Pythagorean theorem, we know that <span class="math inline">\(\|f\|^2 = \|f_1\|^2 +\|f_2\|^2\)</span>. The result follows.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Given the identical and independent property of the white noises, one should expect that the spaces of white noises share some invariant properties.</p>
<p>Recall that in the deterministic setting, stationarity refers to some equilibrium status, in which the dynamics remain invariant. The dynamics of the randomized models should be described by the probability laws. For <a href="ch-UnMulti.html#sub:Markov">Markov processes</a>, we have seen the invariance probability law is about the unchange of <a href="ch-UnMulti.html#sub:Markov">transition probabilities</a>. While for the <a href="ch-randomization.html#sub:RHilbert">second-order processes</a>, since the probability law is reduced to the first- and the second-order information, we can have define a new kind of invariance in a wider sense, called the <strong>(weak or wide sense) stationarity</strong>.</p>
<ul>
<li>
<strong>(weak or wide sense) Stationarity</strong> : A <a href="ch-randomization.html#sub:RHilbert">second-order process</a> <span class="math inline">\(X(t,\omega)\)</span> is <em>weakly stationary</em> if its <a href="ch-randomization.html#sub:RHilbert">covariance function</a> satisfies <span class="math display">\[\mbox{Cov}_X (s+t,s)= \mbox{Cov}_X (t,0),\]</span> for any <span class="math inline">\(s,t\in\mathbb{R}\)</span>, and its <a href="ch-randomization.html#sub:RHilbert">mean function</a> <span class="math inline">\(\mu_X(t) = \mu_X\)</span> where <span class="math inline">\(\mu_X\)</span> is a constant independent of time.</li>
</ul>
<p>The above defintion of the <a href="ch-randomization.html#sub:RHilbert">covariance function</a> says that only the difference between function’s two arguments matters, namely <span class="math display">\[\mbox{Cov}_X (t_1,t_2)= \mbox{Cov}_X (t_1 - t_2).\]</span></p>
<p>Another way to understand the definition is to see that, for a <strong>weak stationary</strong> process, if one shifts the process by <span class="math inline">\(s\)</span>-unit of time, the first- and the second-order structures of the process remain invariant.
Recall that shifting the process backward is conducted by the <a href="ch-eigen.html#sub:matNorms">lagged operator</a> <span class="math inline">\(\mathbf{L}\)</span>. For a <strong>weak stationary</strong> time series <span class="math inline">\(X(\omega)_{t}\)</span>, we have the following equalities in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a><label for="tufte-sn-451" class="margin-toggle sidenote-number">451</label><input type="checkbox" id="tufte-sn-451" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">451</span> Note that the covariance function can be written in terms of an inner product in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a> <span class="math display">\[\begin{align*}\mathbb{E}[(X_{t}(\omega)-\mu_{X})(X_{t-1}(\omega)-\mu_{X})]\\=\langle X_{t}(\omega)-\mu_{X},X_{t-1}(\omega)-\mu_{X}\rangle_{\mathbb{P}}.\end{align*}\]</span>
For the <a href="ch-eigen.html#sub:matNorms">lagged operator</a>, <span class="math display">\[\mathbf{L}(X_{t}(\omega) - \mu_{X})=X_{t-1}(\omega)-\mu_X\]</span> because <span class="math inline">\(\mu_X\)</span> is a constant independent of time.</span>:
<span class="math display">\[\begin{align*}\mbox{Cov}_X (t,t-1) &amp;= \langle X_{t}(\omega) - \mu_{X},X_{t-1}(\omega)-\mu_{X}\rangle_{\mathbb{P}} \\
&amp;=\langle X_{t}(\omega) - \mu_{X},\mathbf{L}(X_{t}(\omega) - \mu_{X})\rangle_{\mathbb{P}}\\
&amp;=\langle\mathbf{L}^{\mbox{H}}(X_{t}(\omega)-\mu_X),X_{t}(\omega)-\mu_X\rangle_{\mathbb{P}}\\
&amp;=\langle X_{t+1}(\omega)-\mu_X,X_{t}(\omega)-\mu_{X}\rangle_{\mathbb{P}}\\
&amp;= \mbox{Cov}_X (t+1,t) \end{align*}\]</span>
where <span class="math inline">\(\mathbf{L}^{\mbox{H}}\)</span> is the <a href="ch-representation.html#sub:conjugacy">adjoint operator</a> of <span class="math inline">\(\mathbf{L}\)</span>, meaning shifting forward.</p>
<p>In other words, the <strong>weak stationarity</strong> is about endowing the lagged operator <span class="math inline">\(\mathbf{L}\)</span> the <a href="ch-representation.html#sub:dualBasis">unitary</a> property, as the operators perserve the value of the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathbb{P}\)</span>-inner product</a> (called <em>isometry</em>)<label for="tufte-sn-452" class="margin-toggle sidenote-number">452</label><input type="checkbox" id="tufte-sn-452" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">452</span> By the equality
<span class="math display">\[\mbox{Cov}_X (t,s)= \mathbb{E}[X_t(\omega)X_s(\omega)] -\mu_{X}^2,\]</span> we know that only the term <span class="math display">\[\mathbb{E}[X_t(\omega)X_s(\omega)]=\langle X_{t}(\omega),X_{s}(\omega)\rangle_{\mathbb{P}}\]</span> influences the covariance function. So we just need to consider <span class="math display">\[\langle X_{t}(\omega),X_{t-1}(\omega)\rangle_{\mathbb{P}}=\langle X_{t+1}(\omega),X_{t}(\omega)\rangle_{\mathbb{P}}.\]</span></span>
<span class="math display">\[\begin{align*}\langle\mathbf{L}X_{t+1}(\omega),\mathbf{L}X_{t}(\omega)\rangle_{\mathbb{P}}&amp;=\langle X_{t}(\omega),X_{t-1}(\omega)\rangle_{\mathbb{P}}=\\\langle X_{t+1}(\omega),\mathbf{L}^{\mbox{H}}\mathbf{L}X_{t}(\omega)\rangle_{\mathbb{P}}&amp;=\langle X_{t+1}(\omega),X_{t}(\omega)\rangle_{\mathbb{P}},\end{align*}\]</span>
and <span class="math inline">\(\mathbf{L}^{\mbox{H}}\mathbf{L}=\mathbf{I}\)</span> or say <span class="math inline">\(\mathbf{L}^{\mbox{H}}=\mathbf{L}^{-1}\)</span>.</p>
<p>For an <a href="ch-MatComp.html#sub:vecSpaces">operator</a> <span class="math inline">\(\mathbf{T}\)</span>, we say that the operator induces an <em>invariant subspace</em> <span class="math inline">\(\mathcal{V}\)</span>, if <span class="math inline">\(\mathbf{T}(\mathcal{V})\subset\mathcal{V}\)</span>. Therefore, the <strong>weak stationary</strong> is to empower the <a href="ch-eigen.html#sub:matNorms">lagged operator</a> <span class="math inline">\(\mathbf{L}\)</span> and its <a href="ch-representation.html#sub:conjugacy">adjoint</a> <span class="math inline">\(\mathbf{L}^{\mbox{H}}\)</span> to induce the <strong>invariant subspaces</strong> of some <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a> generated by the process <span class="math inline">\(X(t,\omega)\)</span> such that<label for="tufte-sn-453" class="margin-toggle sidenote-number">453</label><input type="checkbox" id="tufte-sn-453" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">453</span> The Hilbert space <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span> generated by a <strong>weak stationary</strong> stochastic process <span class="math inline">\(X(t,\omega)\)</span> is to be generated by the <a href="ch-representation.html#sub:dualBasis">unitary</a> lagged operator <span class="math inline">\(\mathbf{L}\)</span> in the following sense: <span class="math display">\[\mathcal{H}_{X(\omega)}=\overline{\mbox{span}}\{\mathbf{L}^{t}X(t,\omega)| t\in\mathbb{Z}\}.\]</span></span>
<span class="math display">\[\mathbf{L}\mathcal{H}_{X(\omega)}^{t-}=\mathcal{H}_{X(\omega)}^{t-},\;\mathbf{L}^{\mbox{H}}\mathcal{H}_{X(\omega)}^{t+}=\mathcal{H}_{X(\omega)}^{t+},\]</span>
where <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t-}\)</span> and <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t+}\)</span> represent the past and the future respectively, and <span class="math inline">\(\mathcal{H}_{X(\omega)}= \mathcal{H}_{X(\omega)}^{t-}+\mathcal{H}_{X(\omega)}^{t+}\)</span>.</p>
<p>With the concepts of <strong>direct sum</strong> (<strong>decomposition</strong>) and <strong>weak stationarity</strong>, we can analyze the <a href="ch-UnMulti.html#sub:rw">random walk</a> model <span class="math inline">\(X_{t}(\omega)=\sum_{i=1}^{t}\varepsilon_{i}(\omega)\)</span> from a different perspective. Let <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t}\)</span> and <span class="math inline">\(\mathcal{H}_{\varepsilon(\omega)}^{t}\)</span> be the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-spaces</a> generated by the <span class="math inline">\(X_t(\omega)\)</span> and <span class="math inline">\(\varepsilon_t(\omega)\)</span> at time <span class="math inline">\(t\)</span>. Because Gaussian white noises <span class="math inline">\(\{\varepsilon_{i}(\omega)\}_{i=1}^{t}\)</span>
are from <em>disjoint orthogonal subspaces</em> <span class="math inline">\(\mathcal{H}^{t}_{\varepsilon(\omega)},\dots,\mathcal{H}^{1}_{\varepsilon(\omega)}\)</span>, the <strong>direct sum</strong> <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t}\)</span> can be <strong>orthogonally decomposed</strong> as
<span class="math display">\[\begin{align*}\mathcal{H}_{X(\omega)}^{t}&amp;=\mathcal{H}^{t}_{\varepsilon(\omega)}\oplus\mathcal{H}^{t-1}_{\varepsilon(\omega)}\cdots\oplus\mathcal{H}^{1}_{\varepsilon(\omega)} = \oplus_{s=1}^{t}\mathcal{H}^{s}_{\varepsilon(\omega)} \\&amp;=
\mathcal{H}^{t}_{\varepsilon(\omega)}\oplus\mathbf{L}\mathcal{H}_{X(\omega)}^{t}.\end{align*}\]</span></p>
<p>The subspace <span class="math inline">\(\mathcal{H}^{t}_{\varepsilon(\omega)}\)</span>, as the <strong>orthogonal complement</strong> of lagged <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t}\)</span>, provides the gap information between <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t}\)</span> and its one step backward shifted <span class="math inline">\(\mathbf{L}\mathcal{H}_{X(\omega)}^{t}\)</span>. Such a subspace is called the <em>wandering subspace</em> if the shift/lagged operator induces <strong>invariant subspaces</strong>.</p>
<p>In short, the space the <strong>weakly stationary</strong>
random walks is decomposed of the <strong>orthogonal direct sums</strong> of <strong>wandering subspaces</strong> generated by the white noises.<label for="tufte-sn-454" class="margin-toggle sidenote-number">454</label><input type="checkbox" id="tufte-sn-454" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">454</span> The <a href="ch-UnMulti.html#sub:rw">random walk</a> is <strong>weakly stationary</strong>, as the covariance function <span class="math inline">\(\mbox{Cov}_X(t,s)=\sigma^2(t-s)\)</span> only depend on the difference of its arguments.</span>
Thus, we can view the space of random walks as the <strong>direct sum</strong> of many subspaces generated by the white noises. The sequence <span class="math inline">\(\{\varepsilon_{i}\}_{i=1}^{t}\)</span> is called the <em>white noise basis</em> of the <a href="ch-UnMulti.html#sub:rw">random walk</a> series.</p>
<p>This kind of the <strong>decomposition</strong> turns out to be valid for all <strong>weakly stationary processes</strong> with zero means in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a>, even if the space consists of infinitely many <strong>wandering subspaces</strong>.<label for="tufte-sn-455" class="margin-toggle sidenote-number">455</label><input type="checkbox" id="tufte-sn-455" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">455</span> The result is called the <em>Wold’s decomposition</em>. However, additional restrictions have to be imposed on the remote past and future subspaces of the processes. The remote past and future subspaces have to be trivial, namely <span class="math display">\[\lim_{t\rightarrow\pm\infty}\mathcal{H}_{X(\omega)}^{t}=0,\]</span> so that they provide zero information to all the other subspaces of the process. The <strong>Wold decomposition</strong> basically says that a <a href="ch-representation.html#sub:dualBasis">unitary operator</a> can induce a sequence of <strong>subspaces</strong> whose <strong>orthogonal direct sum</strong> is invariant under the operator.</span></p>
<p>By the definition of the <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> in chapter <a href="ch-MatComp.html#sub:vecSpaces">11.4</a>, we know that if some objects live in a subspace, any linear combination of these objects also belongs to this subspace. The collection of <strong>wandering subspaces</strong> induced by the <strong>white noise basis</strong> can be infinite dimensional. This motivates us to represent some infinite dimensional elements through a linear combination of infinite dimensional white noises basis (a <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a> of white noise process) in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-spaces</a>.<label for="tufte-sn-456" class="margin-toggle sidenote-number">456</label><input type="checkbox" id="tufte-sn-456" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">456</span> The previous discussion says that the <strong>null space</strong> of a <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a> can be the “maximal” subspace. One may expect that the representation is good enough to approximate almost all <strong>second-order</strong> processes. This is roughly the case, except that the <strong>mean function</strong> (the “missing” one dimension) has to be removed from the approximation. So we only consider zero mean processes over here.</span> Let <span class="math inline">\(\varepsilon(t,\omega)\)</span> be a <a href="ch-randomization.html#sub:RHilbert">Gaussian white noise process</a>. The linear functional <span class="math inline">\(f(\omega)\in\mathcal{H}_{f(\omega)}\)</span> of the <a href="ch-randomization.html#sub:RHilbert">white noise process</a> has the form
<span class="math display">\[f(\omega)=\sum_{s=-\infty}^{\infty}c(-s)\varepsilon(s,\omega),\;\mbox{where }\sum_{s=-\infty}^{\infty}|\phi(-s)|^{2}&lt;\infty\]</span>
where the deterministic function <span class="math inline">\(c(-s)\)</span> is in the <a href="sub-continuity.html#sub:continuousFunc"><span class="math inline">\(\ell_2\)</span>-space</a>, and plays a role as the <span class="math inline">\(s\)</span>-th <a href="ch-representation.html#sub:conjugacy">Fourier coefficient</a> of the functional <span class="math inline">\(f\)</span> with respect to the <strong>white noise basis</strong> <span class="math inline">\(\varepsilon(s,\omega)\)</span> such that <span class="math display">\[c(-t)=\left\langle f(\omega),\varepsilon(t,\omega)\right\rangle _{\mathbb{P}}=\mathbb{E}\left[f(\omega)\varepsilon(t,\omega)\right].\]</span>
As you can see, the function <span class="math inline">\(c(\cdot)\)</span> is analogous to the coefficient in the <a href="ch-representation.html#sub:conjugacy">abstract Fourier series</a> in chapter <a href="ch-representation.html#sub:conjugacy">14.3</a>. Therefore, we expect that these coefficient functions are unique and form a square summable sequence.<label for="tufte-sn-457" class="margin-toggle sidenote-number">457</label><input type="checkbox" id="tufte-sn-457" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">457</span> It is possible to derive the continuous version of this representation. But due to the difficulty of defining “proper” mean square integral of white noises, it is better to construct the functionals in terms of <a href="ch-randomization.html#sub:RHilbert">mean square differential</a> of a Brownian motion <span class="math inline">\(B(t,\omega)\)</span>, i.e., <span class="math display">\[f(\omega)=\int_{-\infty}^{\infty}c(-s)\mbox{d}B(t,\omega)\]</span>
where <span class="math inline">\(\mbox{d}B(t,\omega)=\varepsilon(t,\omega)\mbox{d}t\)</span> according to the previous definition of the white nose.</span></p>
<p>Recall that given <span class="math inline">\(\omega\)</span>, a stochastic process <span class="math inline">\(X(t,\omega)\)</span> is a function (path) over time <span class="math inline">\(t\)</span>.<label for="tufte-sn-458" class="margin-toggle sidenote-number">458</label><input type="checkbox" id="tufte-sn-458" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">458</span> Given <span class="math inline">\(\omega\)</span>, <span class="math inline">\(X(\cdot, \omega):\mathcal{V}\rightarrow\mathbb{F}\)</span> is a <a href="ch-MatComp.html#sub:vecSpaces">functional</a>.</span> It would be natural think about using the previous decomposition for representing a stochastic processes. Because all white noises have zero means, <span class="math inline">\(\mathbb{E}[\varepsilon(t,\omega)]=0\)</span>, the inner product of <span class="math inline">\(X(t,\omega)\)</span> and <span class="math inline">\(\varepsilon(t,\omega)\)</span> actually gives the <a href="ch-randomization.html#sub:RHilbert">covariance function</a>.<label for="tufte-sn-459" class="margin-toggle sidenote-number">459</label><input type="checkbox" id="tufte-sn-459" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">459</span> Note that
<span class="math display">\[\begin{align*}\mbox{Cov}_{X,\varepsilon}(t,s)=&amp;\\\mathbb{E}\left[X(t,\omega)\varepsilon(s,\omega)\right]-&amp;\mathbb{E}[X(t,\omega)]\mathbb{E}[\varepsilon(s,\omega)]\\=&amp;\left\langle X(t,\omega),\varepsilon(s,\omega)\right\rangle _{\mathbb{P}}.\end{align*}\]</span></span>
So we have the following representation <span class="math display">\[X(t,\omega)=\sum_{s=-\infty}^{\infty}\mbox{Cov}_{X,\varepsilon}(t,s)\varepsilon(s,\omega).\]</span>
In particular, if the <a href="ch-randomization.html#sub:RHilbert">second order process</a> <span class="math inline">\(X(t,\omega)\)</span> is <strong>weakly stationary</strong>,
the <a href="ch-representation.html#sub:dualBasis">duality</a> between coefficient function <span class="math inline">\(c(s)=\mbox{Cov}_{X,\omega}(t,s)\)</span> and the <strong>white noise basis</strong> becomes clear. Because when we shift the all the white noises by <span class="math inline">\(\tau\)</span>-period backward, e.g., <span class="math inline">\(\varepsilon(s-\tau,\omega)=\mathbf{L}^{\tau}\varepsilon(s,\omega)\)</span>, the corresponding coefficient needs to be shifted <span class="math inline">\(\tau\)</span>-period forward:
<span class="math display">\[\begin{align*}
\mbox{Cov}_{X,\omega}(t,s-\tau)&amp;=\left\langle X(t,\omega),\varepsilon(s-\tau)\right\rangle _{\mathbb{P}}=\left\langle X(t,\omega),\mathbf{L}^{\tau}\varepsilon(s)\right\rangle _{\mathbb{P}}\\
&amp;=\left\langle \mathbf{L}^{-\tau}X(t,\omega),\varepsilon(s)\right\rangle _{\mathbb{P}}=\mbox{Cov}_{X,\omega}(t+\tau,s)\\
&amp;=\mathbf{L}^{-\tau}c(s).
\end{align*}\]</span>
Then the representation is simplified to<label for="tufte-sn-460" class="margin-toggle sidenote-number">460</label><input type="checkbox" id="tufte-sn-460" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">460</span> This is the exact formula for the <strong>Wold decomposition</strong> of the <strong>weakly stationary</strong> process <span class="math inline">\(X(t,\omega)\)</span>.</span><br><span class="math display">\[X(t,\omega)=\sum_{s=-\infty}^{\infty}\mbox{Cov}_{X,\varepsilon}(t-s)\varepsilon(s,\omega)\]</span> where the <a href="ch-randomization.html#sub:RHilbert">covariance function</a> only has one argument.</p>
</div>
</div></body></html>

<p style="text-align: center;">
<a href="ch-optApp.html"><button class="btn btn-default">Previous</button></a>
<a href="bibliography.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2021-03-10
</p>
</div>
</div>



</body>
</html>
