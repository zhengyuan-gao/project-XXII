<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="16 Randomization | Project XXII" />
<meta property="og:type" content="book" />





<meta name="author" content="Zhengyuan Gao" />

<meta name="date" content="2021-03-31" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="16 Randomization | Project XXII">

<title>16 Randomization | Project XXII</title>

<link href="libs/tufte-css/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding/datatables.js"></script>
<link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>



</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Project XXII<p><p class="author">Zhengyuan Gao</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BA; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Preface</a>
<a href="part-i-inference-based-upon-logic-and-logical-computation.html">PART I: Inference Based upon Logic and Logical Computation</a>
<a href="sub-logic.html"><span class="toc-section-number">1</span> Logic</a>
<a href="sub-set-theory.html"><span class="toc-section-number">2</span> Set Theory</a>
<a href="sub-axioms.html"><span class="toc-section-number">3</span> Axioms</a>
<a href="sub-inferknow.html"><span class="toc-section-number">4</span> Inference and Knowledge</a>
<a href="sub-incomplete.html"><span class="toc-section-number">5</span> Incompleteness</a>
<a href="part-ii-infinitesimal-changes-and-their-consequences.html">PART II: Infinitesimal Changes and their Consequences</a>
<a href="sub-continuity.html"><span class="toc-section-number">6</span> Continuity</a>
<a href="sub-calculus.html"><span class="toc-section-number">7</span> Calculus</a>
<a href="ch-DE.html"><span class="toc-section-number">8</span> Differential Equations</a>
<a href="ch-CalUn.html"><span class="toc-section-number">9</span> Calculus under Uncertainty</a>
<a href="part-iii-emergence-of-abstract-interactions.html">PART III: Emergence of Abstract Interactions</a>
<a href="ch-vecMat.html"><span class="toc-section-number">10</span> Vector and Matrix</a>
<a href="ch-MatComp.html"><span class="toc-section-number">11</span> Matrix Computation</a>
<a href="ch-eigen.html"><span class="toc-section-number">12</span> Eigenvalues and Eigenvectors</a>
<a href="ch-UnMulti.html"><span class="toc-section-number">13</span> Uncertainty in Multiple Dimensions</a>
<a href="part-iv-three-masons-to-illuminate-the-dual-world.html">PART IV: Three Masons to Illuminate the Dual World</a>
<a href="ch-representation.html"><span class="toc-section-number">14</span> Representation</a>
<a href="ch-optApp.html"><span class="toc-section-number">15</span> Optimum Approximation</a>
<a id="active-page" href="ch-randomization.html"><span class="toc-section-number">16</span> Randomization</a><ul class="toc-sections">
<li class="toc"><a href="#sub:RHilbert"> Randomized Hilbert Space</a></li>
<li class="toc"><a href="#sub:SumDecomp"> Direct Sums and Decompositions</a></li>
<li class="toc"><a href="#sub:Kalman"> Miscellaneous: Sample Average, Reduction Principle, and Kalman’s Filter</a></li>
<li class="toc"><a href="#example-global-optimization"> Example: Global Optimization</a></li>
</ul>
<a href="bibliography.html">Bibliography</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="ch:randomization" class="section level1">
<h1>
<span class="header-section-number">16</span> Randomization</h1>
<p>Plato, in his work <em><em>the Republic</em></em>, describes a cave in which the residents are chained to a wall so they can’t see the real world; the best they can perceive are shadows reflected on the wall of the cave by some light outside. The residents have to make up their own interpretations of the outside world according to these shadows. All residents lack the omniscient knowledge of the real world, so they perceive the world differently based upon what is going on inside their minds and their “mental” projections of the shadows. That’s to say, facing the same situation, different people may consciously react differently; the different actions consecutively create different life paths. An observer from outside may find the evolutions of these paths somehow <a href="ch-CalUn.html#sub:rv">random</a>.</p>
<p>The <a href="ch-CalUn.html#sub:rv">randomness</a> emerges when the residents construct various elaborate ideas towards what reality is. The variety of these elaborate “mental” projections breaks down the unitarity and generates random outcomes. Such a construction is called <em>randomization</em>, a process of endowing unpredictable patterns to the events.<label for="tufte-sn-422" class="margin-toggle sidenote-number">422</label><input type="checkbox" id="tufte-sn-422" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">422</span> <strong>Randomization</strong> is particularly important in designing a game. Nobody wants to play a deterministic scissors-paper-stone game. This game is interesting because of the random feature - no one knows the opponent’s action. Similarly, suppose that many residents are characters immersing inside some kind of giant, massively “game platform” that is so well rendered that none of them can detect the artificiality; the way of keeping the game continue, I guess, is to create enough <a href="ch-CalUn.html#sub:rv">randomness</a> to prevent awakening the players. (Although attributing few random events a “non-random” feature to some players would make the game more “addictive,” this trick is non-applicable to all the events and all the players.)</span></p>
<div id="sub:RHilbert" class="section level2">
<h2>
<span class="header-section-number">16.1</span> Randomized Hilbert Space</h2>
<p>One way to think about <a href="ch-randomization.html#ch:randomization">randomization</a> is to consider the physical process of heating the crystals. The cooling crystals of a solid locate statically on some perfect lattice. During the heating process, the free energy accumulates so that the solid begins to melt, and the previous static crystals start to move randomly.<label for="tufte-sn-423" class="margin-toggle sidenote-number">423</label><input type="checkbox" id="tufte-sn-423" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">423</span> The inverse of this physical process, namely cooling down a melting solid, is called annealing.</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:annealing"></span>
<img src="fig/Part4/annealing.gif" alt="Heating the particles" width="100%"><!--
<p class="caption marginnote">-->Figure 16.1: Heating the particles<!--</p>-->
<!--</div>--></span>
</p>
<p>In a technical sense, if we restrict our attention to some objects in a <a href="ch-representation.html#sub:conjugacy">Hilbert space</a>, <span class="math inline">\(\mathcal{H}\)</span>, we can define the <a href="ch-randomization.html#ch:randomization">randomization</a> as mapping these objects into a <strong>Hilbert space</strong> associated with a certain <a href="sub-incomplete.html#sub:beyond2">probability space</a>.</p>
<ul>
<li>
<strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space, and mean square completeness</strong> : Let <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> be a <a href="sub-incomplete.html#sub:beyond2">probability space</a>, and let <span class="math inline">\(\mathcal{H}\)</span> be a (non-random) Hilbert space with the <a href="ch-vecMat.html#sub:vec">inner product</a> <span class="math inline">\(\left\langle \cdot,\cdot\right\rangle\)</span>. The <a href="ch-randomization.html#ch:randomization">randomization</a> of this Hilbert space on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> gives some measurable maps <span class="math inline">\(X(\omega):\Omega\rightarrow\mathcal{H}\)</span>, <span class="math inline">\(Y(\omega):\Omega\rightarrow\mathcal{H}\)</span>, and the <em><span class="math inline">\(\mathbb{P}\)</span>-inner product</em> <span class="math inline">\(\left\langle \cdot,\cdot\right\rangle _{\mathbb{P}}\)</span> such that:
<span class="math display">\[\left\langle X(\omega),Y(\omega)\right\rangle _{\mathbb{P}}=\int\left\langle X(\omega),Y(\omega)\right\rangle \mathbb{P}(\mbox{d}\omega)=\mathbb{E}\left[\left\langle X(\omega),Y(\omega)\right\rangle \right].\]</span>
In particular, any random element <span class="math inline">\(X(\omega)\)</span> in this randomized Hilbert space has the finite second moment, e.g., <span class="math display">\[\mathbb{E}\left[\left\langle X(\omega),X(\omega)\right\rangle \right]^{2}=\mathbb{E}[\|X(\omega)\|^{2}]&lt;\infty.\]</span>
These random elements are known as <em>mean-square integrable <span class="math inline">\(\mathcal{H}\)</span>-value random variables</em>. The space is denoted by <span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>.
A sequence <span class="math inline">\(\{X_{n}(\omega)\}_{n\in\mathbb{N}}\)</span> of the <em><span class="math inline">\(\mathcal{H}\)</span>-valued random variables</em> is said to converge to <span class="math inline">\(X(\omega)\)</span> in <em>mean square</em> if
<span class="math display">\[\lim_{n\rightarrow\infty}\mathbb{E}\left[\left|X_{n}(\omega)-X(\omega)\right|^{2}\right]=0\]</span> with <span class="math inline">\(\mathbb{E}[|X_{n}(\omega)|^{2}]&lt;\infty\)</span> for all <span class="math inline">\(n\)</span>.
The <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span> space</strong> is <a href="ch-representation.html#sub:conjugacy">complete</a> with respect to the <strong>mean square</strong>.<label for="tufte-sn-424" class="margin-toggle sidenote-number">424</label><input type="checkbox" id="tufte-sn-424" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">424</span> Recall that the Hilbert space is complete with respect to the <a href="ch-representation.html#sub:innerProd"><span class="math inline">\(L_{2}\)</span>-norm</a>.</span>
</li>
</ul>
<p>Here are three examples of the <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span> space</strong>.
First, suppose that the <a href="ch-representation.html#sub:conjugacy">Hilbert space</a> <span class="math inline">\(\mathcal{H}\)</span> is just a real-valued <a href="ch-MatComp.html#sub:vecSpaces">vector space</a> in the finite dimension, i.e., <span class="math inline">\(\mathcal{H}=\mathbb{R}^{n}\)</span>, the <strong><span class="math inline">\(\mathcal{H}\)</span>-valued random variables</strong> are standard random vectors, i.e., <span class="math display">\[\begin{align*}\mathbf{\mathbf{X}}(\omega)=\left[\begin{array}{c}
X_{1}(\omega)\\
\vdots\\
X_{n}(\omega)
\end{array}\right]&amp;,\: \mathbf{\mathbf{Y}}(\omega)=\left[\begin{array}{c}
Y_{1}(\omega)\\
\vdots\\
Y_{n}(\omega)
\end{array}\right],\\
\left\langle \mathbf{X}(\omega),\mathbf{Y}(\omega)\right\rangle _{\mathbb{P}}=&amp;\mathbb{E}\left[\sum_{i=1}^{n}X_i(\omega)Y_i(\omega)\right],\end{align*}\]</span>
where the <span class="math inline">\(\mathbb{P}\)</span>-inner product <span class="math inline">\(\left\langle \mathbf{X}(\omega),\mathbf{Y}(\omega)\right\rangle _{\mathbb{P}}\)</span> can also be written as
<span class="math display">\[\left\langle \mathbf{X}(\omega),\mathbf{Y}(\omega)\right\rangle _{\mathbb{P}} = \int_{\mathbb{R}^{n}}\int_{\mathbb{R}^{n}}\langle \mathbf{x},\mathbf{y}\rangle p(\mathbf{x},\mathbf{y})\mbox{d}\mathbf{x}\mbox{d}\mathbf{y}=\int\int  (\mathbf{x}^\top\mathbf{y} p(\mathbf{x},\mathbf{y}))\mbox{d}\mathbf{x}\mbox{d}\mathbf{y},\]</span>
where <span class="math inline">\(p(\cdot,\cdot)\)</span> is the <a href="ch-CalUn.html#sub:conProb">joint probability</a> density function of <span class="math inline">\(\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}\)</span>.<label for="tufte-sn-425" class="margin-toggle sidenote-number">425</label><input type="checkbox" id="tufte-sn-425" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">425</span> If we assume that the set <span class="math inline">\(\Omega\)</span> in the probability space <span class="math inline">\((\Omega, \mathcal{F}, \mathbf{P})\)</span> only contains <a href="ch-CalUn.html#sub:rv">discrete states</a>, the <span class="math inline">\(\mathbb{P}\)</span>-inner product becomes
<span class="math display">\[\begin{align*}\left\langle \mathbf{X}(\omega),\mathbf{Y}(\omega)\right\rangle _{\mathbb{P}}&amp;= \mathbb{E}[X(\omega)Y(\omega)]\\&amp;=\sum_{i=1}^{n}x_{i}y_{i}p(x_{i},y_{i}).\end{align*}\]</span></span></p>
<p>Second, suppose that the <a href="ch-representation.html#sub:conjugacy">Hilbert space</a> <span class="math inline">\(\mathcal{H}\)</span> is an infinite-dimensional space of real-valued functions on the domain <span class="math inline">\(t\in[0,T]\)</span>, i.e., <span class="math inline">\(\mathcal{H}=L_2[0,T]\)</span>, then the <strong><span class="math inline">\(\mathcal{H}\)</span>-valued random variables</strong> are <a href="ch-UnMulti.html#sub:Markov">stochastic processes</a>, i.e., <span class="math inline">\(X(t,\omega)\)</span> and <span class="math inline">\(Y(t,\omega)\)</span>. Informally, we can think to extend the previous example to the infinite dimension.<label for="tufte-sn-426" class="margin-toggle sidenote-number">426</label><input type="checkbox" id="tufte-sn-426" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">426</span> A <a href="ch-UnMulti.html#sub:Markov">stochastic processes</a> <span class="math inline">\(X(t,\omega)\)</span> can be thought of as a collection of infinite many random variables at infinite time points <span class="math inline">\(t_1, t_2,\dots\)</span>. So we have an infinite-length vector <span class="math inline">\(X_{t_1}(\omega),\dots X_{t_n}(\omega)\dots\)</span>. Also, the probability space of the <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong> is a <a href="ch-UnMulti.html#sub:Markov">filtered space</a> <span class="math inline">\((\Omega,\mathcal{F},\{\mathcal{F}_{t}\}_{t&gt;0},\mathbb{P})\)</span> where <span class="math inline">\(\mathcal{F}_1\)</span> is for <span class="math inline">\(X_{t_1}\)</span>, etc.</span> That is, given a fixed <span class="math inline">\(\omega\in\Omega\)</span> (the sample path), the <strong><span class="math inline">\(\mathcal{H}\)</span>-valued</strong> <span class="math inline">\(\{X(\cdot,\omega):t\in[0,T]\}\)</span> is a deterministic function, say <span class="math inline">\(X(\cdot,\omega):[0,t]\rightarrow L_{2}[0,T]\)</span>.
The <span class="math inline">\(\mathbb{P}\)</span>-inner product of these stochastic processes is given by
<span class="math display">\[\begin{align*}&amp; \left\langle X(t,\omega),Y(t,\omega)\right\rangle _{\mathbb{P}}=\int\int X(s,\omega)Y(s,\omega)\mbox{d}s\mathbb{P}(\mbox{d}\omega)\\&amp;=\mathbb{E}\left[\left\langle X(s,\omega),Y(s,\omega)\right\rangle \right]=\mathbb{E}\left[\int_{0}^{t}X(s,\omega)Y(s,\omega)\mbox{d}s\right].\end{align*}\]</span></p>
<p>Third, one fundamental <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> in <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong> is the <em>Gaussian Hilbert space</em>, a <a href="ch-CalUn.html#sub:divRV">complete</a> space consisting of zero-mean <a href="ch-CalUn.html#sub:divRV">Gaussian random variables</a> from the probability space <span class="math inline">\((\Omega,\mathcal{F},\mathcal{N})\)</span> where <span class="math inline">\(\mathcal{N}\)</span> stands for the <a href="ch-CalUn.html#sub:divRV">Gaussian probability law</a>. For example, for any <span class="math inline">\(n\)</span> independent identical <span class="math inline">\(\varepsilon_i \sim \mathcal{N}(0,\sigma^2)\)</span>, the <a href="ch-MatComp.html#sub:vecSpaces">span</a> of these <a href="ch-CalUn.html#sub:divRV">Gaussian random variables</a> <span class="math display">\[\mbox{span}\left\{\sum_{i=1}^{n} c_i\varepsilon_i\,:\, \sum_{i=1}^n|c_i|^2\leq \infty\right\}\]</span> is a <strong>Gaussian Hilbert space</strong>.</p>
<p>In the <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong>, we can formalize the randomness for infinite-dimensional objects.
One example is to randomize the (deterministic) <a href="sub-inferknow.html#sub:dyn">dynamics</a>. Consider the following <a href="ch-DE.html#sub:ode">differential equation</a>
<span class="math display">\[\frac{\mbox{d}x(t)}{\mbox{d}t} =f(x(t)),  \,\, \mbox{with }
x(0) =x_{0}.\]</span>
If the solution of the system exists, say <span class="math inline">\(x(t)\in\mathcal{H}\)</span>, it must be a function <span class="math inline">\(x(\cdot):[0,T]\rightarrow\mathbb{R}\)</span>.<label for="tufte-sn-427" class="margin-toggle sidenote-number">427</label><input type="checkbox" id="tufte-sn-427" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">427</span> Suppose the function <span class="math inline">\(f(\cdot)\)</span> is <a href="sub-continuity.html#sub:continuity">Lipschitz continuous</a>; the solution exists and is unique.</span> <a href="ch-randomization.html#ch:randomization">Randomization</a> is to parameterize the function <span class="math inline">\(f(\cdot)\)</span> by a random variable <span class="math inline">\(\varepsilon_{t}\)</span> at time <span class="math inline">\(t\)</span> with <span class="math inline">\(\mathbb{E}[|\varepsilon_t|^2]\leq\infty\)</span>. The system then becomes <span class="math display">\[
\frac{\mbox{d}X(t,\omega)}{\mbox{d}t} =f\left(X(t,\omega),\varepsilon_t\right),\,\, \mbox{with }
X(0,\omega) =X_{0},\]</span>
whose solution, if it exists, is a <a href="ch-UnMulti.html#sub:Markov">stochastic process</a> <span class="math display">\[X(\cdot,\cdot):[0,T]\times(\Omega,\mathcal{F},\{\mathcal{F}_{t}\}_{t\in[0,T]},\mathbb{P})\rightarrow\mathbb{R}.\]</span> Given <span class="math inline">\(\omega\)</span>, the deterministic function <span class="math inline">\(X(\cdot,\omega)\)</span> is an <span class="math inline">\(\mathcal{H}\)</span>-valued element.</p>
<p>The above differentiation notation of the stochastic process <span class="math inline">\(X(t,\omega)\)</span> may look awkward because the stochastic processes often have zigzags and because we have seen the “non-differentiability” natural of zigzags in chapter <a href="sub-calculus.html#sub:noDiff">7.2</a>. The fact is that the differentiation <span class="math inline">\(\mbox{d}X(t,\omega)/\mbox{d}t\)</span> refers to a “differentiation” in the <strong>mean square</strong> sense. For the <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space’s</strong> processes, we can introduce this new differentiation by some standard calculus conditions on the <a href="ch-CalUn.html#sub:ex">means</a> and the <a href="ch-UnMulti.html#sub:MultiVar">covariances</a>.<label for="tufte-sn-428" class="margin-toggle sidenote-number">428</label><input type="checkbox" id="tufte-sn-428" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">428</span> Notice that for a <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong> stochastic process, the first- and second-order densities (if they exist) can answer most (if not all) important probabilistic questions about the process. In other words, all the probability laws of the process are given by the first- and second-order densities. So it would be quite straightforward to consider the parameters associated with these densities, namely the <a href="ch-CalUn.html#sub:ex">means</a> and the <a href="ch-UnMulti.html#sub:MultiVar">covariance</a>.</span></p>
<p>One can define an equivalence class of stochastic processes having prescribed first and second-order <a href="ch-CalUn.html#sub:ex">moments</a>. This class is normally labeled as the <em>second-order process</em>. Let <span class="math inline">\(\{X(t,\omega)\}_{t\in\mathbb{R}^{+}}\)</span> be a stochastic process and let <span class="math inline">\(\{X_t(\omega)\}_{t\in\mathbb{N}}\)</span> be a time series.</p>
<ul>
<li>
<em>Mean value function</em> of the process or the time series:
<span class="math display">\[\mu_X(t) =\mathbb{E}[X(t,\omega)] = \mathbb{E}[X_t(\omega)].\]</span>
</li>
<li>
<em>(Auto)-covariance function</em> of the process or the time series:
<span class="math display">\[\begin{align*}\mbox{Cov}_X (t,s) &amp;=\mathbb{E}\left[(X(t,\omega)- \mu_{X}(t))(X(s,\omega)- \mu_{X}(s))\right], 
\\&amp;=\mathbb{E}[X(t,\omega)X(s,\omega)]- \mu_{X}(t)\mu_{X}(s);
\end{align*}\]</span>
</li>
</ul>
<p>Many interesting models can be characterized by the <strong>mean and covariance functions</strong>.
For example, consider the <a href="ch-eigen.html#sub:matNorms">AR(1)</a> model <span class="math inline">\(X_{t+1}=\phi X_{t}+\varepsilon_{t}\)</span> where <span class="math inline">\(\varepsilon_{t}\sim\mathcal{N}(0,\sigma_{\epsilon}^{2})\)</span> is <a href="ch-CalUn.html#sub:divRV">independent</a> of <span class="math inline">\(X_t\)</span>. The mean and covariance functions follow the dynamic law:<label for="tufte-sn-429" class="margin-toggle sidenote-number">429</label><input type="checkbox" id="tufte-sn-429" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">429</span> The law comes from
<span class="math display">\[\begin{align*}
\mu_{X}(t+1)&amp;=\mathbb{E}[X_{t+1}]\\&amp;=\mathbb{E}[\phi X_{t}+\varepsilon_{t}]\\
&amp;=\phi\mu_{X}(t)\\
\mbox{Cov}_{X}(t+1,t)   &amp;=\mbox{Cov}(X_{t+1},X_{t})\\
&amp;=\mbox{Cov}(\phi X_{t}+\epsilon_{t},X_{t})
    \\&amp;=\phi\mbox{Cov}(X_{t},X_{t})+\mbox{Cov}(\epsilon_{t},X_{t})
    \\&amp;=\phi\mbox{Cov}_{X}(t,t)\end{align*}\]</span>
Sometimes people are interested in AR models with invariant variances, namely <span class="math display">\[\mbox{Var}(X_t(\omega))=\cdots=\mbox{Var}(X_1(\omega))=\sigma^2.\]</span><br>
Then there is
<span class="math display">\[\sigma^{2} = \phi^{2} \sigma^{2} + \sigma_{\epsilon}^{2}\]</span> which implies <span class="math inline">\(\sigma=\sigma_{\epsilon}^{2}/(1-\phi)\)</span>. Thus the <strong>covariance function</strong> of this AR(1) model can be simplified into <span class="math display">\[\mbox{Cov}_{X}(t+1,t)=\frac{\sigma_{\epsilon}^{2}\phi}{1-\phi}.\]</span></span>
<span class="math display">\[\mu_{X}(t+1)=\phi\mu_{X}(t),\,\,\mbox{Cov}_{X}(t+1,t)=\phi\mbox{Cov}_{X}(t,t).\]</span></p>
<p>Another essential sub-class of the <strong>second-order processes</strong> is the <em>Gaussian process</em>, whose probability law is uniquely determined by the following specification <span class="math display">\[X(\cdot,\omega)\sim \mathcal{N}(\mu_X(\cdot),\mbox{Cov}_X(\cdot,\cdot)).\]</span> To understand the above specification, let’s consider a real-valued random vector <span class="math inline">\(\mathbf{X}(\omega)\in\mathbb{R}^{n}\)</span> with <a href="ch-CalUn.html#sub:divRV">Gaussian distribution</a> <span class="math inline">\(\mathbf{X}(\omega)\sim \mathcal{N}(\mathbf{\mu},\Sigma)\)</span>.
The first-order information is given by the mean vector <span class="math inline">\(\mathbb{E}[\mathbf{X}(\omega)]=\mathbf{\mu}\)</span>; the finite second-order information is contained in the <a href="ch-UnMulti.html#sub:MultiVar">covariance matrix</a> <span class="math inline">\(\mbox{Var}(\mathbf{X}(\omega))=\Sigma\)</span>. Now let’s turn to a <strong>Gaussian process</strong> <span class="math inline">\(X(t,\omega)\in\mathbb{R}\)</span> with mean function <span class="math inline">\(\mu_X(\cdot)\)</span> and <span class="math inline">\(\mbox{Cov}_X(\cdot,\cdot)\)</span>. If we visualize the process at any <span class="math inline">\(n\)</span> time points <span class="math inline">\((t_1,\dots,t_n)\)</span>, then the process looks like a random vector of the time series <span class="math inline">\(X_{t_1}(\omega),\dots,X_{t_n}(\omega)\)</span>:
<span class="math display">\[\underset{\mathbf{X}(\omega)}{\underbrace{\left[\begin{array}{c}
X(t_{1},\omega)\\
\vdots\\
X(t_{n},\omega)
\end{array}\right]}}\sim\mathcal{N}\left(\underset{\mathbf{\mu}}{\underbrace{\left[\begin{array}{c}
\mu_{X}(t_{1})\\
\vdots\\
\mu_{X}(t_{n})
\end{array}\right]}},\underset{\Sigma}{\underbrace{\left[\begin{array}{ccc}
\mbox{Cov}_{X}(t_{1},t_{1}) &amp; \cdots &amp; \mbox{Cov}_{X}(t_{1},t_{n})\\
\vdots &amp; \ddots &amp; \vdots\\
\mbox{Cov}_{X}(t_{n},t_{1}) &amp; \cdots &amp; \mbox{Cov}_{X}(t_{n},t_{n})
\end{array}\right]}}\right).\]</span>
Extending this <span class="math inline">\(n\)</span>-length random vector to infinity, we are supposed to have the <strong>Gaussian process</strong>:<label for="tufte-sn-430" class="margin-toggle sidenote-number">430</label><input type="checkbox" id="tufte-sn-430" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">430</span> The discretization of a continuous time <strong>Gaussian process</strong> has an interesting implication. The <a href="ch-UnMulti.html#sub:MultiVar">covariance matrices</a> as the <a href="ch-UnMulti.html#sub:MultiVar">positive (semi)-definite matrices</a> may have their corresponding underlying functions. Moreover, these functions may also have corresponding underlying <a href="ch-MatComp.html#sub:vecSpaces">operators</a> (the functions’ infinite-dimensional counterparts). In practice, the discretized <strong>Gaussian process</strong>’s <a href="ch-UnMulti.html#sub:MultiVar">covariance matrices</a> come from some “kernels” or called “positive operators.” We will come back to these concepts in sec[?].</span> <span class="math display">\[X(\cdot,\omega)\sim \mathcal{N}(\mu_X(\cdot),\mbox{Cov}_X(\cdot,\cdot)).\]</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:GP"></span>
<img src="fig/Part4/GP.png" alt="Gaussian processes with zero mean" width="100%"><!--
<p class="caption marginnote">-->Figure 16.2: Gaussian processes with zero mean<!--</p>-->
<!--</div>--></span>
</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:GPKern"></span>
<img src="fig/Part4/GPKern.png" alt="The corresponding covariance matrces" width="100%"><!--
<p class="caption marginnote">-->Figure 16.3: The corresponding covariance matrces<!--</p>-->
<!--</div>--></span>
</p>
<div class="solution">
<p class="solution-begin">
Simulation of Gaussian processes <span id="sol-start-187" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-187', 'sol-start-187')"></span>
</p>
<div id="sol-body-187" class="solution-body" style="display: none;">
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">20210228</span>)</a>
<a class="sourceLine" id="cb122-2" data-line-number="2"><span class="kw">library</span>(MASS) <span class="co"># call for multivariate normal sampling function</span></a>
<a class="sourceLine" id="cb122-3" data-line-number="3">n =<span class="st"> </span><span class="dv">100</span>;</a>
<a class="sourceLine" id="cb122-4" data-line-number="4">CovMatrix1 =<span class="st"> </span><span class="cf">function</span>(s, t) {<span class="kw">min</span>(s, t)}; <span class="co"># Brownian motion</span></a>
<a class="sourceLine" id="cb122-5" data-line-number="5">CovMatrix2 =<span class="st"> </span><span class="cf">function</span>(s,t){ <span class="kw">exp</span>(<span class="op">-</span><span class="dv">10</span><span class="op">*</span><span class="kw">abs</span>(s<span class="op">-</span>t))}; <span class="co"># OU</span></a>
<a class="sourceLine" id="cb122-6" data-line-number="6">CovMatrix3 =<span class="st"> </span><span class="cf">function</span>(s,t){ <span class="kw">as.numeric</span>(s<span class="op">==</span>t)}; <span class="co"># Gaussian white noise</span></a>
<a class="sourceLine" id="cb122-7" data-line-number="7">t =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">0</span>, <span class="dt">to =</span> <span class="dv">1</span>, <span class="dt">length.out =</span> n); <span class="co"># t_1=0,..., t_n=1</span></a>
<a class="sourceLine" id="cb122-8" data-line-number="8"></a>
<a class="sourceLine" id="cb122-9" data-line-number="9"><span class="co"># Fill in entities of the covariance matrix</span></a>
<a class="sourceLine" id="cb122-10" data-line-number="10">CovMatrix1 =<span class="st"> </span><span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">1</span>){<span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">2</span>){</a>
<a class="sourceLine" id="cb122-11" data-line-number="11">     <span class="kw">CovMatrix1</span>(argument_<span class="dv">1</span>, argument_<span class="dv">2</span>)})})</a>
<a class="sourceLine" id="cb122-12" data-line-number="12">CovMatrix2 =<span class="st"> </span><span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">1</span>){<span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">2</span>){</a>
<a class="sourceLine" id="cb122-13" data-line-number="13">     <span class="kw">CovMatrix2</span>(argument_<span class="dv">1</span>, argument_<span class="dv">2</span>)})})</a>
<a class="sourceLine" id="cb122-14" data-line-number="14">CovMatrix3 =<span class="st"> </span><span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">1</span>){<span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">2</span>){</a>
<a class="sourceLine" id="cb122-15" data-line-number="15">     <span class="kw">CovMatrix3</span>(argument_<span class="dv">1</span>, argument_<span class="dv">2</span>)})})</a>
<a class="sourceLine" id="cb122-16" data-line-number="16"><span class="co"># Sampling</span></a>
<a class="sourceLine" id="cb122-17" data-line-number="17">samplePath1 =<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dt">mu =</span> <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dt">times =</span> n), <span class="dt">Sigma =</span> CovMatrix1);</a>
<a class="sourceLine" id="cb122-18" data-line-number="18">samplePath2 =<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dt">mu =</span> <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dt">times =</span> n), <span class="dt">Sigma =</span> CovMatrix2);</a>
<a class="sourceLine" id="cb122-19" data-line-number="19">samplePath3 =<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dt">mu =</span> <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dt">times =</span> n), <span class="dt">Sigma =</span> CovMatrix3);</a>
<a class="sourceLine" id="cb122-20" data-line-number="20"></a>
<a class="sourceLine" id="cb122-21" data-line-number="21"><span class="co"># plot</span></a>
<a class="sourceLine" id="cb122-22" data-line-number="22"><span class="kw">library</span>(ggplot2); <span class="kw">library</span>(reshape2); <span class="kw">library</span>(expm)</a>
<a class="sourceLine" id="cb122-23" data-line-number="23">dat =<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">c</span>(t,samplePath1, samplePath2, samplePath3), <span class="dt">ncol=</span><span class="dv">4</span>))</a>
<a class="sourceLine" id="cb122-24" data-line-number="24"><span class="kw">names</span>(dat)=<span class="kw">c</span>(<span class="st">"time"</span>,<span class="st">"Brownian motion"</span>,<span class="st">"Ornstein-Uhlenbeck"</span>,<span class="st">"Gaussian white noise"</span>);</a>
<a class="sourceLine" id="cb122-25" data-line-number="25"><span class="kw">ggplot</span>( <span class="kw">melt</span>(dat, <span class="dt">id.vars=</span><span class="st">"time"</span>), <span class="kw">aes</span>(time, value, <span class="dt">group=</span>variable)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()   <span class="op">+</span></a>
<a class="sourceLine" id="cb122-26" data-line-number="26"><span class="op">+</span><span class="st">   </span><span class="kw">facet_grid</span>(.<span class="op">~</span>variable)<span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</a>
<a class="sourceLine" id="cb122-27" data-line-number="27">m1=<span class="kw">Matrix</span>(CovMatrix1, <span class="dt">sparse=</span><span class="ot">TRUE</span>); m2=<span class="kw">Matrix</span>(CovMatrix2, <span class="dt">sparse=</span><span class="ot">TRUE</span>); m3=<span class="kw">Matrix</span>(CovMatrix3, <span class="dt">sparse=</span><span class="ot">TRUE</span>);</a>
<a class="sourceLine" id="cb122-28" data-line-number="28"><span class="kw">print</span>(<span class="kw">image</span>(m1, <span class="dt">main=</span><span class="st">"Brownian motion"</span>), <span class="dt">split=</span><span class="kw">c</span>(<span class="dt">x=</span><span class="dv">1</span>,<span class="dt">y=</span><span class="dv">1</span>,<span class="dt">nx=</span><span class="dv">3</span>, <span class="dt">ny=</span><span class="dv">1</span>), <span class="dt">more=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb122-29" data-line-number="29"><span class="kw">print</span>(<span class="kw">image</span>(m2, <span class="dt">main=</span><span class="st">"Ornstein-Uhlenbeck"</span>), <span class="dt">split=</span><span class="kw">c</span>(<span class="dt">x=</span><span class="dv">2</span>,<span class="dt">y=</span><span class="dv">1</span>,<span class="dt">nx=</span><span class="dv">3</span>, <span class="dt">ny=</span><span class="dv">1</span>), <span class="dt">more=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb122-30" data-line-number="30"><span class="kw">print</span>(<span class="kw">image</span>(m3, <span class="dt">main=</span><span class="st">"Gaussian white noise"</span>, <span class="dt">Imult=</span><span class="fl">0.1</span>), <span class="dt">split=</span><span class="kw">c</span>(<span class="dt">x=</span><span class="dv">3</span>,<span class="dt">y=</span><span class="dv">1</span>,<span class="dt">nx=</span><span class="dv">3</span>, <span class="dt">ny=</span><span class="dv">1</span>))</a></code></pre></div>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Now, we come back to the discussion of the differentiation issue for the stochastic process <span class="math inline">\(X(t,\omega)\)</span>. The issue is resolvable with defining a meaningful calculus towards the randomness.
The following definition gives the <em>mean square calculus</em> for the processes in <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong>.</p>
<ul>
<li><p><strong>Mean square continuity</strong>: When <span class="math display">\[\lim_{h\rightarrow0}\mathbb{E}\left[\left|X(t+h, \omega)-X(t, \omega)\right|^{2}\right]=0,\]</span>
the random variable <span class="math inline">\(X(t,\omega)\)</span> is <em>mean square continous</em> at <span class="math inline">\(t\)</span>.</p></li>
<li><p><strong>Mean square differentiability</strong> : When the second order derivative <span class="math display">\[\frac{\partial^{2}\mathbb{E}[X(t,\omega)X(s,\omega)]}{\partial t\partial s}\]</span>
exists for any <span class="math inline">\(t,s \in [a,b]\)</span>, <span class="math inline">\(X(t,\omega)\)</span> is <em>mean square differentiable</em> such that <span class="math display">\[\lim_{h\rightarrow0}\mathbb{E}\left[\left|\frac{X(t+h,\omega)-X(t,\omega)}{h}-\frac{\mbox{d}X(t,\omega)}{\mbox{d}t}\right|^{2}\right]=0.\]</span></p></li>
<li><p><strong>Mean square integrability</strong> : When the (Riemann) <a href="sub-calculus.html#sub:diffInt">integral</a> <span class="math display">\[\int_{a}^{b}\int_{a}^{b}\mathbb{E}[X(t,\omega)X(s,\omega)]\mbox{d}t\mbox{d}s\]</span> is well defined over the interval <span class="math inline">\([a,b]\)</span>, then <span class="math inline">\(X(t,\omega)\)</span> is <em>mean square integrable</em> over <span class="math inline">\([a,b]\)</span> with the <em>mean square integral</em> <span class="math inline">\(\int_{a}^{b} X(t,\omega) \mbox{d}t\)</span>.<label for="tufte-sn-431" class="margin-toggle sidenote-number">431</label><input type="checkbox" id="tufte-sn-431" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">431</span> Here is a more formal definition of the <em>mean square integral</em>. For an interval <span class="math inline">\([a,b]\)</span>, we make a partition <span class="math inline">\(a=t_{0}&lt;t_{1}&lt;\cdots&lt;t_{n}=b\)</span> and
let <span class="math inline">\(\kappa=\max_{i}(t_{i+1}-t_{i})\)</span>, <span class="math inline">\(t_{i}\leq m_{i}\leq t_{i+1}\)</span>. The <strong>mean square integral</strong> of <span class="math inline">\(X(t,\omega)\)</span> is given by the <strong>mean square limit</strong> of <span class="math inline">\(\sum_{i=0}^{n-1}X_{m_{i}}(\omega)(t_{i+1}-t_{i})\)</span> that is
<span class="math display">\[\begin{align*}\lim_{\kappa\rightarrow0, n\rightarrow\infty}\mathbb{E}\left[\left|\sum_{i=0}^{n-1}X_{m_{i}}(\omega)(t_{i+1}-t_{i})-\\
\int_{a}^{b}X(t,\omega)\mbox{d}t\right|^{2}\right]=0.\end{align*}\]</span></span></p></li>
<li><p><em>Fundamental theorem of mean square calculus</em> : <span class="math display">\[\Pr\left\{ \left|X(t,\omega)-X(a,\omega)=\int_{a}^{t}\left(\frac{\mbox{d}X(\tau,\omega)}{\mbox{d}\tau}\right)\mbox{d}\tau\right|\right\} =1.\]</span></p></li>
</ul>
<p>The term <span class="math inline">\(\mathbb{E}[X(t,\omega)X(s,\omega)]\)</span> connects to the <strong>mean square</strong> criterion:
<span class="math display">\[\begin{align*}\mathbb{E}\left[\left|X(t+h)-X(t)\right|^{2}\right]&amp;=\mathbb{E}\left[X(t+h)X(t+h)\right]-\mathbb{E}\left[X(t+h)X(t)\right]\\
&amp;-\mathbb{E}\left[X(t)X(t+h)\right]+\mathbb{E}\left[X(t)X(t)\right].\end{align*}\]</span>
That is to say, the <strong>mean square continuity</strong> is all about continuity of <span class="math inline">\(\mathbb{E}[X(t,\omega)X(s,\omega)]\)</span> for any <span class="math inline">\(t,s\)</span>. In addition, both <strong>mean square differentiability and integrability</strong> relate to the calculus of <span class="math inline">\(\mathbb{E}[X(t,\omega)X(s,\omega)]\)</span>.<label for="tufte-sn-432" class="margin-toggle sidenote-number">432</label><input type="checkbox" id="tufte-sn-432" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">432</span> To have a vague idea about this relationship, recall that the definitions of <a href="sub-calculus.html#sub:diffInt">differentiation</a> and <a href="sub-calculus.html#sub:diffInt">integration</a> are both based on the continuity of some criteria of <a href="sub-continuity.html#sub:rational">infinitesimal changes</a>. Thus, the <strong>mean square differentiability and integrability</strong> must be based on the continuity of some <strong>mean square</strong> criteria of <a href="sub-continuity.html#sub:rational">infinitesimal changes</a>. As these <strong>mean square</strong> criteria all relate to the terms of <span class="math inline">\(\mathbb{E}[X(t,\omega)X(s,\omega)]\)</span>, we can see that the essence of defining <strong>mean square differentiability and integrability</strong> is to define the differentiability and integrability of <span class="math inline">\(\mathbb{E}[X(t,\omega)X(s,\omega)]\)</span> for any <span class="math inline">\(t,s\)</span>.</span>
By the equality
<span class="math display">\[\mathbb{E}[X(t,\omega)X(s,\omega)]=\mbox{Cov}_X (t,s)+\mu_{X}(t)\mu_{X}(s),\]</span> one can induce that the <strong>mean square continuity, differentiability, and integrability</strong> actually relate to the continuity and the calculus of <strong>mean and covariance functions</strong>.</p>
<div class="solution">
<p class="solution-begin">
Some remark about the mean square criterion <span id="sol-start-188" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-188', 'sol-start-188')"></span>
</p>
<div id="sol-body-188" class="solution-body" style="display: none;">
<p>Let’s discretize the stochastic process <span class="math inline">\(X(t,\omega)\)</span> into a stochastic sequence <span class="math inline">\(\{X_{n}(\omega)\}_{n\in\mathbb{N}}\)</span>.</p>
<ol style="list-style-type: decimal">
<li>The <strong>mean square convergence</strong> implies probabilistic convergence in section <a href="ch-UnMulti.html#sub:WLLN">13.4</a>.</li>
</ol>
<p>Proof: For any <span class="math inline">\(\epsilon&gt;0\)</span> and a random variable <span class="math inline">\(Y\)</span> with a finite second moment, we have
<span class="math display">\[\begin{align*}\mathbb{E}[|Y|^{2}]=&amp;   \int|y|^{2}p(y)\mbox{d}y\\ &amp;=\int_{-\infty}^{-\epsilon}|y|^{2}p(y)\mbox{d}y+\int_{-\epsilon}^{\infty}|y|^{2}p(y)\mbox{d}y\\
&amp;\geq   \epsilon^{2}\left(\int_{-\infty}^{-\epsilon}p(y)\mbox{d}y+\int_{-\epsilon}^{\infty}p(y)\mbox{d}y\right)\\
&amp;=\epsilon^{2}\Pr\left\{ |y|\geq\epsilon\right\}.\end{align*}\]</span>
Substituting <span class="math inline">\(X-X_{n}\)</span> for <span class="math inline">\(Y\)</span>, we have
<span class="math display">\[\Pr\left\{ |X-X_{n}|\geq\epsilon\right\} \leq\frac{\mathbb{E}\left[\left|X_{n}-X\right|^{2}\right]}{\epsilon^{2}}\]</span>
by the <a href="ch-UnMulti.html#sub:WLLN">Markov inequality</a> (see the proof of <a href="ch-UnMulti.html#sub:WLLN">weak law of large numbers</a>).</p>
<ol start="2" style="list-style-type: decimal">
<li><p>The <a href="sub-continuity.html#sub:Cauchy">Cauchy criterion</a> in the <strong>mean square</strong> sense means
<span class="math display">\[\lim_{n,m\rightarrow\infty}\mathbb{E}\left[X_{n}-X_{m}\right]^{2}=0.\]</span>
If the <a href="sub-continuity.html#sub:Cauchy">Cauchy criterion</a> holds, we say that the stochastic sequence <span class="math inline">\(\{X_{n}\}\)</span> <strong>converges in mean square</strong> to some <span class="math inline">\(X\)</span>,
<span class="math display">\[\lim_{n\rightarrow\infty}\mathbb{E}\left[X_{n}-X\right]^{2}=0.\]</span></p></li>
<li><p>If <span class="math inline">\(X_n\)</span> <strong>converges in mean square</strong> to <span class="math inline">\(X\)</span>, then
<span class="math display">\[\mathbb{E}[X]=\lim_{n\rightarrow\infty}\mathbb{E}[X_n].\]</span></p></li>
</ol>
<p>Proof: Notice that the following equality <span class="math display">\[\lim_{n\rightarrow\infty}\mathbb{E}\left[\left|X_{n}-X\right|^{2}\right]=\lim_{n\rightarrow\infty}\left|\mathbb{E}[X_{n}-X]\right|^{2}=0\]</span>
can be verified by the <a href="ch-vecMat.html#sub:vec">Schwarz inequality</a>: <span class="math display">\[\left|\mathbb{E}[X_{n}-X]\right|^{2}\leq\mathbb{E}\left[\left|X_{n}-X\right|^{2}\right].\]</span>
Because <span class="math inline">\(\lim_{n\rightarrow\infty}\left|\mathbb{E}[X_{n}-X]\right|^{2}=0\)</span> means
<span class="math display">\[\lim_{n\rightarrow\infty}\mathbb{E}[X_{n}-X]=0,\]</span> the result follows.</p>
<ol start="4" style="list-style-type: decimal">
<li>If <span class="math inline">\(X_n\)</span> and <span class="math inline">\(Y_n\)</span> <strong>converge in mean square</strong> to <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, then <span class="math display">\[\mathbb{E}[XY]=\lim_{n\rightarrow\infty}\mathbb{E}[X_{n}Y_{n}].\]</span>
</li>
</ol>
<p>Proof: Note that if <span class="math inline">\(X_n\)</span> and <span class="math inline">\(Y_n\)</span> <strong>converge in mean square</strong> to <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, we have
<span class="math display">\[\lim_{n\rightarrow\infty}\mathbb{E}[X_{n}-X]=\lim_{n\rightarrow\infty}\mathbb{E}[Y_{n}-Y]=0.\]</span></p>
<p>To see this, we make the following inequality
<span class="math display">\[\begin{align*}\left|\mathbb{E}[XY]-\mathbb{E}[X_{n}Y_{n}]\right|  =\left|\mathbb{E}[(Y-Y_{n})X_{n}]+\mathbb{E}[(X-X_{t})Y_{n}]-\mathbb{E}[(X-X_{t})(Y-Y_{n})]\right|\\
\leq    \left|\mathbb{E}[(Y-Y_{n})X_{n}]\right|+\left|\mathbb{E}[(X-X_{n})Y_{n}]\right|+\left|\mathbb{E}[(X-X_{n})(Y-Y_{n})]\right|.\end{align*}\]</span>
Then we apply <a href="ch-vecMat.html#sub:vec">Schwarz inequality</a> to the last three terms and take the limit
<span class="math display">\[\begin{align*}
&amp;\lim_{n\rightarrow\infty}\left|\mathbb{E}[XY]-\mathbb{E}[X_{n}Y_{n}]\right|    \leq
\lim_{n\rightarrow\infty}   \mathbb{E}\left|Y-Y_{n}|\mathbb{E}|X_{n}\right|\\
+&amp;\mathbb{E}\left|X-X_{n}| \mathbb{E}|Y_{n}\right|+\mathbb{E}\left|X-X_{n}|\mathbb{E}|Y-Y_{n}\right|\rightarrow0.\end{align*}\]</span>
The result follows.</p>
<ol start="5" style="list-style-type: decimal">
<li>In the view of the <a href="sub-continuity.html#sub:Cauchy">Cauchy criterion</a>, the <strong>mean square differentiability</strong> is about the convergence of the infinitesimal stochastic sequence <span class="math inline">\(\left\{\frac{X_{t+h}-X_{t}}{h}\right\}\)</span> such that <span class="math display">\[\lim_{h,s\rightarrow0}\mathbb{E}\left[\left|\frac{X_{t+h}-X_{t}}{h}-\frac{X_{t+s}-X_{t}}{s}\right|^{2}\right]=0.\]</span> We can show that the convergence holds if <span class="math inline">\(\frac{\partial^{2}\mathbb{E}[X_{t}(\omega)X_{s}(\omega)]}{\partial t\partial s}\)</span> exists.</li>
</ol>
<p>Proof: Because <span class="math display">\[\begin{align*}\mathbb{E}\left[\left|\frac{X_{t+h}-X_{t}}{h}-\frac{X_{t+s}-X_{t}}{s}\right|^{2}\right]&amp;=\mathbb{E}\left[\frac{X_{t+h}-X_{t}}{h}\frac{X_{t+h}-X_{t}}{h}\right]\\-2\mathbb{E}\left[\frac{X_{t+h}-X_{t}}{h}\frac{X_{t+s}-X_{t}}{s}\right]&amp;+\mathbb{E}\left[\frac{X_{t+s}-X_{t}}{s}\frac{X_{t+s}-X_{t}}{s}\right].\end{align*}\]</span></p>
<p>Notice that <span class="math display">\[\lim_{h,s\rightarrow0}\mathbb{E}\left[\frac{X_{t+h}-X_{t}}{h}\times\frac{X_{t+s}-X_{t}}{s}\right] =\lim_{h,s\rightarrow0}\frac{1}{hs}\left[\mathbb{E}[X_{t+h}X_{t+s}]\\-\mathbb{E}[X_{t}X_{t+s}]-(\mathbb{E}[X_{t+h}X_{t}]-\mathbb{E}[X_{t}X_{t}])\right]=\frac{\partial^{2}\mathbb{E}[X_{t}X_{t}]}{\partial t\partial t}.\]</span></p>
<p>Similarly,
<span class="math display">\[\begin{align*}\lim_{h\rightarrow0}\mathbb{E}\left[\frac{X_{t+h}-X_{t}}{h}\frac{X_{t+h}-X_{t}}{h}\right]&amp;=\lim_{s\rightarrow0}\mathbb{E}\left[\frac{X_{t+h}-X_{t}}{h}\frac{X_{t+h}-X_{t}}{h}\right]\\&amp;=\frac{\partial^{2}\mathbb{E}[X_{t}X_{t}]}{\partial t\partial t}.\end{align*}\]</span>
So when the second derivatives of <span class="math inline">\(\mathbb{E}[X_{t}X_{t}]\)</span> exists, the sequence <span class="math inline">\(\left\{\frac{X_{t+h}-X_{t}}{h}\right\}\)</span> <strong>converges in mean square</strong> to <span class="math inline">\(\mbox{d}X_{t}/\mbox{d}t\)</span>.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>With the <strong>mean square calculus</strong>, we can define a <strong>white noise process</strong>, a baseline of modeling unpredictable fluctuation in the <strong>second-order processes</strong>. (See figure <a href="ch-randomization.html#fig:GP">16.2</a>.)</p>
<p>Let <span class="math inline">\(\varepsilon(t,\omega)\)</span> denote the <strong>white noise</strong>. The process has zero <strong>mean function</strong> and its <strong>covariance function</strong> is <span class="math display">\[\mbox{Cov}_{\varepsilon}(t,s)=\sigma^{2}\delta(t-s)\]</span> where <span class="math inline">\(\delta(\cdot)\)</span> is called the <em>delta function</em> such that <span class="math display">\[\begin{align*}\delta(x)=\begin{cases}
\infty, &amp; x=0,\\
0, &amp; x\neq0,
\end{cases} &amp; \; \mbox{ and }\\ 
\int_{-\infty}^{\infty}\delta(x)\mbox{d}x=1,&amp; \,\, \int_{-\infty}^{\infty}f(x)\delta(x)\mbox{d}x=f(0)
\end{align*}\]</span>
for any continuous function <span class="math inline">\(f(\cdot)\)</span>. Basically, a <strong>delta function</strong> is an infinite “spike” above a single point.<label for="tufte-sn-433" class="margin-toggle sidenote-number">433</label><input type="checkbox" id="tufte-sn-433" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">433</span> One can shift the origin to any point <span class="math inline">\(t\)</span>
by setting the argument to <span class="math inline">\(x-t\)</span> such that <span class="math display">\[\delta(x-t)=\begin{cases}
\infty, &amp; x=t,\\
0, &amp; x\neq t.
\end{cases}\]</span></span>
The <strong>delta function</strong> can be thought of as the derivative of the step function <span class="math inline">\(\mathbf{1}_{\{x&gt;0\}}(x)\)</span> that has zero everywhere and goes to infinity at <span class="math inline">\(0\)</span>.<label for="tufte-sn-434" class="margin-toggle sidenote-number">434</label><input type="checkbox" id="tufte-sn-434" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">434</span> Note that for any <span class="math inline">\(\epsilon&gt;0\)</span>, the derivative of the step function is <span class="math display">\[\begin{align*}
&amp;\lim_{\epsilon\rightarrow0}\frac{\mathbf{1}_{\{x&gt;0\}}(x+\epsilon)-\mathbf{1}_{\{x&gt;0\}}(x)}{\epsilon}=\\
&amp;\begin{cases}
\lim_{\epsilon\rightarrow0}\frac{1}{\epsilon}\rightarrow\infty &amp; x=0\\
\lim_{\epsilon\rightarrow0}\frac{0}{\epsilon}\rightarrow 0 &amp; x\neq 0
\end{cases}.\end{align*}\]</span></span></p>
<p>For a <strong>white noise process</strong>, if the probability law in the <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong> is Gaussian, then the <strong>white noise</strong> is called <em>Gaussian white noise</em>,<label for="tufte-sn-435" class="margin-toggle sidenote-number">435</label><input type="checkbox" id="tufte-sn-435" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">435</span> Notice that the inner product <span class="math inline">\(\langle f,\delta \rangle=f(0)\)</span> is well-defined. So the <strong>delta function</strong> is somehow similar to the <a href="sub-set-theory.html#sub:func">indicator function</a> at the origin as <span class="math display">\[\int_{-\infty}^{\infty}f(x)\mathbf{1}_{\{x=0\}}(x)\mbox{d}x=f(0).\]</span> In practice, one implements <span class="math inline">\(\mathbf{1}_{\{x=t\}}(x)\)</span>
instead of <span class="math inline">\(\delta(x-t)\)</span> as a (numerical) <strong>delta function</strong>, because the infinite value of the <strong>delta function</strong> is impossible for the implementation.</span> <span class="math display">\[\varepsilon(t,\omega)\sim\mathcal{N}(0,\sigma^{2}\delta(x-t)).\]</span></p>
<p>The <strong>Gaussian white noise</strong> <span class="math inline">\(\varepsilon(t,\omega)\)</span> corresponds to the <strong>mean square derivative</strong> of a <a href="ch-DE.html#sub:pde">Brownian motion</a> <span class="math inline">\(X(t,\omega)\)</span>:
<span class="math display">\[\varepsilon(t,\omega)=\frac{\mbox{d}X(t,\omega)}{\mbox{d}t}.\]</span>
To see the connection, we analyze the <strong>mean and covariance functions</strong> of both processes. The <strong>mean functions</strong> of both white noise and Brownian motion are zero. So the <strong>mean square derivative</strong> does not change anything for the mean. The <strong>covariance function</strong> of the <a href="ch-DE.html#sub:pde">Brownian motion</a> is <span class="math inline">\(\mbox{Cov}_{X}(t,s)=\sigma^{2}\min(t,s)\)</span>. According to the <strong>mean square differentiability</strong>, we should check the second derivative of this function, and we can see it equals to the <strong>covariance function</strong> of the <strong>white noise</strong><label for="tufte-sn-436" class="margin-toggle sidenote-number">436</label><input type="checkbox" id="tufte-sn-436" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">436</span> Note that<span class="math display">\[\min(t,s)=\begin{cases}
s, &amp; s&lt;t\\
t, &amp; s&gt;t
\end{cases},\\
\frac{\partial\min(t,s)}{\partial t}=\mathbf{1}_{\{s&gt;t\}}(s)=\begin{cases}
0, &amp; s&lt;t\\
1, &amp; s&gt;t
\end{cases}.\]</span>
That is, the first derivative of <span class="math inline">\(\min(t,s)\)</span> is the step function <span class="math inline">\(\mathbf{1}_{\{s&gt;t\}}(s)\)</span>. As we know, the derivative of a step function is the <strong>delta function</strong>. The result follows.</span> <span class="math display">\[\mbox{Cov}_{\varepsilon}(t,s)=\frac{\partial^{2}\mbox{Cov}_{B}(t,s)}{\partial t\partial s}=\sigma^{2}\frac{\partial^{2}\min(t,s)}{\partial t\partial s}=\sigma^{2}\delta(t-s).\]</span>
Thus, we have the result that the <strong>mean square differentiation</strong> of a <a href="ch-DE.html#sub:pde">Brownian motion</a> is a <strong>white noise process</strong>.<label for="tufte-sn-437" class="margin-toggle sidenote-number">437</label><input type="checkbox" id="tufte-sn-437" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">437</span> The discretization of the covariance functions of Brownian motion and white noise can be found in figure <a href="ch-randomization.html#fig:GPKern">16.3</a>.</span></p>
<p>Conversely, if we consider <span class="math inline">\(\int_{s}^{t}\varepsilon(\tau,\omega)\mbox{d}\tau\)</span> to be a “proper” integral, by the <strong>fundamental theorem of mean square calculus</strong>, we expect to have
<span class="math display">\[\int_{s}^{t}\varepsilon(\tau,\omega)\mbox{d}\tau=X(t,\omega)-X(s,\omega)\mbox{ with probability }1,\]</span>
where <span class="math inline">\(X(t,\omega)-X(s,\omega)\)</span> is also called the <em>increment</em> of <a href="ch-DE.html#sub:pde">Brownian motion</a>.<label for="tufte-sn-438" class="margin-toggle sidenote-number">438</label><input type="checkbox" id="tufte-sn-438" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">438</span> The “integral” <span class="math inline">\(\int_{s}^{t}\varepsilon(\tau,\omega)\mbox{d}\tau\)</span>, however, is not technically well defined over here. A precise definition of the integral is not trivial, because some quantities that are finite sums in the finite discrete case must be replaced by integrals that may not converge in the <strong>mean square</strong> sense. Thus, instead of dealing with the rigorous definition, here we only consider a vague presentation of the result.</span>
The <strong>covariance function</strong> of the <strong>increment of Brownian motion</strong> <span class="math inline">\(X(t,\omega)-X(s,\omega)\)</span> follows the probability law <span class="math inline">\(\mathcal{N}(0,(t-s)\sigma^{2})\)</span>, which is what we expect to see on the “integral” of the <strong>white noise process</strong>.<label for="tufte-sn-439" class="margin-toggle sidenote-number">439</label><input type="checkbox" id="tufte-sn-439" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">439</span> The fact is that <a href="ch-DE.html#sub:pde">Brownian motion</a> is a mathematically tractable version of the idea of the <a href="ch-UnMulti.html#sub:rw">random walk</a> in the continuous time setting. Intuitively, one can think of <a href="ch-UnMulti.html#sub:rw">random walks</a> <span class="math inline">\(X_{s}=\varepsilon_{1}+\cdots+\varepsilon_{s}\)</span>
and <span class="math inline">\(X_{t}=\varepsilon_{1}+\cdots+\varepsilon_{t}\)</span>. Then <span class="math display">\[X_{t}-X_{s}=\underset{(t-s)-\mbox{entities}}{\underbrace{\varepsilon_{t}+\varepsilon_{t-1}+\cdots+\varepsilon_{s}}}.\]</span> For each <span class="math inline">\(\varepsilon_{i}\sim\mathcal{N}(0,\sigma^{2})\)</span>, the <a href="ch-CalUn.html#sub:divRV">Gaussian property</a> tells that <span class="math display">\[\sum_{i=s}^{t}\varepsilon_{i}\sim\mathcal{N}(0,(t-s)\sigma^{2}).\]</span></span></p>
</div>
<div id="sub:SumDecomp" class="section level2">
<h2>
<span class="header-section-number">16.2</span> Direct Sums and Decompositions</h2>
<p>When we compare the <a href="ch-representation.html#ch:representation">representation</a> of a <a href="ch-UnMulti.html#sub:rw">random walk</a> <span class="math inline">\(X_{t}(\omega)\in\mathbb{R}\)</span> and that of a vector <span class="math inline">\(\mathbf{x}\in\mathbb{R}^{t}\)</span>, we can find some interesting similarities:
<span class="math display">\[\begin{align*}X_{t}(\omega) &amp;=    \varepsilon_{1}(\omega)+\cdots+\varepsilon_{t}(\omega), \, \varepsilon_{i} \sim \mathcal{N}(0,1), \\
\left\langle \varepsilon_{i}(\omega),\varepsilon_{j}(\omega)\right\rangle _{\mathbb{P}}&amp;=\mathbb{E}\left[\left\langle \varepsilon_{i}(\omega),\varepsilon_{j}(\omega)\right\rangle \right]  =\begin{cases}
0, &amp; i\neq j\\
1, &amp; i=j
\end{cases},\\
\mathbf{x} &amp;=   c_{1}\mathbf{e}_{1}+\cdots+c_{n}\mathbf{e}_{t},\\
\left\langle \mathbf{e}_{i},\mathbf{e}_{j}\right\rangle &amp;=\begin{cases}
0, &amp; i\neq j\\
1, &amp; i=j
\end{cases}.\end{align*}\]</span>
The (discrete time) <a href="ch-randomization.html#sub:RHilbert">Gaussian white noise</a> <span class="math inline">\(\varepsilon_i(\omega)\)</span> with unit variance looks like a “whitened” <a href="ch-vecMat.html#sub:vec">unit vector</a> <span class="math inline">\(\mathbf{e}_i\)</span>. Moreover, the collection of white noises seems to play the same role as the <a href="ch-UnMulti.html#sub:MultiVar">orthonormal basis</a> to the random walk <span class="math inline">\(X_{t}(\omega)\)</span> in a <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a>.</p>
<p>Given this result, a natural question is to ask whether we can represent any <a href="ch-randomization.html#sub:RHilbert">second-order</a> <span class="math inline">\(\mathcal{H}\)</span>-valued time series or stochastic process in terms of white noises? If this is the case, then we can think the <a href="ch-randomization.html#sub:RHilbert">randomization</a> of the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a> is about randomizing the Hilbert space <span class="math inline">\(\mathcal{H}\)</span> through “whitening” the <a href="ch-representation.html#sub:innerProd">basis</a> of <span class="math inline">\(\mathcal{H}\)</span>.</p>
<p>The route of our exposition will start with defining how to construct a space by many <a href="ch-MatComp.html#sub:vecSpaces">subspaces</a>; then we will study the subspaces generated by white noises and will see what kind of processes can live in the “combination” of those subspaces. The switch of the focus from white noises to the space of white noises will help us to bridge the difference between the <a href="ch-representation.html#ch:representation">representations</a> of the stochastic objects and of the deterministic ones.</p>
<p>Now we start with introducing the way of “combining” the <a href="ch-MatComp.html#sub:vecSpaces">subspaces</a>.</p>
<ul>
<li>
<strong>Direct sums, decompositions</strong> and <strong>complements</strong> : Consider two <a href="ch-MatComp.html#sub:vecSpaces">vector spaces</a> <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> over the same <a href="ch-MatComp.html#sub:vecSpaces">scalar field</a> <span class="math inline">\(\mathbb{F}\)</span>. When the two spaces can be combined to form a new vector space <span class="math inline">\(\mathcal{V}\)</span>, then <span class="math inline">\(\mathcal{V}\)</span> is called the <em>direct sum</em> of <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span>, denoted by <span class="math display">\[\mathcal{V}=\mathcal{V}_{1}+\mathcal{V}_{2}.\]</span>
If <span class="math inline">\(\mathcal{V}\)</span> is the <strong>direct sum</strong> of <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span>, then all objects in <span class="math inline">\(\mathcal{V}\)</span> can be <em>decompose</em> into those from <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> such that for any <span class="math inline">\(f\in\mathcal{V}\)</span>, there is <span class="math inline">\(f=f_{1}+f_{2}\)</span> where <span class="math inline">\(f_{1}\in\mathcal{V}_{1}\)</span> and <span class="math inline">\(f_{2}\in\mathcal{V}_{2}\)</span>, and we say <span class="math inline">\(\mathcal{V}_1\)</span> and <span class="math inline">\(\mathcal{V}_2\)</span> are <em>complements</em> of each other. In addition, if <span class="math inline">\(\mathcal{V}_{1}\)</span>
and <span class="math inline">\(\mathcal{V}_{2}\)</span> are <a href="ch-representation.html#sub:conjugacy">Hilbert spaces</a>, and they are <em>disjoint</em><label for="tufte-sn-440" class="margin-toggle sidenote-number">440</label><input type="checkbox" id="tufte-sn-440" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">440</span> The disjoint property tells that the <strong>direct sum</strong> can be uniquely <strong>decomposed</strong>. The proof is given below.</span> and <em>orthogonal</em><label for="tufte-sn-441" class="margin-toggle sidenote-number">441</label><input type="checkbox" id="tufte-sn-441" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">441</span> The orthogonality of the subspaces <span class="math inline">\(\mathcal{V}_1 \bot \mathcal{V}_2\)</span> are defined as <span class="math display">\[\langle f,g\rangle=0\]</span> for all <span class="math inline">\(f\in\mathcal{V}_1\)</span> and <span class="math inline">\(g\in\mathcal{V}_2\)</span>.</span>, we say <span class="math inline">\(\mathcal{V}\)</span> is the <em>orthogonal direct sum</em> of <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span>, denoted by <span class="math display">\[\mathcal{V}=\mathcal{V}_{1}\oplus\mathcal{V}_{2},\]</span> where <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> are <em>orthogonal complements</em> of one another.<label for="tufte-sn-442" class="margin-toggle sidenote-number">442</label><input type="checkbox" id="tufte-sn-442" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">442</span> The <strong>orthogonal complement</strong> of <span class="math inline">\(\mathcal{V}_1\subset\mathcal{V}\)</span> is denoted by <span class="math inline">\(\mathcal{V}_1^{\bot}\)</span>. If <span class="math inline">\(\mathcal{V}=\mathcal{V}_{1}\oplus\mathcal{V}_{2}\)</span>, then <span class="math inline">\(\mathcal{V}_2 = \mathcal{V}_1^{\bot}\)</span>.</span>
</li>
</ul>
<div class="solution">
<p class="solution-begin">
Proof <span id="sol-start-189" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-189', 'sol-start-189')"></span>
</p>
<div id="sol-body-189" class="solution-body" style="display: none;">
<p>For two subspaces <span class="math inline">\(\mathcal{V}_1\)</span> and <span class="math inline">\(\mathcal{V}_2\)</span>, the following two statements are equivalent:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(f\in \mathcal{V}_{1} + \mathcal{V}_{2}\)</span>, and <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> are disjoint.</p></li>
<li><p>There is a unique <span class="math inline">\(f_{1}\in\mathcal{V}_{1}\)</span> and a unique <span class="math inline">\(f_{2}\in\mathcal{V}_{2}\)</span> to <strong>decompose</strong> <span class="math inline">\(f\)</span>: <span class="math display">\[f=f_{1}+f_{2}.\]</span></p></li>
</ol>
<p>Proof:</p>
<ol style="list-style-type: decimal">
<li>Suppose that <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> are disjoint. Let <span class="math display">\[f=f_{1}+f_{2}=g_{1}+g_{2},\]</span> where <span class="math inline">\(f_{1},g_{1}\in\mathcal{V}_{1}\)</span> and <span class="math inline">\(f_{2},g_{2}\in\mathcal{V}_{2}\)</span>. So we have <span class="math inline">\(f_{1}-g_{1}=g_{2}-f_{2}\)</span>. By the <a href="ch-MatComp.html#sub:vecSpaces">subspace property</a>, we have <span class="math inline">\(f_{1}-g_{1}\in\mathcal{V}_{1}\)</span> and <span class="math inline">\(f_{2}-g_{2}\in\mathcal{V}_{2}\)</span>. By the disjoint condition <span class="math inline">\(\mathcal{V}_{1}\cap\mathcal{V}_{2}=\{0\}\)</span>, one can induce that <span class="math display">\[f_{1}-g_{1}=g_{2}-f_{2}=0,\]</span> which implies <span class="math inline">\(f_{1}=g_{1}\)</span> and <span class="math inline">\(g_{2}=f_{2}\)</span>.</li>
</ol>
<p>2: Now suppose that <span class="math inline">\(f_{1}\)</span> and <span class="math inline">\(f_{2}\)</span> are uniquely determined <span class="math inline">\(f=f_{1}+f_{2}\)</span> in <span class="math inline">\(\mathcal{V}_{1}+\mathcal{V}_{2}\)</span>, we need to show that <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> are disjoint. Suppose that <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> are not disjoint, then there exists <span class="math inline">\(g\neq0\)</span> in <span class="math inline">\(\mathcal{V}_{1}\cap\mathcal{V}_{2}\)</span>. Then for any <span class="math inline">\(f=f_{1}+f_{2}\)</span>, we have
<span class="math display">\[f=(f_{1}+cg)+(f_{2}-cg)\]</span>
for any scalar <span class="math inline">\(c\)</span>. But then <span class="math inline">\(f\)</span> is not uniquely determined, a contradiction.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Let’s consider three examples of “summing” the subspaces or “decomposing” a space.</p>
<p><span class="newthought">Range space and null space </span></p>
<p>For a non-square matrix <span class="math inline">\(\mathbf{A}\in\mathbb{F}^{n\times m}\)</span>, the <em>column space</em> (or called <em>range space</em> when we consider the operator rather than the matrix) of <span class="math inline">\(\mathbf{A}\)</span> is defined by:<label for="tufte-sn-443" class="margin-toggle sidenote-number">443</label><input type="checkbox" id="tufte-sn-443" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">443</span> Notice that the <strong>column/range space</strong> <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span> is the <a href="sub-set-theory.html#sub:func">image</a> of the linear transformation <span class="math inline">\(\mathbf{A}\)</span>.</span>
<span class="math display">\[\mbox{Range}(\mathbf{A}):=\left\{ \mathbf{y}\in\mathbb{F}^{n}:\,\mathbf{y}=\mathbf{A}\mathbf{x},\mathbf{x}\in\mathbb{F}^{m},\mathbf{A}\in\mathbb{F}^{n\times m}\right\}.\]</span>
The solution of the system <span class="math inline">\(\mathbf{b}=\mathbf{A}\mathbf{x}\)</span> exists only if <span class="math inline">\(\mathbf{b}\)</span> is in <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span>, i.e., <span class="math inline">\(\mathbf{b}\in\mbox{Range}(\mathbf{A})\)</span>. The <strong>column space</strong> <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span> contains all the <a href="ch-vecMat.html#sub:vec">linear combinations</a> of the columns of <span class="math inline">\(\mathbf{A}\)</span>. It is a <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> of <span class="math inline">\(\mathbb{F}^{n}\)</span>.</p>
<p>The <em>null space</em> is <a href="ch-vecMat.html#sub:vec">orthogonal</a> to <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span><label for="tufte-sn-444" class="margin-toggle sidenote-number">444</label><input type="checkbox" id="tufte-sn-444" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">444</span> To see the orthogonality, for any <span class="math inline">\(\mathbf{x}\in\mbox{Range}(\mathbf{A})\)</span>, there is
<span class="math display">\[\left\langle \mathbf{A}\mathbf{x},\,\mathbf{y}\right\rangle =\mathbf{x}^{\mbox{H}}\mathbf{A}^{\mbox{H}}\mathbf{y}=0\]</span> for any <span class="math inline">\(\mathbf{y}\in\mbox{Null}(\mathbf{A}^{\mbox{H}})\)</span>.</span>
<span class="math display">\[\mbox{Null}(\mathbf{A}^{\mbox{H}}):=\left\{ \mathbf{y}\in\mathbb{F}^{n}:\mathbf{A}^{\mbox{H}}\mathbf{y}=0,\,\mathbf{A}^{\mbox{H}}\in\mathbb{F}^{m\times n}\right\}\]</span> The <strong>null space</strong> of <span class="math inline">\(\mathbf{A}^{\mbox{H}}\)</span> consists of all solutions <span class="math inline">\(\mathbf{A}^{\mbox{H}}\mathbf{y}=0\)</span>, it is also a <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> of <span class="math inline">\(\mathbb{F}^{n}\)</span>.</p>
<p>The <strong>null space</strong> and <strong>column space</strong> are <strong>orthogonal complements</strong> such that:
<span class="math display">\[\mbox{Range}(\mathbf{A})\oplus\mbox{Null}(\mathbf{A}^{\mbox{H}})=\mathbb{F}^{n}.\]</span>
This result is a generalization of the <a href="ch-optApp.html#sub:Proj1">Farkas’ lemma</a>. Basically it reveals a dichotomy about the sandwich form <span class="math inline">\(\left\langle \mathbf{x},\,\mathbf{A}^{\mbox{H}}\mathbf{y}\right\rangle\)</span> for a given system <span class="math inline">\(\mathbf{y}=\mathbf{A}\mathbf{x}\)</span>: either the sandwich form is zero or non-zero.<label for="tufte-sn-445" class="margin-toggle sidenote-number">445</label><input type="checkbox" id="tufte-sn-445" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">445</span> That is, for any <span class="math inline">\(\mathbf{x},\mathbf{y}\in\mathbb{F}^{n}\)</span>, either <span class="math inline">\(\left\langle \mathbf{x},\,\mathbf{A}^{\mbox{H}}\mathbf{y}\right\rangle =0\)</span> or
<span class="math display">\[\begin{align*}\left\langle \mathbf{x},\,\mathbf{A}^{\mbox{H}}\mathbf{y}\right\rangle &amp;=\left\langle \mathbf{A}\mathbf{x},\,\mathbf{y}\right\rangle\\
&amp;=\left\langle \mathbf{y},\,\mathbf{y}\right\rangle =\|\mathbf{y}\|^{2}\neq0.\end{align*}\]</span></span> The dichotomy is also known as the <em>Fredholm alternative</em>.</p>
<div class="solution">
<p class="solution-begin">
Fredholm alternative <span id="sol-start-190" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-190', 'sol-start-190')"></span>
</p>
<div id="sol-body-190" class="solution-body" style="display: none;">
<p>The <strong>Fredholm alternative</strong> states as follows:</p>
<p>For any <span class="math inline">\(\mathbf{y}\in\mathbb{R}^{n}\)</span> such that <span class="math inline">\(\mathbf{A}^{\top}\mathbf{y}=0\)</span>, then <span class="math inline">\(\mathbf{b}=\mathbf{A}\mathbf{x}\)</span> has a solution if and only if
<span class="math display">\[\langle\mathbf{A}\mathbf{x},\mathbf{y}\rangle=\langle\mathbf{b},\mathbf{y}\rangle=0.\]</span></p>
<p>The statement can be summarized as: the system <span class="math inline">\(\mathbf{b}=\mathbf{A}\mathbf{x}\)</span> has a solution (<span class="math inline">\(\mathbf{b}\in\mbox{Range}(\mathbf{A})\)</span> if and only if <span class="math inline">\(\langle\mathbf{b},\mathbf{y}\rangle=0\)</span> for any <span class="math inline">\(\mathbf{y}\in \mbox{Null}(\mathbf{A}^{\top})\)</span> (<span class="math inline">\(\mathbf{b}\in(\mbox{Null}(\mathbf{A}^{\top}))^{\bot}\)</span>).</p>
<p>Notice that <span class="math inline">\(\mbox{Range}(\mathbf{A})=(\mbox{Null}(\mathbf{A}^{\top}))^{\bot}\)</span>.</p>
<p>Thus, the <strong>Fredholm alternative</strong> essentially tells the dichotomy: <span class="math inline">\(\mathbf{b}\in\mbox{Range}(\mathbf{A})\)</span> (has a solution) or <span class="math inline">\(\mathbf{b}\in\mbox{Null}(\mathbf{A}^{\top})\)</span> (no solution).</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>The <strong>complementary subspaces</strong> tell that the “dimension” of the <strong>column space</strong> is not only a property of <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span> but also the property of <span class="math inline">\(\mbox{Null}(\mathbf{A}^{\mbox{H}})\)</span>.<label for="tufte-sn-446" class="margin-toggle sidenote-number">446</label><input type="checkbox" id="tufte-sn-446" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">446</span> The dimension of the space refers to the cardinal number of the <a href="ch-MatComp.html#sub:vecSpaces">basis</a> of that space.</span> That is, if the “dimension” of <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span> grows, then the “size” of <span class="math inline">\(\mbox{Null}(\mathbf{A}^{\mbox{H}})\)</span> will shrink, and vice versa. In the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a>, the zero mean <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathcal{H}\)</span>-valued random variables</a> belong to the <strong>null space</strong> of a <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a>. It turns out that the size of this <strong>null space</strong> is only one dimensional lower than the (non-randomized) <span class="math inline">\(\mathcal{H}\)</span>.<label for="tufte-sn-447" class="margin-toggle sidenote-number">447</label><input type="checkbox" id="tufte-sn-447" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">447</span> For a <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathcal{H}\)</span>-valued random variable</a>, its expectation can be thought of as a <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a>, <span class="math display">\[\begin{align*}\mathbb{E}[X(\omega)]=\int_{\Omega}x\mathbb{P}(\mbox{d}x)\\=\int_{\Omega}xf(x)\mbox{d}x=\langle x,f\rangle\end{align*}\]</span>
where <span class="math inline">\(\mathbb{P}(\mbox{d}x)=f(x)\mbox{d}x\)</span> for the density function <span class="math inline">\(f(\cdot)\)</span>. For a non-trivial <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a> <span class="math inline">\(\langle \cdot,f\rangle\)</span>, i.e., <span class="math inline">\(f\neq0\)</span>, the <strong>null space</strong> <span class="math display">\[\mbox{Null}(f)=\{x\in\mathcal{H}:\langle x,f\rangle=0\}\]</span> is a subspace of <span class="math inline">\(\mathcal{H}\)</span>. In fact, the space <span class="math inline">\(\mbox{Null}(f)\)</span> is so large that it is the “maximal” subspace of <span class="math inline">\(\mathcal{H}\)</span>, i.e., one dimensional lower than <span class="math inline">\(\mathcal{H}\)</span>.</span></p>
<div class="solution">
<p class="solution-begin">
Remarks about the null space of a linear functional <span id="sol-start-191" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-191', 'sol-start-191')"></span>
</p>
<div id="sol-body-191" class="solution-body" style="display: none;">
<p>Consider the null space of a linear functional <span class="math inline">\(g:\mathcal{V}\rightarrow\mathbb{F}\)</span> <span class="math display">\[\mbox{Null}(g):=\left\{ \langle f,\, g\rangle=0,\; f\in\mathcal{V}\right\}.\]</span></p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\mbox{Null}(g)\)</span> is one dimensional lower than the original space <span class="math inline">\(\mathcal{V}\)</span>.</li>
</ol>
<p>Proof: To see that, consider a point <span class="math inline">\(f_{0}\)</span> in <span class="math inline">\(\mathcal{V}\)</span> such that <span class="math inline">\(\langle f_{0},\, g\rangle\neq0\)</span>. Then for any point <span class="math inline">\(f\in\mathcal{V}\)</span>,
we have <span class="math display">\[f_{p}=f-\frac{\langle f,\, g\rangle}{\langle f_{0},\, g\rangle}f_{0}\]</span> where <span class="math inline">\(\langle f_{p},\, g\rangle=0\)</span> because <span class="math display">\[\langle f_{p},g\rangle=\langle f,g\rangle-\frac{\langle f,\, g\rangle}{\langle f_{0},\, g\rangle}\langle f_{0},g\rangle=0.\]</span>
That means each point <span class="math inline">\(f\in\mathcal{V}\)</span> can be expressed as the <strong>direct sum</strong> of a point in <span class="math inline">\(\mbox{Null}(g)\)</span> and a point in the one-dimensional subpsace spanned by <span class="math inline">\(f_{0}\)</span>. Since <span class="math inline">\(\mbox{span}(f_{0})\cap\mbox{ker}(g)=\emptyset\)</span>, we have <span class="math display">\[\mbox{span}(f_{0})\oplus\mbox{ker}(g)=\mathcal{V}.\]</span>
The result follows.</p>
<ol start="2" style="list-style-type: decimal">
<li><p>The inverse of the above statement is also true. That is, if a subspace whose dimension of the <strong>complement</strong> is only one, then we can use a linear functional <span class="math inline">\(g\)</span> to characterize this subspace so that the <strong>orthogonal complement</strong> subspace is equivalent to <span class="math inline">\(\mbox{Null}(g)\)</span>.</p></li>
<li><p>In chapter @ref(#sub:Proj1), we have seen that a <a href="ch-optApp.html#sub:Proj1">hyperlane</a> or (a regression) is one dimensional lower than the original space. In fact, a <a href="ch-optApp.html#sub:Proj1">hyperplane</a> can be viewed as a translated subspace of the <strong>null space</strong> of the linear functional, i.e. <span class="math inline">\(\mbox{Null}(g)\)</span>. Consider the hyperplane <span class="math inline">\(\mathcal{S}(g,c)=\{f\in\mathcal{V}:\,\langle f,g\rangle=c\}\)</span> and let <span class="math inline">\(f_{0}\)</span> be an arbitrary point in <span class="math inline">\(\mathcal{S}\)</span>
. We construct another hyperplane
<span class="math display">\[\mathcal{M}(g,c)=\{f\in\mathcal{V}:\, f+f_{0}\in\mathcal{S}(g,c)\}.\]</span>
This hyperplane is the <strong>null space</strong> of the linear functional <span class="math inline">\(g\)</span>, because <span class="math inline">\(\langle f+f_{0},g\rangle=c\)</span> in <span class="math inline">\(\mathcal{M}(g,c)\)</span> and <span class="math inline">\(\langle f_{0},g\rangle=c\)</span> in <span class="math inline">\(\mathcal{S}(g,c)\)</span>. So for any <span class="math inline">\(f\in \mathcal{M}(g,c)\)</span>, we have <span class="math display">\[\langle f_{0},g\rangle=0.\]</span> That means <span class="math inline">\(\mathcal{M}(g,c)=\mbox{Null}(g)\)</span>.</p></li>
</ol>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p><span class="newthought">Future and past space </span></p>
<p>Let’s look at another example. Consider a vector space linearly generated by <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathcal{H}\)</span>-valued time series</a> <span class="math inline">\(X_{t}(\omega)\)</span> such that
<span class="math display">\[\mbox{span}\left\{ X_{t}(\omega)\right\} =\left\{ \sum_{t}\phi(t)X_{t}(\omega),\,\phi\in\mathcal{H} \right\}.\]</span>
By <em>closing</em> this space, i.e., adding all <a href="ch-randomization.html#sub:RHilbert">mean square limits</a> of all sequences of <span class="math inline">\(\sum_{t}\phi(t)X_{t}(\omega)\)</span>, we will obtain a <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a> generated by the series <span class="math inline">\(X_{t}(\omega)\)</span>, denoted by <span class="math display">\[\mathcal{H}_{X(\omega)}=\overline{\mbox{span}}\left\{ \left.X_{t}(\omega)\right|t\in(-\infty,\infty)\right\},\]</span>
where <span class="math inline">\(\overline{\mbox{span}}\)</span> says that the <a href="">span</a> is <strong>closed</strong>.<label for="tufte-sn-448" class="margin-toggle sidenote-number">448</label><input type="checkbox" id="tufte-sn-448" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">448</span> The extension of the span to the continuous time requires more technicalities and is beyond our current concerns.</span></p>
<p>The space <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span> can be decomposed into two subspaces of interests. They are representing the past and the future space generated by <span class="math inline">\(X_t(\omega)\)</span> respectively:<label for="tufte-sn-449" class="margin-toggle sidenote-number">449</label><input type="checkbox" id="tufte-sn-449" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">449</span> The convention is to include the present in the future only and not in the past.</span>
<span class="math display">\[\mathcal{H}_{X(\omega)}^{t-}=\overline{\mbox{span}}\left\{ \left.X_{s}(\omega)\right|s&lt;t\right\} ,\;\mathcal{H}_{X(\omega)}^{t+}=\overline{\mbox{span}}\left\{ \left.X_{s}(\omega)\right|s\geq t\right\}.\]</span>
Combing the past and the future gives us the whole space, namely <span class="math inline">\(\mathcal{H}_{X(\omega)}=\mathcal{H}_{X(\omega)}^{t-}+\mathcal{H}_{X(\omega)}^{t+}\)</span>. In particular, if the process <span class="math inline">\(X_t(\omega)\)</span> is <a href="ch-UnMulti.html#sub:Markov">Markovian</a>, then the future and the past are orthogonal given the present, which means given <span class="math inline">\(X_{t}(\omega)\)</span>,<label for="tufte-sn-450" class="margin-toggle sidenote-number">450</label><input type="checkbox" id="tufte-sn-450" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">450</span> The orthogonality can be justified by the conditional expectation. We will discuss it in sec[?].</span> <span class="math display">\[\mathcal{H}_{X(\omega)}=\mathcal{H}_{X(\omega)}^{t-}\oplus \mathcal{H}_{X(\omega)}^{t+}.\]</span></p>
<p><span class="newthought">Decomposition by the projection </span></p>
<p>The third example is about decomposing the space by the <a href="ch-representation.html#sub:conjugacy">projection operator</a>. Let <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}:\mathcal{H}\rightarrow \mathcal{Y}\)</span> be the <a href="ch-representation.html#sub:conjugacy">projection operator</a> defined on the Hilbert space <span class="math inline">\(\mathcal{H}\)</span>, such that <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}\mathbf{P}_{\mathcal{Y}}=\mathbf{P}_{\mathcal{Y}}\)</span>. By the property of the projection, it is straightforward to see that <span class="math inline">\(\mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span> and <span class="math inline">\(\mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span> are <strong>disjoint subspaces</strong> of <span class="math inline">\(\mathcal{H}\)</span> such that
<span class="math display">\[\mathcal{H} = \mbox{Range}(\mathbf{P}_{\mathcal{Y}}) + \mbox{Null}(\mathbf{P}_{\mathcal{Y}})=\mathcal{Y} + \mbox{Null}(\mathbf{P}_{\mathcal{Y}}).\]</span> In addition, if the projection is <a href="ch-vecMat.html#sub:vec">orthogonal</a>, then
<span class="math display">\[\mathcal{H} = \mbox{Range}(\mathbf{P})_{\mathcal{Y}} \oplus \mbox{Null}(\mathbf{P}_{\mathcal{Y}})=\mathcal{Y} \oplus \mathcal{Y}^{\bot},\]</span> and <span class="math inline">\(\|f\|^2 = \|f_1\|^2 +\|f_2\|^2\)</span> for <span class="math inline">\(f\in \mathcal{H}\)</span>, <span class="math inline">\(f_1\in \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>, <span class="math inline">\(f_2 \in \mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span>.</p>
<p>In other words, one <a href="ch-representation.html#sub:conjugacy">projection</a> can uniquely decompose the Hilbert space into two <strong>disjoint subspaces</strong>. The <strong>range space</strong> of the projection operator is simply the target subspace (<span class="math inline">\(\mathcal{Y}\)</span>) of the projection. Given a target space, there could be many projections. If we want the <strong>null space</strong> of the projection to be <a href="ch-vecMat.html#sub:vec">orthogonal</a> to the <strong>range space</strong>, then we need the <a href="ch-representation.html#sub:conjugacy">orthogonal projection</a>. Because the <strong>decomposition</strong> is unique, we expect that for any target subspace, there is one and only one <a href="ch-representation.html#sub:conjugacy">orthogonal projection</a>.<label for="tufte-sn-451" class="margin-toggle sidenote-number">451</label><input type="checkbox" id="tufte-sn-451" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">451</span> In fact, if <span class="math inline">\(\mathcal{A}\)</span> is any <strong>closed</strong> subspace of a Hilbert space <span class="math inline">\(\mathcal{H}\)</span>. Then <span class="math inline">\(\mathcal{H}=\mathcal{A}\oplus\mathcal{A}^{\bot}\)</span>. The proof is given below.</span></p>
<div class="solution">
<p class="solution-begin">
Proof <span id="sol-start-192" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-192', 'sol-start-192')"></span>
</p>
<div id="sol-body-192" class="solution-body" style="display: none;">
<p>Let <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}:\mathcal{H}\rightarrow \mathcal{Y}\)</span> be the <a href="ch-representation.html#sub:conjugacy">projection operator</a> defined on the Hilbert space <span class="math inline">\(\mathcal{H}\)</span>, such that <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}\mathbf{P}_{\mathcal{Y}}=\mathbf{P}_{\mathcal{Y}}\)</span>.</p>
<p>To show that <span class="math inline">\(\mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span> and <span class="math inline">\(\mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>, are <strong>disjoint subspaces</strong>, let <span class="math inline">\(f\in \mbox{Null}(\mathbf{P}_{\mathcal{Y}}) \cap \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>. Because <span class="math inline">\(f\in \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span> then there must be some <span class="math inline">\(g \in \mathcal{H}\)</span> such that <span class="math inline">\(\mathbf{P}_{\mathcal{Y}} (g) =f\)</span>. By the projection property, we know that <span class="math display">\[\mathbf{P}_{\mathcal{Y}}\mathbf{P}_{\mathcal{Y}} (g) = \mathbf{P}_{\mathcal{Y}} f =f.\]</span> Also, because <span class="math inline">\(f\in \mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span>, <span class="math inline">\(\mathbf{P}_{\mathcal{Y}} f=0\)</span>. That tells <span class="math inline">\(f=\mathbf{P}_{\mathcal{Y}} f=0\)</span>. Since <span class="math inline">\(f\)</span> is any object in <span class="math inline">\(\mbox{Null}(\mathbf{P}_{\mathcal{Y}}) \cap \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>, we have <span class="math inline">\(\mbox{Null}(\mathbf{P}_{\mathcal{Y}}) \cap \mbox{Range}(\mathbf{P}_{\mathcal{Y}})=\{0\}\)</span>.</p>
<p>Now consider the orthogonal projection.
Firstly, note that <span class="math inline">\(\mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span> and <span class="math inline">\(\mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span> are <strong>null spaces</strong> of <span class="math inline">\(\mathbf{I}-\mathbf{P}_{\mathcal{Y}}\)</span> and <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}\)</span> where <span class="math inline">\(\mathbf{I}\)</span> is an identical operator/matrix. That says
<span class="math display">\[\langle \mathbf{P}_{\mathcal{Y}}, \mathbf{I}-\mathbf{P}_{\mathcal{Y}}\rangle =0.\]</span> Thus<br><span class="math display">\[\mbox{Range}(\mathbf{P}_{\mathcal{Y}}) = \mbox{Null}(\mathbf{I}-\mathbf{P}_{\mathcal{Y}}) = \mbox{Null}(\mathbf{P}_{\mathcal{Y}})^{\bot}.\]</span>
Secondly, note that any <span class="math inline">\(f\in\mathcal{H}\)</span> can be uniquely decompose as <span class="math display">\[f=f_1 + f_2\]</span> for <span class="math inline">\(f_1\in \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>, <span class="math inline">\(f_2 \in \mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span>. Since <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}\)</span> is orthogonal, we have <span class="math inline">\(\langle f_1, f_2\rangle =0\)</span> for any <span class="math inline">\(f_1\in \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>, <span class="math inline">\(f_2 \in \mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span>. By Pythagorean theorem, we know that <span class="math inline">\(\|f\|^2 = \|f_1\|^2 +\|f_2\|^2\)</span>. The result follows.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Given the identical and independent property of the white noises, one should expect that the spaces of white noises share some invariant properties.</p>
<p>Recall that in the deterministic setting, stationarity refers to some equilibrium status, in which the dynamics remain invariant. The dynamics of the randomized models should be described by the probability laws. For <a href="ch-UnMulti.html#sub:Markov">Markov processes</a>, we have seen the invariance probability law is about the unchange of <a href="ch-UnMulti.html#sub:Markov">transition probabilities</a>. While for the <a href="ch-randomization.html#sub:RHilbert">second-order processes</a>, since the probability law is reduced to the first- and the second-order information, we can have define a new kind of invariance in a wider sense, called the <strong>weak (or wide sense) stationarity</strong>.</p>
<ul>
<li>
<strong>Weak (or wide sense) stationarity</strong> : A <a href="ch-randomization.html#sub:RHilbert">second-order process</a> <span class="math inline">\(X(t,\omega)\)</span> is <em>weakly stationary</em> if its <a href="ch-randomization.html#sub:RHilbert">covariance function</a> satisfies <span class="math display">\[\mbox{Cov}_X (s+t,s)= \mbox{Cov}_X (t,0),\]</span> for any <span class="math inline">\(s,t\in\mathbb{R}\)</span>, and its <a href="ch-randomization.html#sub:RHilbert">mean function</a> <span class="math inline">\(\mu_X(t) = \mu_X\)</span> where <span class="math inline">\(\mu_X\)</span> is a constant independent of time.</li>
</ul>
<p>The above defintion of the <a href="ch-randomization.html#sub:RHilbert">covariance function</a> says that only the difference between function’s two arguments matters, namely <span class="math display">\[\mbox{Cov}_X (t_1,t_2)= \mbox{Cov}_X (t_1 - t_2).\]</span></p>
<p>Another way to understand the definition is to see that, for a <strong>weakly stationary</strong> process, if one shifts the process by <span class="math inline">\(s\)</span>-unit of time, the first- and the second-order structures of the process remain invariant.
Recall that shifting the process backward is conducted by the <a href="ch-eigen.html#sub:matNorms">lagged operator</a> <span class="math inline">\(\mathbf{L}\)</span>. For a <strong>weak stationary</strong> time series <span class="math inline">\(X(\omega)_{t}\)</span>, we have the following equalities in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a><label for="tufte-sn-452" class="margin-toggle sidenote-number">452</label><input type="checkbox" id="tufte-sn-452" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">452</span> Note that the covariance function can be written in terms of an inner product in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a> <span class="math display">\[\begin{align*}\mathbb{E}[(X_{t}(\omega)-\mu_{X})(X_{t-1}(\omega)-\mu_{X})]\\=\langle X_{t}(\omega)-\mu_{X},X_{t-1}(\omega)-\mu_{X}\rangle_{\mathbb{P}}.\end{align*}\]</span>
For the <a href="ch-eigen.html#sub:matNorms">lagged operator</a>, <span class="math display">\[\mathbf{L}(X_{t}(\omega) - \mu_{X})=X_{t-1}(\omega)-\mu_X\]</span> because <span class="math inline">\(\mu_X\)</span> is a constant independent of time.</span>:
<span class="math display">\[\begin{align*}\mbox{Cov}_X (t,t-1) &amp;= \langle X_{t}(\omega) - \mu_{X},X_{t-1}(\omega)-\mu_{X}\rangle_{\mathbb{P}} \\
&amp;=\langle X_{t}(\omega) - \mu_{X},\mathbf{L}(X_{t}(\omega) - \mu_{X})\rangle_{\mathbb{P}}\\
&amp;=\langle\mathbf{L}^{\mbox{H}}(X_{t}(\omega)-\mu_X),X_{t}(\omega)-\mu_X\rangle_{\mathbb{P}}\\
&amp;=\langle X_{t+1}(\omega)-\mu_X,X_{t}(\omega)-\mu_{X}\rangle_{\mathbb{P}}\\
&amp;= \mbox{Cov}_X (t+1,t) \end{align*}\]</span>
where <span class="math inline">\(\mathbf{L}^{\mbox{H}}\)</span> is the <a href="ch-representation.html#sub:conjugacy">adjoint operator</a> of <span class="math inline">\(\mathbf{L}\)</span>, meaning a forward shift.</p>
<p>In other words, the <strong>weak stationarity</strong> is about endowing the lagged operator <span class="math inline">\(\mathbf{L}\)</span> the <a href="ch-representation.html#sub:dualBasis">unitary</a> property, as the operators perserve the value of the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathbb{P}\)</span>-inner product</a> (called <em>isometry</em>)<label for="tufte-sn-453" class="margin-toggle sidenote-number">453</label><input type="checkbox" id="tufte-sn-453" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">453</span> By the equality
<span class="math display">\[\mbox{Cov}_X (t,s)= \mathbb{E}[X_t(\omega)X_s(\omega)] -\mu_{X}^2,\]</span> we know that only the term <span class="math display">\[\mathbb{E}[X_t(\omega)X_s(\omega)]=\langle X_{t}(\omega),X_{s}(\omega)\rangle_{\mathbb{P}}\]</span> influences the covariance function. So we just need to consider <span class="math display">\[\langle X_{t}(\omega),X_{t-1}(\omega)\rangle_{\mathbb{P}}=\langle X_{t+1}(\omega),X_{t}(\omega)\rangle_{\mathbb{P}}.\]</span></span>
<span class="math display">\[\begin{align*}\langle\mathbf{L}X_{t+1}(\omega),\mathbf{L}X_{t}(\omega)\rangle_{\mathbb{P}}&amp;=\langle X_{t}(\omega),X_{t-1}(\omega)\rangle_{\mathbb{P}}=\\\langle X_{t+1}(\omega),\mathbf{L}^{\mbox{H}}\mathbf{L}X_{t}(\omega)\rangle_{\mathbb{P}}&amp;=\langle X_{t+1}(\omega),X_{t}(\omega)\rangle_{\mathbb{P}},\end{align*}\]</span>
and <span class="math inline">\(\mathbf{L}^{\mbox{H}}\mathbf{L}=\mathbf{I}\)</span> or say <span class="math inline">\(\mathbf{L}^{\mbox{H}}=\mathbf{L}^{-1}\)</span>.</p>
<p>For an <a href="ch-MatComp.html#sub:vecSpaces">operator</a> <span class="math inline">\(\mathbf{T}\)</span>, we say that the operator induces an <em>invariant subspace</em> <span class="math inline">\(\mathcal{V}\)</span>, if <span class="math inline">\(\mathbf{T}(\mathcal{V})\subset\mathcal{V}\)</span>. Therefore, the <strong>weak stationarity</strong> is to empower the <a href="ch-eigen.html#sub:matNorms">lagged operator</a> <span class="math inline">\(\mathbf{L}\)</span> and its <a href="ch-representation.html#sub:conjugacy">adjoint</a> <span class="math inline">\(\mathbf{L}^{\mbox{H}}\)</span> to induce the <strong>invariant subspaces</strong> of some <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a> generated by the process <span class="math inline">\(X(t,\omega)\)</span>:<label for="tufte-sn-454" class="margin-toggle sidenote-number">454</label><input type="checkbox" id="tufte-sn-454" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">454</span> The Hilbert space <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span> generated by a <strong>weak stationary</strong> stochastic process <span class="math inline">\(X(t,\omega)\)</span> is to be generated by the <a href="ch-representation.html#sub:dualBasis">unitary</a> lagged operator <span class="math inline">\(\mathbf{L}\)</span> in the following sense: <span class="math display">\[\mathcal{H}_{X(\omega)}=\overline{\mbox{span}}\{\mathbf{L}^{t}X(t,\omega)| t\in\mathbb{Z}\}.\]</span></span>
<span class="math display">\[\mathbf{L}\mathcal{H}_{X(\omega)}^{t-}=\mathcal{H}_{X(\omega)}^{t-},\;\mathbf{L}^{\mbox{H}}\mathcal{H}_{X(\omega)}^{t+}=\mathcal{H}_{X(\omega)}^{t+},\]</span>
where <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t-}\)</span> and <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t+}\)</span> represent the past and the future respectively, and <span class="math inline">\(\mathcal{H}_{X(\omega)}= \mathcal{H}_{X(\omega)}^{t-}+\mathcal{H}_{X(\omega)}^{t+}\)</span>.</p>
<p>With the concepts of <strong>direct sum</strong> (<strong>decomposition</strong>) and <strong>weak stationarity</strong>, we can analyze the <a href="ch-UnMulti.html#sub:rw">random walk</a> model <span class="math inline">\(X_{t}(\omega)=\sum_{i=1}^{t}\varepsilon_{i}(\omega)\)</span> from a different perspective. Let <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t}\)</span> and <span class="math inline">\(\mathcal{H}_{\varepsilon(\omega)}^{t}\)</span> be the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-spaces</a> generated by the <span class="math inline">\(X_t(\omega)\)</span> and <span class="math inline">\(\varepsilon_t(\omega)\)</span> at time <span class="math inline">\(t\)</span>. Because Gaussian white noises <span class="math inline">\(\{\varepsilon_{i}(\omega)\}_{i=1}^{t}\)</span>
are from <em>disjoint orthogonal subspaces</em> <span class="math inline">\(\mathcal{H}^{t}_{\varepsilon(\omega)},\dots,\mathcal{H}^{1}_{\varepsilon(\omega)}\)</span>, the <strong>direct sum</strong> <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t}\)</span> can be <strong>orthogonally decomposed</strong> as
<span class="math display">\[\begin{align*}\mathcal{H}_{X(\omega)}^{t}&amp;=\mathcal{H}^{t}_{\varepsilon(\omega)}\oplus\mathcal{H}^{t-1}_{\varepsilon(\omega)}\cdots\oplus\mathcal{H}^{1}_{\varepsilon(\omega)} = \oplus_{s=1}^{t}\mathcal{H}^{s}_{\varepsilon(\omega)} \\&amp;=
\mathcal{H}^{t}_{\varepsilon(\omega)}\oplus\mathbf{L}\mathcal{H}_{X(\omega)}^{t}.\end{align*}\]</span></p>
<p>The subspace <span class="math inline">\(\mathcal{H}^{t}_{\varepsilon(\omega)}\)</span>, as the <strong>orthogonal complement</strong> of lagged <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t}\)</span>, provides the gap information between <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t}\)</span> and its one step backward shifted <span class="math inline">\(\mathbf{L}\mathcal{H}_{X(\omega)}^{t}\)</span>. Such a subspace is called the <em>wandering subspace</em> if the shift/lagged operator induces <strong>invariant subspaces</strong>.</p>
<p>In short, the space the <strong>weakly stationary</strong>
random walks is decomposed of the <strong>orthogonal direct sums</strong> of <strong>wandering subspaces</strong> generated by the white noises.<label for="tufte-sn-455" class="margin-toggle sidenote-number">455</label><input type="checkbox" id="tufte-sn-455" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">455</span> The <a href="ch-UnMulti.html#sub:rw">random walk</a> is <strong>weakly stationary</strong>, as the covariance function <span class="math inline">\(\mbox{Cov}_X(t,s)=\sigma^2(t-s)\)</span> only depend on the difference of its arguments.</span>
Thus, we can view the space of random walks as the <strong>direct sum</strong> of many subspaces generated by the white noises. The sequence <span class="math inline">\(\{\varepsilon_{i}\}_{i=1}^{t}\)</span> is called the <em>white noise basis</em> of the <a href="ch-UnMulti.html#sub:rw">random walk</a> series.</p>
<p>This kind of the <strong>decomposition</strong> turns out to be valid for all <strong>weakly stationary processes</strong> with zero means in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a>, even if the space consists of infinitely many <strong>wandering subspaces</strong>.<label for="tufte-sn-456" class="margin-toggle sidenote-number">456</label><input type="checkbox" id="tufte-sn-456" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">456</span> The result is called the <em>Wold’s decomposition</em>. However, additional restrictions have to be imposed on the remote past and future subspaces of the processes. The remote past and future subspaces have to be trivial, namely <span class="math display">\[\lim_{t\rightarrow\pm\infty}\mathcal{H}_{X(\omega)}^{t}=0,\]</span> so that they provide zero information to all the other subspaces of the process. The <strong>Wold decomposition</strong> basically says that a <a href="ch-representation.html#sub:dualBasis">unitary operator</a> can induce a sequence of <strong>subspaces</strong> whose <strong>orthogonal direct sum</strong> is invariant under the operator.</span></p>
<p>By the definition of the <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> in chapter <a href="ch-MatComp.html#sub:vecSpaces">11.4</a>, we know that if some objects live in a subspace, any linear combination of these objects also belongs to this subspace. The collection of <strong>wandering subspaces</strong> induced by the <strong>white noise basis</strong> can be infinite dimensional. This motivates us to represent some infinite dimensional elements through a linear combination of infinite dimensional white noises basis (a <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a> of white noise process) in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-spaces</a>.<label for="tufte-sn-457" class="margin-toggle sidenote-number">457</label><input type="checkbox" id="tufte-sn-457" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">457</span> The previous discussion says that the <strong>null space</strong> of a <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a> can be the “maximal” subspace. One may expect that the representation is good enough to approximate almost all <strong>second-order</strong> processes. This is roughly the case, except that the <strong>mean function</strong> (the “missing” one dimension) has to be removed from the approximation. So we only consider zero mean processes over here.</span> Let <span class="math inline">\(\varepsilon(t,\omega)\)</span> be a <a href="ch-randomization.html#sub:RHilbert">Gaussian white noise process</a>. The linear functional <span class="math inline">\(f(\omega)\in\mathcal{H}_{f(\omega)}\)</span> of the <a href="ch-randomization.html#sub:RHilbert">white noise process</a> has the form
<span class="math display">\[f(\omega)=\sum_{s=-\infty}^{\infty}c(-s)\varepsilon(s,\omega),\;\mbox{where }\sum_{s=-\infty}^{\infty}|\phi(-s)|^{2}&lt;\infty\]</span>
where the deterministic function <span class="math inline">\(c(-s)\)</span> is in the <a href="sub-continuity.html#sub:continuousFunc"><span class="math inline">\(\ell_2\)</span>-space</a>, and plays a role as the <span class="math inline">\(s\)</span>-th <a href="ch-representation.html#sub:conjugacy">Fourier coefficient</a> of the functional <span class="math inline">\(f\)</span> with respect to the <strong>white noise basis</strong> <span class="math inline">\(\varepsilon(s,\omega)\)</span> such that <span class="math display">\[c(-t)=\left\langle f(\omega),\varepsilon(t,\omega)\right\rangle _{\mathbb{P}}=\mathbb{E}\left[f(\omega)\varepsilon(t,\omega)\right].\]</span>
As you can see, the function <span class="math inline">\(c(\cdot)\)</span> is analogous to the coefficient in the <a href="ch-representation.html#sub:conjugacy">abstract Fourier series</a> in chapter <a href="ch-representation.html#sub:conjugacy">14.3</a>. Therefore, we expect that these coefficient functions are unique and form a square summable sequence.<label for="tufte-sn-458" class="margin-toggle sidenote-number">458</label><input type="checkbox" id="tufte-sn-458" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">458</span> It is possible to derive the continuous version of this representation. But due to the difficulty of defining “proper” mean square integral of white noises, it is better to construct the functionals in terms of <a href="ch-randomization.html#sub:RHilbert">mean square differential</a> of a Brownian motion <span class="math inline">\(B(t,\omega)\)</span>, i.e., <span class="math display">\[f(\omega)=\int_{-\infty}^{\infty}c(-s)\mbox{d}B(t,\omega)\]</span>
where <span class="math inline">\(\mbox{d}B(t,\omega)=\varepsilon(t,\omega)\mbox{d}t\)</span> according to the previous definition of the white nose.</span></p>
<p>Recall that given <span class="math inline">\(\omega\)</span>, a stochastic process <span class="math inline">\(X(t,\omega)\)</span> is a function (path) over time <span class="math inline">\(t\)</span>.<label for="tufte-sn-459" class="margin-toggle sidenote-number">459</label><input type="checkbox" id="tufte-sn-459" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">459</span> Given <span class="math inline">\(\omega\)</span>, <span class="math inline">\(X(\cdot, \omega):\mathcal{V}\rightarrow\mathbb{F}\)</span> is a <a href="ch-MatComp.html#sub:vecSpaces">functional</a>.</span> It would be natural think about using the previous decomposition for representing a stochastic processes. Because all white noises have zero means, <span class="math inline">\(\mathbb{E}[\varepsilon(t,\omega)]=0\)</span>, the inner product of <span class="math inline">\(X(t,\omega)\)</span> and <span class="math inline">\(\varepsilon(t,\omega)\)</span> actually gives the <a href="ch-randomization.html#sub:RHilbert">covariance function</a>.<label for="tufte-sn-460" class="margin-toggle sidenote-number">460</label><input type="checkbox" id="tufte-sn-460" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">460</span> Note that
<span class="math display">\[\begin{align*}\mbox{Cov}_{X,\varepsilon}(t,s)=&amp;\\\mathbb{E}\left[X(t,\omega)\varepsilon(s,\omega)\right]-&amp;\mathbb{E}[X(t,\omega)]\mathbb{E}[\varepsilon(s,\omega)]\\=&amp;\left\langle X(t,\omega),\varepsilon(s,\omega)\right\rangle _{\mathbb{P}}.\end{align*}\]</span></span>
So we have the following representation <span class="math display">\[X(t,\omega)=\sum_{s=-\infty}^{\infty}\mbox{Cov}_{X,\varepsilon}(t,s)\varepsilon(s,\omega).\]</span>
In particular, if the <a href="ch-randomization.html#sub:RHilbert">second order process</a> <span class="math inline">\(X(t,\omega)\)</span> is <strong>weakly stationary</strong>,
the <a href="ch-representation.html#sub:dualBasis">duality</a> between coefficient function <span class="math inline">\(c(s)=\mbox{Cov}_{X,\omega}(t,s)\)</span> and the <strong>white noise basis</strong> becomes clear. Because when we shift the all the white noises by <span class="math inline">\(\tau\)</span>-period backward, e.g., <span class="math inline">\(\varepsilon(s-\tau,\omega)=\mathbf{L}^{\tau}\varepsilon(s,\omega)\)</span>, the corresponding coefficient needs to be shifted <span class="math inline">\(\tau\)</span>-period forward:
<span class="math display">\[\begin{align*}
\mbox{Cov}_{X,\omega}(t,s-\tau)&amp;=\left\langle X(t,\omega),\varepsilon(s-\tau)\right\rangle _{\mathbb{P}}=\left\langle X(t,\omega),\mathbf{L}^{\tau}\varepsilon(s)\right\rangle _{\mathbb{P}}\\
&amp;=\left\langle \mathbf{L}^{-\tau}X(t,\omega),\varepsilon(s)\right\rangle _{\mathbb{P}}=\mbox{Cov}_{X,\omega}(t+\tau,s)\\
&amp;=\mathbf{L}^{-\tau}c(s).
\end{align*}\]</span>
Then the representation is simplified to<label for="tufte-sn-461" class="margin-toggle sidenote-number">461</label><input type="checkbox" id="tufte-sn-461" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">461</span> This is the exact formula for the <strong>Wold decomposition</strong> of the <strong>weakly stationary</strong> process <span class="math inline">\(X(t,\omega)\)</span>.</span><br><span class="math display">\[X(t,\omega)=\sum_{s=-\infty}^{\infty}\mbox{Cov}_{X,\varepsilon}(t-s)\varepsilon(s,\omega)\]</span> where the <a href="ch-randomization.html#sub:RHilbert">covariance function</a> only has one argument.</p>
</div>
<div id="sub:Kalman" class="section level2">
<h2>
<span class="header-section-number">16.3</span> Miscellaneous: Sample Average, Reduction Principle, and Kalman’s Filter</h2>
<p>For the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathcal{H}\)</span>-valued random variables</a> <span class="math inline">\(X(\omega)\)</span> and <span class="math inline">\(Y(\omega)\)</span> in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a>, the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathbb{P}\)</span>-inner product</a> <span class="math inline">\(\mathbb{E}[X(\omega)Y(\omega)]\)</span> implies the <em><span class="math inline">\(\mathbb{P}\)</span>-norm</em> <span class="math inline">\(\|X(\omega)\|_{\mathbb{P}}=\sqrt{\mathbb{E}[X(\omega)]^{2}}\)</span>, and the <em>mean square distance</em><label for="tufte-sn-462" class="margin-toggle sidenote-number">462</label><input type="checkbox" id="tufte-sn-462" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">462</span> In particular, if both <span class="math inline">\(X(\omega)\)</span> and <span class="math inline">\(Y(\omega)\)</span> are mean zero random variables, the distance is simply the covariance between <span class="math inline">\(X(\omega)\)</span> and <span class="math inline">\(Y(\omega)\)</span>.</span>
<span class="math display">\[\begin{align*}\mbox{d}_{\mathbb{P}}(X(\omega),Y(\omega))&amp;=\langle X(\omega)-Y(\omega), X(\omega)-Y(\omega) \rangle_{\mathbb{P}}\\
&amp;=\mathbb{E}[X(\omega)-Y(\omega)]^{2}=\|X(\omega)-Y(\omega)\|^{2}_{\mathbb{P}}.\end{align*}\]</span>
With these definitions, we can derive the <a href="ch-optApp.html#sub:appSys">metric projection operator</a> in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a>, which is nothing else but the <a href="ch-CalUn.html#sub:ex">expectation operator</a>.</p>
<p>Let’s illustrate the result by setting <span class="math inline">\(\mathcal{H}=\mathbb{R}\)</span>.
According to the definition of the <a href="ch-optApp.html#sub:appSys">metric projection</a>, the projection will map <span class="math inline">\(X(\omega)\in\mathbb{R}\)</span> to a single value in <span class="math inline">\(\mathbb{R}\)</span> that is the deterministic <a href="ch-optApp.html#sub:appSys">optimal approximation</a> for the random variable <span class="math inline">\(X(\omega)\)</span>. Thus, the <a href="ch-optApp.html#sub:appSys">hat operator</a> <span class="math inline">\(\hat{x}\)</span> will minimize the <strong>mean square distance</strong> such that<label for="tufte-sn-463" class="margin-toggle sidenote-number">463</label><input type="checkbox" id="tufte-sn-463" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">463</span> The <a href="ch-optApp.html#sub:appSys">hat operator</a> <span class="math inline">\(\hat{(\cdot)}:\mathcal{H}_{X(\omega)}\rightarrow \mathbb{R}\)</span> maps from the <a href="ch-randomization.html#sub:RHilbert">randomized Hilbert space</a> generated by <span class="math inline">\(X(\omega)\)</span> to <span class="math inline">\(\mathbb{R}\)</span>. So <span class="math inline">\(\hat{x}\)</span> is a <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a> in <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span>.</span>
<span class="math display">\[\begin{align*}\min_{c\in\mathbb{R}}\mbox{d}_{\mathbb{P}}(X,c)&amp;=\min_{c\in\mathbb{R}}\mathbb{E}[X(\omega)-c]^{2}=\mathbb{E}[X(\omega)-\hat{x}]^{2}.\end{align*}\]</span>
Let <span class="math inline">\(\mu\)</span> be the mean value of the random variable <span class="math inline">\(X(\omega)\)</span>. Note that
<span class="math display">\[\begin{align*}&amp;\mathbb{E}[(X(\omega)-c)]^{2}=\mathbb{E}[(X(\omega)-\mu+\mu-c)]^{2}\\
=&amp;\mathbb{E}\left[(X(\omega)-\mu)^{2}+(\mu-c)^{2}+2(X(\omega)-\mu)(\mu-c)\right]\\
=&amp;  \underset{=\mbox{Var}(X(\omega))}{\underbrace{\mathbb{E}[(X(\omega)-\mu)^{2}]}}+(\mu-c)^{2}+2(\mu-c)\underset{=0}{\underbrace{\mathbb{E}[(X(\omega)-\mu)]}}.
\end{align*}\]</span>
The ﬁrst term is the variance, so its value is the same regardless of the choice of <span class="math inline">\(c\)</span>. The minimization only depends on the second term that is clearly minimized by <span class="math inline">\(\hat{x}=\mu=\mathbb{E}[X(\omega)]\)</span>.<label for="tufte-sn-464" class="margin-toggle sidenote-number">464</label><input type="checkbox" id="tufte-sn-464" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">464</span> In other words, we can also think of the <a href="ch-MatComp.html#sub:vecSpaces">linear operator</a> <span class="math inline">\(\mathbb{E}[\cdot]\)</span> of a random variable as the <a href="ch-optApp.html#sub:appSys">metric projector</a> in <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathbb{R})\)</span></a>:
<span class="math display">\[\mathbf{P}_{\mathcal{\mathbb{R}}}(X(\omega))=\mathbb{E}[X(\omega)]=\int_{\mathcal{\mathbb{R}}}x\mathbb{P}(\mbox{d}x).\]</span></span></p>
<p><span class="newthought">Sample average </span></p>
<p>The expectation is about probability integration. But the probability is often hard to attain in practice. In this case, we may turn to some coarse techniques, such as the sample average in statistics.</p>
<p>Let’s consider a sample vector <span class="math inline">\(\mathbf{x}\in\mathbb{R}^{n}\)</span> of <span class="math inline">\(n\)</span> observations. A coarse statistics can be a simple value in <span class="math inline">\(\mathbb{R}\)</span> like a constant vector in <span class="math inline">\(\mathbb{R}^{n}\)</span>. So the <a href="ch-optApp.html#sub:appSys">projection</a> is to project <span class="math inline">\(\mathbf{x}\)</span> onto a <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> spanned by a constant vector, say <span class="math inline">\(\mathbf{1}=[1,\dots,1]^{\top}\in\mathbb{R}^{n}\)</span>. The form of this <a href="ch-representation.html#sub:conjugacy">projection matrix</a> is given by<label for="tufte-sn-465" class="margin-toggle sidenote-number">465</label><input type="checkbox" id="tufte-sn-465" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">465</span> This matrix is a <a href="ch-optApp.html#sub:appSys">projection</a> because <span class="math display">\[\begin{align*}&amp;\mathbf{P}_{\mbox{span}(\mathbf{1})}\mathbf{P}_{\mbox{span}(\mathbf{1})}\\&amp;=\mathbf{1}(\mathbf{1}^{\top}\mathbf{1})^{-1}\mathbf{1}^{\top}\mathbf{1}(\mathbf{1}^{\top}\mathbf{1})^{-1}\mathbf{1}^{\top}\\&amp;= \mathbf{1}(\mathbf{1}^{\top}\mathbf{1})^{-1}\mathbf{1}^{\top}=\mathbf{P}_{\mbox{span}(\mathbf{1})}.\end{align*}\]</span> Also, the projection is <a href="ch-optApp.html#sub:Optimization">orthogonal</a> because <span class="math inline">\(\mathbf{P}_{\mbox{span}(\mathbf{1})}^{\top}=\mathbf{P}_{\mbox{span}(\mathbf{1})}\)</span>.</span>
<span class="math display">\[\begin{align*}\mathbf{P}_{\mbox{span}(\mathbf{1})}=\mathbf{1}(\mathbf{1}^{\top}\mathbf{1})^{-1}\mathbf{1}^{\top}=\left[\begin{array}{ccc}
\frac{1}{n} &amp; \cdots &amp; \frac{1}{n}\\
\vdots &amp; \ddots &amp; \vdots\\
\frac{1}{n} &amp; \cdots &amp; \frac{1}{n}
\end{array}\right].\end{align*}\]</span>
For any sample vector <span class="math inline">\(\mathbf{x}\in\mathbb{R}^{n}\)</span>, the difference between the vector and its projected vector is a residual vector <span class="math display">\[\left(\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})}\right)\mathbf{x}=\mathbf{x}-\hat{\mathbf{x}}=\left[\begin{array}{c}
x_{1}-\hat{x}\\
\vdots\\
x_{n}-\hat{x}
\end{array}\right],\]</span>
where the <a href="ch-optApp.html#sub:appSys">hat operator</a> <span class="math inline">\(\hat{(\cdot)}:\mathbb{R}^n\rightarrow\mbox{span}(\mathbf{1})\)</span> induces a constant vector <span class="math inline">\(\hat{\mathbf{x}}=\hat{x}\mathbf{1}\)</span> of the statistical <a href="ch-CalUn.html#sub:ex">mean</a> <span class="math inline">\(\hat{x}=\sum_{i=1}^{n}x_i/n\)</span>, namely the statistical counterpart of <a href="ch-CalUn.html#sub:ex">expectation</a>. The <a href="ch-vecMat.html#sub:vec">norm</a> of this residual vector measures the distance between the sample vector and its projection<label for="tufte-sn-466" class="margin-toggle sidenote-number">466</label><input type="checkbox" id="tufte-sn-466" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">466</span> The residual of this projection is stored in the <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> <span class="math inline">\((\mbox{span}(\mathbf{1}))^{\bot}\)</span> where <span class="math inline">\((\mbox{span}(\mathbf{1}))\oplus(\mbox{span}(\mathbf{1}))^{\bot}=\mathbb{R}^{n}\)</span>. The explicit form of <span class="math inline">\(\mathbf{P}_{\mbox{span}(\mathbf{1})^{\bot}}=\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})}\)</span> is
<span class="math display">\[\begin{align*}&amp;\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})}\\&amp;=\left[\begin{array}{ccc}
1-\frac{1}{n} &amp; \cdots &amp; -\frac{1}{n}\\
\vdots &amp; \ddots &amp; \vdots\\
-\frac{1}{n} &amp; \cdots &amp; 1-\frac{1}{n}
\end{array}\right].\end{align*}\]</span>
It is another projection onto the <a href="ch-randomization.html#sub:SumDecomp">orthogonal complement</a> subspace of <span class="math inline">\(\mbox{span}(\mathbf{1})\)</span> such that
<span class="math display">\[\begin{align*}&amp;(\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})})^{2}=\mathbf{I}^{2}+(\mathbf{P}_{\mbox{span}(\mathbf{1})})^{2}\\&amp;-2\mathbf{I}\times\mathbf{P}_{\mbox{span}(\mathbf{1})}=\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})},\end{align*}\]</span>
and <span class="math inline">\((\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})})\mathbf{P}_{\mbox{span}(\mathbf{1})}\\=\mathbf{P}_{\mbox{span}(\mathbf{1})}-\mathbf{P}_{\mbox{span}(\mathbf{1})}^{2}=0.\)</span></span> <span class="math display">\[\begin{align*}\|\mathbf{x}-\hat{\mathbf{x}}\|^{2}&amp;=\left\|\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})}\mathbf{x}\right\|^2\\
&amp;=\mathbf{x}^{\top}(\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})})(\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})})\mathbf{x}\\
&amp;=\mathbf{x}^{\top}(\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})})\mathbf{x}.\end{align*}\]</span>
By the properties of the <a href="ch-optApp.html#sub:appSys">orthogonal metric projection</a> (see section <a href="ch-optApp.html#sub:Optimization">15.2</a>), we know that <span class="math inline">\(\mathbf{P}_{\mbox{span}(\mathbf{1})}\)</span> minimizes the distance while <span class="math inline">\(\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})}\)</span> orthogonalizes the sample vector to the vector in the <a href="ch-randomization.html#sub:SumDecomp">complement subspace</a> (the residual) of the projection:<label for="tufte-sn-467" class="margin-toggle sidenote-number">467</label><input type="checkbox" id="tufte-sn-467" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">467</span> Here is a general result regarding the orthogonality. Let <span class="math inline">\(\mathcal{Y}\)</span> be a <a href="ch-randomization.html#sub:SumDecomp">closed subspace</a> of the <a href="ch-representation.html#sub:conjugacy">Hilbert space</a> <span class="math inline">\(\mathcal{H}\)</span>. Given <span class="math inline">\(f\in\mathcal{H}\)</span>, the solution <span class="math inline">\(\hat{f}\)</span> of the problem <span class="math display">\[\min_{g\in\mathcal{Y}}\|f-g\|\]</span> is unique; and <span class="math inline">\(\hat{f}\)</span> is the orthogonal projection of <span class="math inline">\(f\)</span> onto <span class="math inline">\(\mathcal{Y}\)</span>, namely <span class="math display">\[\langle f-\hat{f},g\rangle=0,\]</span>
for any <span class="math inline">\(g\in\mathcal{Y}\)</span>.</span>
<span class="math display">\[\begin{align*}&amp;\min_{\mathbf{c}\in\mbox{span}(\mathbf{1})}\|\mathbf{x}-\mathbf{c}\|^{2}\\&amp;=\begin{cases}
\mbox{minimum distance}: &amp; \|\mathbf{x}-\mathbf{P}_{\mbox{span}(\mathbf{1})}\mathbf{x}\|^{2},\\
\mbox{orthogonality}: &amp; \left\langle(\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})})\mathbf{x}, \mathbf{c}\right\rangle=0.
\end{cases}\end{align*}\]</span></p>
<div class="solution">
<p class="solution-begin">
Numerical illustration <span id="sol-start-193" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-193', 'sol-start-193')"></span>
</p>
<div id="sol-body-193" class="solution-body" style="display: none;">
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb123-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb123-2" data-line-number="2"></a>
<a class="sourceLine" id="cb123-3" data-line-number="3">n =<span class="st"> </span><span class="dv">5</span>; x=<span class="kw">rnorm</span>(n,<span class="dv">3</span>); one=<span class="kw">rep</span>(<span class="dv">1</span>,n); </a>
<a class="sourceLine" id="cb123-4" data-line-number="4"><span class="co"># projection matrix P</span></a>
<a class="sourceLine" id="cb123-5" data-line-number="5">proj =one <span class="op">%*%</span><span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(one) <span class="op">%*%</span><span class="st"> </span>one) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(one);</a>
<a class="sourceLine" id="cb123-6" data-line-number="6"><span class="co"># I - P</span></a>
<a class="sourceLine" id="cb123-7" data-line-number="7">comp.proj =<span class="st"> </span><span class="kw">diag</span>(n)<span class="op">-</span>proj; </a>
<a class="sourceLine" id="cb123-8" data-line-number="8"></a>
<a class="sourceLine" id="cb123-9" data-line-number="9"><span class="kw">solve</span>(<span class="kw">t</span>(one) <span class="op">%*%</span><span class="st"> </span>one) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(one)<span class="op">%*%</span>x</a></code></pre></div>
<pre><code>##          [,1]
## [1,] 3.407266</code></pre>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb125-1" data-line-number="1"><span class="kw">mean</span>(x)</a></code></pre></div>
<pre><code>## [1] 3.407266</code></pre>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb127-1" data-line-number="1"><span class="co"># minimum vector norm</span></a>
<a class="sourceLine" id="cb127-2" data-line-number="2"><span class="kw">t</span>(comp.proj<span class="op">%*%</span>x) <span class="op">%*%</span><span class="st"> </span>(comp.proj<span class="op">%*%</span>x)</a></code></pre></div>
<pre><code>##           [,1]
## [1,] 0.5482674</code></pre>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb129-1" data-line-number="1">normFn =<span class="st"> </span><span class="cf">function</span>(c) {   <span class="co">## norm function</span></a>
<a class="sourceLine" id="cb129-2" data-line-number="2">      (x <span class="op">-</span><span class="st"> </span>c <span class="op">%*%</span>one)<span class="op">%*%</span><span class="kw">t</span>(x <span class="op">-</span><span class="st"> </span>c<span class="op">%*%</span>one)</a>
<a class="sourceLine" id="cb129-3" data-line-number="3">}</a>
<a class="sourceLine" id="cb129-4" data-line-number="4"><span class="kw">optim</span>(<span class="kw">c</span>(<span class="dv">1</span>), normFn)<span class="op">$</span>value</a></code></pre></div>
<pre><code>## [1] 0.5482675</code></pre>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb131-1" data-line-number="1"><span class="co"># orthogonality with any constant vector</span></a>
<a class="sourceLine" id="cb131-2" data-line-number="2"><span class="kw">t</span>(comp.proj<span class="op">%*%</span>x) <span class="op">%*%</span><span class="st"> </span>(proj<span class="op">%*%</span><span class="kw">rep</span>(<span class="dv">10000</span>,n))</a></code></pre></div>
<pre><code>##               [,1]
## [1,] -9.094947e-13</code></pre>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p><span class="newthought">Reduction principle </span></p>
<p>The expectation (or the sample average) provides a unification of the minimization of <a href="ch-optApp.html#sub:appSys">metric distances</a>, the projection, and the <a href="ch-optApp.html#sub:Proj1">hyperplane</a> in the Hilbert space. This machinery can be extended the projection of <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathcal{H}\)</span>-valued random variables</a> onto some <a href="ch-MatComp.html#sub:vecSpaces">subspace</a>. Let <span class="math inline">\(Y(\omega)\)</span> belong to a randomized Hilbert space <span class="math inline">\(L_{2}(\Omega,\mathcal{Y},\mathbb{P};\mathcal{H})\)</span>. Consider the projection of <span class="math inline">\(Y(\omega)\)</span> onto <span class="math inline">\(L_{2}(\Omega,\mathcal{X},\mathbb{P};\mathcal{H})\)</span>, a <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> of <span class="math inline">\(L_{2}(\Omega,\mathcal{Y},\mathbb{P};\mathcal{H})\)</span>, with the <a href="sub-incomplete.html#sub:beyond2"><span class="math inline">\(\sigma\)</span>-algebra</a> set <span class="math inline">\(\mathcal{X}\subset\mathcal{Y}\)</span>. The projection minimizes the <strong>mean square distance</strong>: <span class="math display">\[\begin{align*}\min_{g\in L_{2}(\Omega,\mathcal{X},\mathbb{P};\mathcal{H})}\|Y(\omega)-g\|_{\mathbb{P}}^{2}&amp;=\min_{g\in {L_{2}(\Omega,\mathcal{X},\mathbb{P};\mathcal{H})}}\mathbb{E}\left[Y(\omega)-g\right]^{2}\\&amp;=\mathbb{E}\left[Y(\omega)-\hat{g}\right]^{2}\end{align*}\]</span>
where <span class="math inline">\(\hat{g}=\mathbb{E}[Y(\omega)|\mathcal{X}]=\mathbb{E}[Y(\omega)(\mathbf{1}_{\mathcal{X}}(Y(\omega))]\)</span> is the expectation of <span class="math inline">\(Y(\omega)\)</span> conditional on <span class="math inline">\(\mathcal{X}\)</span>. Moreover, if the set <span class="math inline">\(\mathcal{X}\)</span> is the <a href="sub-incomplete.html#sub:beyond2"><span class="math inline">\(\sigma\)</span>-algebra</a> generated by another <a href="ch-CalUn.html#sub:rv">random variable</a> <span class="math inline">\(X(\omega)\)</span>, then the expectation becomes <span class="math inline">\(\hat{g}=\mathbb{E}[Y(\omega)|X(\omega)]\)</span>.</p>
<p>The exact representation of a conditional expectation may involve some tedious computation, e.g., the computation of a <a href="ch-UnMulti.html#sub:MultiVar">conditional normal distribution</a> in chapter <a href="ch-UnMulti.html#sub:MultiVar">13.1</a>. To escape from the computational catastrophe, one often assumes the conditional expectation has some <a href="ch-vecMat.html#sub:linearity">linear regression</a> setting: <span class="math display">\[\mathbb{E}[Y(\omega)|X(\omega)]=\hat{\beta}X(\omega)\]</span> for some projected linear coefficient <span class="math inline">\(\hat{\beta}\in\mathbb{R}\)</span>.<label for="tufte-sn-468" class="margin-toggle sidenote-number">468</label><input type="checkbox" id="tufte-sn-468" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">468</span> This is a special case of the general <a href="ch-CalUn.html#sub:conProb">regression</a> setting, e.g., <span class="math inline">\(\mathbb{E}[Y(\omega)|X(\omega)]=\hat{g}(X(\omega))\)</span> for some continuous function <span class="math inline">\(\hat{g}(\cdot)\)</span>. In the regression setting, the projection is about <span class="math display">\[\begin{align*}&amp;\min_{g\in {L_{2}(\Omega,\mathcal{X},\mathbb{P};\mathcal{H})}}\mathbb{E}\left[Y(\omega)-g\right]^{2}
\\&amp;=\min_{g\in\mathcal{C}}\mathbb{E}\left[Y(\omega)-g(X(\omega))\right]^{2}.\end{align*}\]</span></span>
Then the projection associates with the following minimization:<label for="tufte-sn-469" class="margin-toggle sidenote-number">469</label><input type="checkbox" id="tufte-sn-469" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">469</span> The minimization problem can be rigorously written as <span class="math display">\[\begin{align*}&amp;\min_{g\in\mathcal{L}(\mathcal{H}_{X(\omega)},\mathcal{H}_{Y(\omega)})}\mathbb{E}\left[Y(\omega)-g\right]^{2}\\&amp;=\mathbb{E}\left[Y(\omega)-\mathbb{E}\left[\left.Y(\omega)\right|X(\omega)\right]\right]^{2}\\&amp;=\mathbb{E}\left[Y(\omega)-\hat{\beta}X(\omega)\right]^{2}\end{align*}\]</span>
where <span class="math inline">\(\mathcal{L}(\mathcal{H}_{X(\omega)},\mathcal{H}_{Y(\omega)})\)</span> stands for the space of all <a href="ch-MatComp.html#sub:vecSpaces">linear operators</a> mapping from <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span> to <span class="math inline">\(\mathcal{H}_{Y(\omega)}\)</span>. Recall that <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span> is the space generated by the <a href="ch-vecMat.html#sub:vec">linear combination</a> of <span class="math inline">\(X(\omega)\)</span>.</span><span class="math display">\[\begin{align*}\min_{g\in {L_{2}(\Omega,\mathcal{X},\mathbb{P};\mathcal{H})}}\mathbb{E}\left[Y(\omega)-g\right]^{2}&amp;=\min_{\beta\in\mathbb{R}}\mathbb{E}\left[Y(\omega)-\beta X(\omega)\right]^{2}\\
&amp;=\mathbb{E}\left[Y(\omega)-\hat{\beta}X(\omega)\right]^{2},\end{align*}\]</span>
where <span class="math inline">\(\hat{\beta}X(\omega)=\mathbb{E}[Y(\omega)|X(\omega)]=\mathbf{P}_{\mathcal{H}_{X(\omega)}}Y(\omega)\)</span>
for the linear <a href="ch-optApp.html#sub:appSys">projection operator</a>: <span class="math display">\[\mathbb{E}[\cdot|X(\omega)]=\mathbf{P}_{\mathcal{H}_{X(\omega)}}(\cdot)=\frac{\left\langle \cdot,X(\omega)\right\rangle _{\mathbb{P}}}{\left\langle X(\omega),X(\omega)\right\rangle _{\mathbb{P}}}X(\omega)\]</span> that coincides with the <a href="ch-vecMat.html#sub:vec">projection formula</a> in an <a href="ch-representation.html#sub:conjugacy">abstract Fourier series</a>.<label for="tufte-sn-470" class="margin-toggle sidenote-number">470</label><input type="checkbox" id="tufte-sn-470" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">470</span> The orthogonality of the projection gives <span class="math display">\[\left\langle Y(\omega)-\hat{\beta}X(\omega),Z(\omega)\right\rangle _{\mathbb{P}}=0\]</span>
for <span class="math inline">\(Z(\omega)\in\mathcal{H}_{X(\omega)}\)</span>.</span></p>
<p>As you can see, the <a href="ch-vecMat.html#sub:linearity">linear regression</a> imposes two conditions on the projection 1. the specific functional form of <span class="math inline">\(\mathbb{E}[Y(\omega)|X(\omega)]\)</span> and 2. the linear projected coefficient <span class="math inline">\(\hat{\beta}\)</span>. The first condition attaches the <a href="ch-vecMat.html#sub:linearity">linearity</a> to the projection. The second condition makes the projection map to the space <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span> that is generated by the <a href="ch-vecMat.html#sub:vec">linear combination</a> of <span class="math inline">\(X(\omega)\)</span>.<label for="tufte-sn-471" class="margin-toggle sidenote-number">471</label><input type="checkbox" id="tufte-sn-471" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">471</span> The orthogonality always holds when <span class="math inline">\(\mathbb{E}[Y(\omega)|X(\omega)]\)</span> satisfies these two conditions, i.e. being a linear projection onto <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span>. However, if one has no idea about the functional form of <span class="math inline">\(\mathbb{E}[Y(\omega)|X(\omega)]\)</span> or if <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span> is not in the subspace <span class="math inline">\(L_{2}(\Omega,\mathcal{X},\mathbb{P};\mathcal{H})\)</span>, then the <a href="ch-randomization.html#sub:SumDecomp">orthogonality</a> may be violated. For example, if <span class="math inline">\(\mathbb{E}[Y|X]=g(X)=\hat{\beta}_{1}X+\hat{\beta}_{2}X^{2}\)</span>, but one insists (wrongly) using the linear setting <span class="math inline">\(\hat{\beta}_{1}X\)</span>, then obviously one will face <span class="math display">\[\left\langle Y(\omega)-\hat{\beta}_1X(\omega),Z(\omega)\right\rangle _{\mathbb{P}}\neq0.\]</span></span></p>
<p>These two conditions (on <a href="ch-vecMat.html#sub:linearity">linearity</a>) turn out to be essential to design two commuting projections to satisfy the <strong>reduction principle</strong>. The <em>reduction principle</em> says that if the product of two commuting projections is also a projection, then one can “reduce” a “causal chain” of two causalities into one.</p>
<p>Let’s illustrate the <strong>reduction principle</strong> by the following linear system:
<span class="math display">\[\begin{cases}
X(\omega)= &amp; \hat\beta_{1}Z(\omega)+e_{1}(\omega)\\
Y(\omega)= &amp; \hat\beta_{2}X(\omega)+e_{2}(\omega)
\end{cases}\Leftrightarrow\begin{cases}
\mathbb{E}\left[\left.X(\omega)\right|Z(\omega)\right]= &amp; \hat\beta_{1}Z(\omega)\\
\mathbb{E}\left[\left.Y(\omega)\right|X(\omega)\right]= &amp; \hat\beta_{2}X(\omega)
\end{cases},\]</span>
where <span class="math inline">\(e_{i}(\omega)\)</span> for <span class="math inline">\(i=1,2\)</span> are some zero-mean noises.<label for="tufte-sn-472" class="margin-toggle sidenote-number">472</label><input type="checkbox" id="tufte-sn-472" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">472</span> The subspaces generated by these noises should be orthogonal to <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span> and <span class="math inline">\(\mathcal{H}_{Z(\omega)}\)</span>.</span> You may find a simple <a href="ch-UnMulti.html#sub:Markov">hierarchical</a> structure of causality. In the system, <span class="math inline">\(Z(\omega)\)</span> gives the causal effect of <span class="math inline">\(X(\omega)\)</span>, and <span class="math inline">\(X(\omega)\)</span> transfers the effect to <span class="math inline">\(Y(\omega)\)</span>. By this causal chain, we can iteratively project <span class="math inline">\(Y(\omega)\)</span> onto the <a href="ch-MatComp.html#sub:vecSpaces">subspaces</a> generated by <span class="math inline">\(X(\omega)\)</span> and <span class="math inline">\(Z(\omega)\)</span> through <span class="math inline">\(\mathbb{E}[\cdot|X(\omega)]\)</span> and <span class="math inline">\(\mathbb{E}[\cdot|Z(\omega)]\)</span>:
<span class="math display">\[\mathbb{E}\left[\left.\mathbb{E}\left[\left.Y(\omega)\right|X(\omega)\right]\right|Z(\omega)\right]=\mathbb{E}\left[\left.\hat{\beta}_{2}X(\omega)\right|Z(\omega)\right]=\hat{\beta_{2}}\hat{\beta_{1}}Z(\omega).\]</span>
Notice that this expectation coincides with <span class="math inline">\(\mathbb{E}\left[\left.Y(\omega)\right|Z(\omega)\right]\)</span> from a “reduced” regression <span class="math display">\[\begin{align*}Y(\omega)&amp;=\hat{\beta}_{2}\left(\hat{\beta}_{1}Z(\omega)+e_1(\omega)\right)+e_2(\omega)\\
&amp;=\hat{\beta}_{2}\hat{\beta}_{1}Z(\omega)+e_{12}(\omega)\end{align*}\]</span>
where <span class="math inline">\(e_{12}(\omega)=\hat\beta_{2}e_{1}(\omega)+e_{2}(\omega)\)</span>.<label for="tufte-sn-473" class="margin-toggle sidenote-number">473</label><input type="checkbox" id="tufte-sn-473" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">473</span> The result <span class="math display">\[\begin{align*}&amp;\mathbb{E}\left[\left.\mathbb{E}\left[\left.Y(\omega)\right|X(\omega)\right]\right|Z(\omega)\right]\\&amp;=\mathbb{E}\left[\left.Y(\omega)\right|Z(\omega)\right]\end{align*}\]</span> is also known as the <em>law of iterated expectations</em> when the <span class="math inline">\(\sigma\)</span>-algebra generated by <span class="math inline">\(X(\omega)\)</span> is contained in the <span class="math inline">\(\sigma\)</span>-algebra generated by <span class="math inline">\(Z(\omega)\)</span>.</span></p>
<div class="solution">
<p class="solution-begin">
Numerical illustration <span id="sol-start-194" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-194', 'sol-start-194')"></span>
</p>
<div id="sol-body-194" class="solution-body" style="display: none;">
<p>Using the formula <span class="math display">\[\mathbb{E}\left[\left.\cdot\right|Z(\omega)\right]=\frac{\left\langle \cdot,Z(\omega)\right\rangle _{\mathbb{P}}}{\left\langle Z(\omega),Z(\omega)\right\rangle _{\mathbb{P}}}Z(\omega),\]</span>
we have <span class="math inline">\(\mathbf{P}_{\mathcal{H}_{Z(\omega)}}Y(\omega)=\mathbb{E}[Y(\omega)|Z(\omega)]=\frac{\left\langle Y(\omega),Z(\omega)\right\rangle _{\mathbb{P}}}{\left\langle Z(\omega),Z(\omega)\right\rangle _{\mathbb{P}}}Z(\omega)\)</span>, and<label for="tufte-sn-474" class="margin-toggle sidenote-number">474</label><input type="checkbox" id="tufte-sn-474" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">474</span> The result follows by combining <span class="math display">\[\begin{align*}&amp;\mathbb{E}\left[\left.\mathbb{E}\left[\left.Y(\omega)\right|X(\omega)\right]\right|Z(\omega)\right]\\&amp;=\mathbb{E}\left[\left.\hat{\beta}_{2}X(\omega)\right|Z(\omega)\right]\end{align*}\]</span>
and <span class="math display">\[\begin{align*}&amp;\mathbb{E}\left[\left.\hat{\beta}_{2}X(\omega)\right|Z(\omega)\right]\\&amp;=\mathbb{E}\left[\left.\mathbf{P}_{\mathcal{H}_{X(\omega)}}Y(\omega)\right|Z(\omega)\right].\end{align*}\]</span></span><br><span class="math display">\[\begin{align*}&amp;\mathbf{P}_{\mathcal{H}_{Z(\omega)}}\mathbf{P}_{\mathcal{H}_{X(\omega)}}Y(\omega)\\
&amp;=\mathbb{E}\left[\left.\mathbb{E}\left[\left.Y(\omega)\right|X(\omega)\right]\right|Z(\omega)\right]=\hat{\beta}_{2}\hat{\beta}_{1}Z(\omega)
\\&amp;=\frac{\left\langle \mathbf{P}_{\mathcal{H}_{X(\omega)}}Y(\omega),Z(\omega)\right\rangle _{\mathbb{P}}}{\left\langle Z(\omega),Z(\omega)\right\rangle _{\mathbb{P}}}Z(\omega).\end{align*}\]</span>
and <span class="math display">\[\begin{align*}&amp;\mathbf{P}_{\mathcal{H}_{X(\omega)}}\mathbf{P}_{\mathcal{H}_{Z(\omega)}}Y(\omega)\\
&amp;=\mathbb{E}\left[\left.\mathbb{E}\left[\left.Y(\omega)\right|Z(\omega)\right]\right|X(\omega)\right]
\\&amp;=\frac{\left\langle \mathbf{P}_{\mathcal{H}_{Z(\omega)}}Y(\omega),X(\omega)\right\rangle _{\mathbb{P}}}{\left\langle X(\omega),X(\omega)\right\rangle _{\mathbb{P}}}X(\omega).\end{align*}\]</span>
The following example shows the equivalence of <span class="math inline">\(\mathbf{P}_{\mathcal{H}_{Z(\omega)}}\)</span>, <span class="math inline">\(\mathbf{P}_{\mathcal{H}_{Z(\omega)}}\mathbf{P}_{\mathcal{H}_{X(\omega)}}\)</span>, and <span class="math inline">\(\mathbf{P}_{\mathcal{H}_{X(\omega)}}\mathbf{P}_{\mathcal{H}_{Z(\omega)}}\)</span>.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb133-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb133-2" data-line-number="2">n =<span class="st"> </span><span class="dv">300</span>; b1=<span class="fl">0.5</span>; b2=<span class="dv">2</span>; z =<span class="st"> </span><span class="kw">runif</span>(n,<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>); </a>
<a class="sourceLine" id="cb133-3" data-line-number="3"><span class="co"># create orthogonal noises</span></a>
<a class="sourceLine" id="cb133-4" data-line-number="4"><span class="kw">library</span>(MASS); orth =<span class="st"> </span><span class="kw">sample</span>(n<span class="op">*</span><span class="dv">2</span>, n); orth.sample =<span class="st"> </span><span class="kw">Null</span>(orth);</a></code></pre></div>
<pre><code>## 
## Attaching package: 'MASS'</code></pre>
<pre><code>## The following object is masked from 'package:dplyr':
## 
##     select</code></pre>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb136-1" data-line-number="1">error1 =<span class="st"> </span>orth.sample[,<span class="kw">sample</span>(n,<span class="dv">1</span>)];  </a>
<a class="sourceLine" id="cb136-2" data-line-number="2">error2 =<span class="st"> </span>orth.sample[,<span class="kw">sample</span>(n,<span class="dv">1</span>)]; </a>
<a class="sourceLine" id="cb136-3" data-line-number="3"></a>
<a class="sourceLine" id="cb136-4" data-line-number="4"><span class="co"># commuting property appears in a slower rate: </span></a>
<a class="sourceLine" id="cb136-5" data-line-number="5"><span class="co"># error1 = rnorm(n,0); error2 =rnorm(n,0)</span></a>
<a class="sourceLine" id="cb136-6" data-line-number="6"></a>
<a class="sourceLine" id="cb136-7" data-line-number="7">x =<span class="st"> </span>b1<span class="op">*</span>z <span class="op">+</span><span class="st"> </span>error1;</a>
<a class="sourceLine" id="cb136-8" data-line-number="8">y =<span class="st"> </span>b2<span class="op">*</span>x <span class="op">+</span><span class="st"> </span>error2;</a>
<a class="sourceLine" id="cb136-9" data-line-number="9"></a>
<a class="sourceLine" id="cb136-10" data-line-number="10">proj.x =x <span class="op">%*%</span><span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(x) <span class="op">%*%</span><span class="st"> </span>x) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(x);</a>
<a class="sourceLine" id="cb136-11" data-line-number="11"></a>
<a class="sourceLine" id="cb136-12" data-line-number="12"><span class="co"># b12 = 2*0.5 = 1</span></a>
<a class="sourceLine" id="cb136-13" data-line-number="13">b12h.Pz =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(z) <span class="op">%*%</span><span class="st"> </span>z) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(z)<span class="op">%*%</span><span class="st"> </span>y; </a>
<a class="sourceLine" id="cb136-14" data-line-number="14">b12h.PzPx =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(z) <span class="op">%*%</span><span class="st"> </span>z) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(z)<span class="op">%*%</span><span class="st"> </span>proj.x <span class="op">%*%</span><span class="st"> </span>y; </a>
<a class="sourceLine" id="cb136-15" data-line-number="15">b12h.PxPz=<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(x) <span class="op">%*%</span><span class="st"> </span>x) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(x)<span class="op">%*%</span><span class="st"> </span>(<span class="kw">c</span>(b12h.Pz)<span class="op">*</span>x) ;</a>
<a class="sourceLine" id="cb136-16" data-line-number="16"></a>
<a class="sourceLine" id="cb136-17" data-line-number="17"><span class="kw">print</span>(<span class="kw">c</span>(b12h.Pz,b12h.PzPx,b12h.PxPz))</a></code></pre></div>
<pre><code>## [1] 1.001882 1.001882 1.001882</code></pre>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>The following <em>commuting projection</em> property tells us such a reduction emerges:<label for="tufte-sn-475" class="margin-toggle sidenote-number">475</label><input type="checkbox" id="tufte-sn-475" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">475</span> Note that <span class="math display">\[\begin{align*}\mathbf{P}_{\mathcal{H}_{Z(\omega)}}X(\omega)=\mathbb{E}\left[\left.X(\omega)\right|Z(\omega)\right]=&amp;\hat{\beta}_{1}Z(\omega),\\\mathbf{P}_{\mathcal{H}_{X(\omega)}}Y(\omega)=\mathbb{E}\left[\left.Y(\omega)\right|X(\omega)\right]=&amp;\hat{\beta}_{2}X(\omega)\end{align*}\]</span>
The iterated expectations can be written as <span class="math display">\[\begin{align*}&amp;\mathbb{E}\left[\left.\mathbb{E}\left[\left.Y(\omega)\right|X(\omega)\right]\right|Z(\omega)\right]\\&amp;=\mathbb{E}\left[\left.\mathbf{P}_{\mathcal{H}_{X(\omega)}}Y(\omega)\right|Z(\omega)\right]\\&amp;=\mathbf{P}_{\mathcal{H}_{Z(\omega)}}\mathbf{P}_{\mathcal{H}_{X(\omega)}}Y(\omega).\end{align*}\]</span></span>
<span class="math display">\[\mathbf{P}_{\mathcal{H}_{Z(\omega)}}\mathbf{P}_{\mathcal{H}_{X(\omega)}}=\mathbf{P}_{\mathcal{H}_{X(\omega)}}\mathbf{P}_{\mathcal{H}_{Z(\omega)}}=\mathbf{P}_{\mathcal{H}_{Z(\omega)}}\]</span> when the <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> generated by <span class="math inline">\(X(\omega)\)</span> belongs to the one generated by <span class="math inline">\(Z(\omega)\)</span>, namely <span class="math inline">\(\mathcal{H}_{X(\omega)}\subset\mathcal{H}_{Z(\omega)}\)</span>. This condition is indeed the reason for having the <strong>reduction principle</strong> in the previous example.</p>
<ul>
<li>Assume that two different <a href="ch-randomization.html#sub:SumDecomp">orthogonal direct sums</a> are valid in a <a href="ch-representation.html#sub:conjugacy">Hilbert space</a> <span class="math inline">\(\mathcal{V}\)</span> <span class="math display">\[\mathcal{V}=\mathcal{V}_{1}\oplus\mathcal{W}_{1}=\mathcal{V}_{2}\oplus\mathcal{W}_{2}.\]</span>
Let <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}:\mathcal{V}\rightarrow\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{2}}:\mathcal{V}\rightarrow\mathcal{V}_{2}\)</span> be <a href="ch-randomization.html#sub:SumDecomp">orthogonal projection operators</a>. The <strong>reduction principle</strong> says:<label for="tufte-sn-476" class="margin-toggle sidenote-number">476</label><input type="checkbox" id="tufte-sn-476" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">476</span> Moreover. <span class="math inline">\(\mathcal{V}_{1}\subset\mathcal{W}_{2}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\subset\mathcal{W}_{1}\)</span> if and only if
<span class="math display">\[\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}=0.\]</span> The proof of these two statements is given below.</span> <span class="math inline">\(\mathcal{V}_{1}\subset\mathcal{V}_{2}\)</span> and <span class="math inline">\(\mathcal{W}_{2}\subset\mathcal{W}_{1}\)</span> if and only if <span class="math display">\[\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}=\mathbf{P}_{\mathcal{V}_{1}}.\]</span>
</li>
</ul>
<div class="solution">
<p class="solution-begin">
Proof <span id="sol-start-195" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-195', 'sol-start-195')"></span>
</p>
<div id="sol-body-195" class="solution-body" style="display: none;">
<ol style="list-style-type: lower-roman">
<li>Assume <span class="math display">\[\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}=\mathbf{P}_{\mathcal{V}_{1}}.\]</span>
</li>
</ol>
<p>For any <span class="math inline">\(f\in\mathcal{V}\)</span>, <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}f\in\mathcal{V}_{1}\)</span> implies <span class="math display">\[\mathbf{P}_{\mathcal{V}_{1}}f=\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}f\in\mathcal{V}_{2}.\]</span> This implication tells <span class="math inline">\(\mathcal{V}_{1}\subset\mathcal{V}_{2}\)</span>. Moreover,
<span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=\mathbf{P}_{\mathcal{V}_{1}}\)</span> implies
<span class="math display">\[(\mathbf{I}-\mathbf{P}_{\mathcal{V}_{1}})(\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}})=\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}}-\mathbf{P}_{\mathcal{V}_{1}}+\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}}.\]</span>
So for any <span class="math inline">\((\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}})g\in\mathcal{W}_{2}\)</span>, we know that <span class="math display">\[(\mathbf{I}-\mathbf{P}_{\mathcal{V}_{1}})(\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}})g\in\mathcal{W}_{1},\]</span> which implies <span class="math inline">\(\mathcal{W}_{2}\subset\mathcal{W}_{1}\)</span>.</p>
<p>Assume <span class="math inline">\(\mathcal{V}_{1}\subset\mathcal{V}_{2}\)</span> and <span class="math inline">\(\mathcal{W}_{2}\subset\mathcal{W}_{1}\)</span>.</p>
<p>If <span class="math inline">\(\mathcal{V}_{1}\subset\mathcal{V}_{2}\)</span>, for any <span class="math inline">\(f\in\mathcal{V}\)</span>, the projection gives <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}f\in\mathcal{V}_{1}\subset\mathcal{V}_{2}\)</span>. The fixed point property of the projection operator tells that <span class="math display">\[\mathbf{P}_{\mathcal{V}_{2}}(\mathbf{P}_{\mathcal{V}_{1}}f)=\mathbf{P}_{\mathcal{V}_{1}}f.\]</span>
Thus we can see <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}=\mathbf{P}_{\mathcal{V}_{1}}\)</span>. Moreover, because <span class="math inline">\(\mathcal{W}_{2}\subset\mathcal{W}_{1}\)</span>, for any <span class="math inline">\(g\in\mathcal{V}\)</span>, <span class="math display">\[(\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}})g\in\mathcal{W}_{2}\subset\mathcal{W}_{1}.\]</span> Again we have <span class="math display">\[(\mathbf{I}-\mathbf{P}_{\mathcal{V}_{1}})\left((\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}})g\right)=(\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}})g,\]</span>
which implies <span class="math display">\[\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}}-\mathbf{P}_{\mathcal{V}_{1}}+\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}}\]</span>
or say <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=\mathbf{P}_{\mathcal{V}_{1}}\)</span>.</p>
<ol start="2" style="list-style-type: lower-roman">
<li>Assume <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}=0\)</span>.</li>
</ol>
<p>For any <span class="math inline">\(f\in\mathcal{V}_{1}\)</span>, the fixed point property tells that <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}f=f\)</span>. Hence <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}f=\mathbf{P}_{\mathcal{V}_{2}}f=0\)</span>
implies <span class="math inline">\(f\in\mathcal{W}_{2}\)</span>, and so <span class="math inline">\(\mathcal{V}_{1}\subset\mathcal{W}_{2}\)</span>. On the other hand, for any <span class="math inline">\(g\in\mathcal{V}_{2}\)</span>, it follows that <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{2}}g\in\mathcal{V}_{2}\)</span>. The condition <span class="math display">\[\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}g=\mathbf{P}_{\mathcal{V}_{1}}g=0\]</span>
tells that <span class="math inline">\(g\in\mathcal{W}_{2}\)</span>. Thus we have <span class="math inline">\(\mathcal{V}_{2}\subset\mathcal{W}_{2}\)</span>.</p>
<p>Assume <span class="math inline">\(\mathcal{V}_{1}\subset\mathcal{W}_{2}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\subset\mathcal{W}_{1}\)</span>.</p>
<p>For any <span class="math inline">\(f\in\mathcal{V}\)</span>, <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}f\in\mathcal{V}_{1}\)</span>
and the condition
<span class="math display">\[(\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}})\mathbf{P}_{\mathcal{V}_{1}}f=\mathbf{P}_{\mathcal{V}_{1}}f-\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}f=\mathbf{P}_{\mathcal{V}_{1}}f\]</span>
holds for any <span class="math inline">\(f\)</span> as <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}f\in\mathcal{W}_{1}\)</span>. Thus we conclude <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}=0\)</span>. A similar argument works for <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=0\)</span>.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Executing a sequence of projections behaves like multiplying matrices, so the order matters. However, the <strong>reduction principle</strong> reveals a possibility that one can permute the order.<label for="tufte-sn-477" class="margin-toggle sidenote-number">477</label><input type="checkbox" id="tufte-sn-477" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">477</span> In the general setting, one may be interested in a sequence of non-commuting pair of projections but converges to a new projection. This sequence would also achieve the <strong>reduction principle</strong> in a wider sense. For example, <span class="citation">Neumann (<a href="bibliography.html#ref-vonNeumann1950">1950</a>)</span> (Chapter 14) considers any two closed subspace <span class="math inline">\(\mathcal{V}_{1}\)</span>, <span class="math inline">\(\mathcal{V}_{2}\)</span> in the Hilbert space <span class="math inline">\(\mathcal{H}\)</span>. The new projection operator <span class="math display">\[\lim_{n\rightarrow\infty}(\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}})^{n}(f)=\mathbf{P}_{\mathcal{V}_{1}\cap\mathcal{V}_{2}}(f)\]</span>
emerges for any <span class="math inline">\(f\in\mathcal{H}\)</span>.</span> In several quantum mechanics related disciplines, the experiments are identified by the orthogonal projections on a Hilbert space. For example, a binary decision of yes and no may be thought of as a projection onto two orthogonal subspaces, the <a href="ch-randomization.html#sub:SumDecomp">range space</a>, and the <a href="ch-randomization.html#sub:SumDecomp">null space</a>. Moreover, unlike classical mechanics, the outcome of two yes-no experiments may depend on the order in which the experiments are measured. Therefore, the <strong>reduction principle</strong> can play an essential role in simplifying the identification.</p>
<div class="solution">
<p class="solution-begin">
Two-stage estimate and pseudo-norm <span id="sol-start-196" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-196', 'sol-start-196')"></span>
</p>
<div id="sol-body-196" class="solution-body" style="display: none;">
<p>Consider the data vector <span class="math inline">\(\mathbf{x},\mathbf{y},\mathbf{z}\in\mathbb{R}^n\)</span>. One may assume that they are generated by the following model <span class="math display">\[\begin{cases}
\mathbf{x}= &amp; \beta_{1,0}\mathbf{z}+\mathbf{e}_{1},\\
\mathbf{y}= &amp; \beta_{2,0}\mathbf{x}+\mathbf{e}_{2}.
\end{cases}\]</span></p>
<p>The estimates of <span class="math inline">\(\beta_{1,0}\)</span>, <span class="math inline">\(\beta_{2,0}\)</span> are also projections. Unlike the <strong>reduction principle</strong>, in this case, one has to estimate the parameters seperately.</p>
<p>It turns out that the following two way of estimating <span class="math inline">\(\beta_{2,0}\)</span> are equivalent.</p>
<ol style="list-style-type: decimal">
<li><p>One can first estimate <span class="math inline">\(\beta_{1,0}\)</span>, then reconstruct <span class="math inline">\(\mathbf{x}\)</span> by <span class="math inline">\(\hat{\beta}_{1}\mathbf{z}\)</span> so that one estimates <span class="math inline">\(\beta_{2,0}\)</span> through the regression
<span class="math display">\[\mathbf{y}=\beta_{2,0}(\hat{\beta}_{1}\mathbf{z})+\mathbf{e}_{2}.\]</span></p></li>
<li><p>For a projection matrix <span class="math inline">\(\mathbf{P}_{\mathbf{y}}\)</span>, we’ve derived the connection between the distance and the sandwich form <span class="math display">\[\|(\mathbf{I}-\mathbf{P}_{\mathbf{y}})\mathbf{y}\|^{2}=\left\langle \mathbf{P}_{\mathbf{y}^{\bot}}\mathbf{y},\mathbf{P}_{\mathbf{y}^{\bot}}\mathbf{y}\right\rangle =\mathbf{y}^{\top}\mathbf{P}_{\mathbf{y}^{\bot}}\mathbf{y}.\]</span> Recall that for any psotive definited matrix <span class="math inline">\(\mathbf{M}\in\mathbb{R}^{n\times n}\)</span>, we have <span class="math inline">\(\mathbf{y}^{\top}\mathbf{M}\mathbf{y}\geq0\)</span> for any <span class="math inline">\(\mathbf{y}\in\mathbb{R}^{n}\)</span>. The sandwich form <span class="math inline">\(\mathbf{y}^{\top}\mathbf{M}\mathbf{y}\)</span> serves like a norm.<label for="tufte-sn-478" class="margin-toggle sidenote-number">478</label><input type="checkbox" id="tufte-sn-478" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">478</span> It is a <em>pseudo norm</em>, because <span class="math inline">\(\mathbf{y}^{\top}\mathbf{M}\mathbf{y}\)</span> can be zero for <span class="math inline">\(\mathbf{y}\neq 0\)</span> when <span class="math inline">\(\mathbf{M}\)</span> is singular.</span> Because any projection matrix <span class="math inline">\(\mathbf{P}_{\mathbf{y}}\)</span> is positive definited, we can formalize the estimate of <span class="math inline">\(\beta_{2,0}\)</span> under the <strong>pseduo-norm</strong> of <span class="math inline">\(\mathbf{P}_{\mathbf{z}}\)</span>: <span class="math display">\[\min_{\beta_{2}} \left\langle \mathbf{P}_{\mathbf{z}}(\mathbf{y}-\beta_{2}\mathbf{x}),\mathbf{P}_{\mathbf{z}}(\mathbf{y}-\beta_{2}\mathbf{x})\right\rangle =(\mathbf{y}-\beta_{2}\mathbf{x})^{\top}\mathbf{P}_{\mathbf{z}}(\mathbf{y}-\beta_{2}\mathbf{x})^{\top}.\]</span> By the orthogonality condition, <span class="math display">\[\left[\mathbf{P}_{\mathbf{z}}(\mathbf{y}-\hat{\beta}_{2}\mathbf{x})\right]^{\top}\mathbf{P}_{\mathbf{z}}\mathbf{x}=\left[\mathbf{P}_{\mathbf{z}}\mathbf{x}\right]^{\top}\left[\mathbf{P}_{\mathbf{z}}(\mathbf{y}-\hat{\beta}_{2}\mathbf{x})\right]=0\]</span>
we have <span class="math inline">\((\mathbf{P}_{\mathbf{z}}\mathbf{x})^{\top}\mathbf{y}-\hat{\beta}_{2}\mathbf{x}^{\top}\mathbf{P}_{\mathbf{z}}\mathbf{x}=0\)</span> which implies <span class="math inline">\(\hat{\beta}_{2}=(\mathbf{x}^{\top}\mathbf{P}_{\mathbf{z}}\mathbf{x})^{-1}\mathbf{x}^{\top}\mathbf{P}_{\mathbf{z}}\mathbf{y}\)</span>.</p></li>
</ol>
<p>The numerical example will show us these two estimates return exactly the same results. Because the second expression is just a mathematical tautology of the first one.</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb138-1" data-line-number="1"><span class="co"># Method 1</span></a>
<a class="sourceLine" id="cb138-2" data-line-number="2"></a>
<a class="sourceLine" id="cb138-3" data-line-number="3"><span class="co"># regression x = b1 * z + e_1</span></a>
<a class="sourceLine" id="cb138-4" data-line-number="4">b1h =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(z) <span class="op">%*%</span><span class="st"> </span>z) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(z)<span class="op">%*%</span><span class="st"> </span>x; </a>
<a class="sourceLine" id="cb138-5" data-line-number="5"><span class="co"># construct x_hat</span></a>
<a class="sourceLine" id="cb138-6" data-line-number="6">xhat=<span class="st"> </span>z<span class="op">%*%</span>b1h;</a>
<a class="sourceLine" id="cb138-7" data-line-number="7"><span class="co"># regression y = b2 * x_hat + e_2</span></a>
<a class="sourceLine" id="cb138-8" data-line-number="8">b2h<span class="fl">.1</span> =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(xhat) <span class="op">%*%</span><span class="st"> </span>xhat) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(xhat)<span class="op">%*%</span><span class="st"> </span>y;</a>
<a class="sourceLine" id="cb138-9" data-line-number="9"></a>
<a class="sourceLine" id="cb138-10" data-line-number="10"><span class="co"># Method 2</span></a>
<a class="sourceLine" id="cb138-11" data-line-number="11"><span class="co"># projection matrix</span></a>
<a class="sourceLine" id="cb138-12" data-line-number="12">proj.z =z <span class="op">%*%</span><span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(z) <span class="op">%*%</span><span class="st"> </span>z) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(z);</a>
<a class="sourceLine" id="cb138-13" data-line-number="13"><span class="co"># || y-b2x || under the pseudo-norm induced by P_z</span></a>
<a class="sourceLine" id="cb138-14" data-line-number="14">b2h<span class="fl">.2</span>  =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(x) <span class="op">%*%</span><span class="st"> </span>proj.z <span class="op">%*%</span><span class="st"> </span>x) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(x) <span class="op">%*%</span><span class="st"> </span>proj.z <span class="op">%*%</span><span class="st"> </span>y</a>
<a class="sourceLine" id="cb138-15" data-line-number="15"></a>
<a class="sourceLine" id="cb138-16" data-line-number="16"><span class="kw">print</span>(<span class="kw">c</span>(b2h<span class="fl">.1</span>,b2h<span class="fl">.2</span>))</a></code></pre></div>
<pre><code>## [1] 2.000449 2.000449</code></pre>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p><span class="newthought">Kalman filter </span></p>
<p><a href="ch-randomization.html#sub:SumDecomp">Wold decomposition</a> gives a causal representation of the <a href="ch-randomization.html#sub:SumDecomp">weakly stationary processes</a>. It says that a <a href="ch-randomization.html#sub:SumDecomp">white noise basis</a> may explain the causality of all these processes. On the other hand, apart from the space <a href="ch-MatComp.html#sub:vecSpaces">spanned</a> by <a href="ch-randomization.html#sub:RHilbert">white noises</a>, one may have some underlying processes in mind that are believed (by oneself) to acquire the driving forces for some observations of interest.</p>
<p>The current concern is to derive a scheme from which the model of interest and the observations resonate. To achieve an ideal resonation, one needs to filter out unnecessary information. A natural attempt is to project the dynamical law of the model onto that of the observable <span class="math inline">\(\mathbf{y}_t\)</span> by minimizing their <strong>mean square distance</strong>. This scheme is called <strong>Kalman’s filter</strong>.</p>
<p>The implementation of this scheme relates to the use of the <a href="ch-vecMat.html#sub:linearity">affine function</a> for the conditional expectation. Let’s start with a toy example. Suppose that <span class="math inline">\(X(\omega)\)</span> represents the underlying model and <span class="math inline">\(Y(\omega)\)</span> be the observable process. We need to project <span class="math inline">\(X(\omega)\)</span> onto the space generated by <span class="math inline">\(Y(\omega)\)</span>, and we expect that the projection will be <span class="math inline">\(\mathbb{E}[X(\omega)|Y(\omega)]\)</span>. Now assume the functional form of <span class="math inline">\(\mathbb{E}[X(\omega)|Y(\omega)]\)</span> is <a href="ch-vecMat.html#sub:linearity">affine</a>, the projection becomes <span class="math display">\[\mathbb{E}\left[X(\omega)-\mathbb{E}[X(\omega)|Y(\omega)]\right]^{2}=\min_{a,b}\mathbb{E}\left[X(\omega)-a-bY(\omega)\right]^{2},\]</span>
with <span class="math inline">\(\hat{a}=\mathbb{E}[X(\omega)]-\hat{b}\mathbb{E}[Y(\omega)]\)</span> and <span class="math inline">\(\hat{b}=\mbox{Cov}(Y(\omega),X(\omega))/\mbox{Var}(Y(\omega))\)</span> when both <span class="math inline">\(X(\omega)\)</span> and <span class="math inline">\(Y(\omega)\)</span> are Gaussian (or normal).<label for="tufte-sn-479" class="margin-toggle sidenote-number">479</label><input type="checkbox" id="tufte-sn-479" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">479</span> To see this result, we use the result of the <a href="ch-UnMulti.html#sub:MultiVar">conditional normal expectation</a> given in chapter <a href="ch-UnMulti.html#sub:MultiVar">13.1</a>:
<span class="math display">\[\begin{align*}&amp;\mathbb{E}[X(\omega)|Y(\omega)]=\mathbb{E}[X(\omega)]\\&amp;+\frac{\mbox{Cov}(Y(\omega),X(\omega))}{\mbox{Var}(Y(\omega))}\left(Y(\omega)-\mathbb{E}[Y(\omega)]\right)=\\
&amp;= \mathbb{E}[X(\omega)]-\frac{\mbox{Cov}(Y(\omega),X(\omega))}{\mbox{Var}(Y(\omega))}\mathbb{E}[Y(\omega)]\\
&amp;+ \frac{\mbox{Cov}(Y(\omega),X(\omega))}{\mbox{Var}(Y(\omega))}Y(\omega).
\end{align*}\]</span></span></p>
<p>Now we can extend this idea to a more sophisticated setting. Consider the model of interests as a time series <a href="ch-UnMulti.html#sub:Markov">state vector</a> <span class="math inline">\(\mathbf{X}_{t}(\omega)\in\mathbb{R}^{m}\)</span>. Let the observations be the data vector <span class="math inline">\(\mathbf{y}_{t}\in\mathbb{R}^{n}\)</span>. The specification of the system is<label for="tufte-sn-480" class="margin-toggle sidenote-number">480</label><input type="checkbox" id="tufte-sn-480" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">480</span> Note that <span class="math inline">\(\mathbf{y}_t\)</span> is the realization of some random vector <span class="math inline">\(\mathbf{Y}_t(\omega)\)</span>. That is, <span class="math inline">\(\mathbf{X}_t(\omega)\)</span> does not generate the observations. However, <span class="math inline">\(\mathbf{X}_t(\omega)\)</span> is the model of interest. And it could be the case that <span class="math display">\[\mathbf{X}_t(\omega)=\mathbf{Y}_{t}(\omega).\]</span> However, the second equation of the model simply says that <span class="math inline">\(\mathbf{X}_{t}(\omega)\)</span> may have a linear relationship with the observations <span class="math inline">\(\mathbf{y}_t\)</span>. Thus <span class="math inline">\(\mathbf{X}_t(\omega)\)</span> does not need to equal to (or not even cause the change of) <span class="math inline">\(\mathbf{Y}_t(\omega)\)</span>.</span>
<span class="math display">\[\begin{align*}
\mathbf{X}_{t+1}(\omega)&amp;=  \mathbf{A}\mathbf{X}_{t}(\omega)+\mathbf{C}\mathbf{W}_{1,t}(\omega)\\
\mathbf{y}_{t}&amp;=    \mathbf{B}\mathbf{X}_{t}(\omega)+\mathbf{W}_{2,t}(\omega)\\
\left(\begin{array}{c}
\mathbf{W}_{1,t}(\omega)\\
\mathbf{W}_{2,t}(\omega)
\end{array}\right)  &amp;\sim\mathcal{N}\left(0,\left[\begin{array}{cc}
\mathbf{Q} &amp; 0\\
0 &amp; \mathbf{R}
\end{array}\right]\right)
\end{align*}\]</span>
The <em>filtering procedure</em> is about (re-)constructing a <a href="ch-randomization.html#sub:RHilbert">probabilistic dynamical law</a> of the underlying state <span class="math inline">\(\{\mathbf{X}_{s}(\omega)\}_{s\leq t}\)</span> given all observations <span class="math inline">\(\{\mathbf{y}_{s}\}_{s\leq t-1}\)</span>.
We can see that at time <span class="math inline">\(t\)</span>, <span class="math inline">\(\mathbf{X}_{t}(\omega)\)</span> is a Gaussian random vector in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F}_{t},\mathbb{P};\mathbb{R})\)</span></a>. In addition, the system is <a href="ch-UnMulti.html#sub:Markov">Markovian</a>. Thus, the dynamics of <span class="math inline">\(\{\mathbf{X}_{s}(\omega)\}_{s\leq t}\)</span> is driven by a <a href="ch-UnMulti.html#sub:Markov">transition probability</a> that only depends on the information from the previous period, namely <span class="math inline">\(\mathcal{F}_{t-1}\)</span>. We can express the <a href="ch-randomization.html#sub:RHilbert">probabilistic dynamical law</a> of <span class="math inline">\(\{\mathbf{X}_{s}(\omega)\}_{s\leq t}\)</span> in terms of the <a href="ch-randomization.html#sub:RHilbert">mean and covariance functions</a> of <span class="math inline">\(\mathbf{X}_{t}(\omega)\)</span> conditioning on the previous information <span class="math inline">\(\mathcal{F}_t\)</span>:
<span class="math display">\[\begin{align*}
&amp;\Pr\left(\mathbf{X}_{t}(\omega)|\mathcal{F}_{t-1}\right)   \sim\mathcal{N}\left(\hat{\mathbf{x}}_{t},\hat{\Sigma}_{t}\right)\\
 &amp;\hat{\mathbf{x}}_{t}=\mathbb{E}[\mathbf{X}_{t}(\omega)|\mathcal{F}_{t-1}],\\  &amp; \hat{\Sigma}_{t}=\mathbb{E}\left[\left.(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})^{\top}\right|\mathcal{F}_{t-1}\right].
\end{align*}\]</span>
If we want <span class="math inline">\(\mathbf{X}_{t}\)</span> to resonate with <span class="math inline">\(\mathbf{y}_t\)</span>, the scheme has to make the previous <a href="ch-randomization.html#sub:RHilbert">probabilistic dynamical law</a> incorporate with the observations. For the <a href="ch-randomization.html#sub:RHilbert">mean function</a>, it is about conditioning the new information provided by <span class="math inline">\(\mathbf{y}_t\)</span>. So we just use the previous <a href="ch-UnMulti.html#sub:MultiVar">conditional normal mean</a> formula to incorporate <span class="math inline">\(\mathbf{y}_t\)</span>,
<span class="math display">\[\begin{align*}\mathbb{E}[\mathbf{X}_{t}(\omega)|\mathbf{y}_{t},\mathcal{F}_{t-1}]&amp;=\mathbb{E}[\mathbf{X}_{t}(\omega)|\mathcal{F}_{t-1}]+\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbb{E}\left[\left.\mathbf{y}_{t}\right|\mathcal{F}_{t-1}\right]\right)\\&amp;= \mathbb{E}[\mathbf{X}_{t}(\omega)|\mathcal{F}_{t-1}]+\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbb{E}\left[\left.\mathbf{B}\mathbf{X}_{t}(\omega)+\mathbf{W}_{2,t}(\omega)\right|\mathcal{F}_{t-1}\right]\right)\\
&amp;=  \hat{\mathbf{x}}_{t}+\underset{\mbox{error correction}}{\underbrace{\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)}},\end{align*}\]</span> where the ad hoc term <span class="math inline">\(\mathbf{K}_{t}\)</span> (like the coefficient <span class="math inline">\(b\)</span> in the affine function)
is called (optimal) <em>Kalman gain matrix</em>.</p>
<p>Then one can derive the <a href="ch-randomization.html#sub:RHilbert">covariance function</a> in order to have the full picture of this dynamical law:<label for="tufte-sn-481" class="margin-toggle sidenote-number">481</label><input type="checkbox" id="tufte-sn-481" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">481</span> Here is a quick and dirty “derivation.” In chapter <a href="ch-UnMulti.html#sub:MultiVar">13.1</a>, we know that the <a href="ch-UnMulti.html#sub:MultiVar">conditional normal variance</a> formula gives
<span class="math display">\[\begin{align*}&amp;\mbox{Var}[X(\omega)|Y(\omega)]=\mbox{Var}(X(\omega))\\&amp;-\frac{\mbox{Cov}(Y(\omega),X(\omega))\times\mbox{Cov}(X(\omega),Y(\omega))}{\mbox{Var}(Y(\omega))}.\end{align*}\]</span> Notice that <span class="math inline">\(\mathbf{K}_{t}\)</span>
plays the same role as <span class="math inline">\(\mbox{Cov}(Y(\omega),X(\omega))/\mbox{Var}(Y(\omega))\)</span> in the <a href="ch-UnMulti.html#sub:MultiVar">conditional normal mean</a> projection. So we can guess that <span class="math display">\[\begin{align*}&amp;\mbox{Var}[\mathbf{X}_{t}(\omega)|\mathbf{y}_{t},\mathcal{F}_{t-1}]=   \mbox{Var}(\mathbf{X}_{t}(\omega)|\mathcal{F}_{t-1})\\&amp;-\mathbf{K}_{t}\left[\mbox{Var}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)\right]\mathbf{K}_{t}^{\top}
\\&amp;=    \hat{\Sigma}_{t}-\mathbf{K}_{t}\left[\mbox{Var}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)\right]\mathbf{K}_{t}^{\top}.\end{align*}\]</span> Indeed, it is.</span>
<span class="math display">\[\begin{align*}\Pr\left(\mathbf{X}_{t}(\omega)|\mathbf{y}_{t},\mathcal{F}_{t-1}\right)\sim\mathcal{N}\left(\underset{\mbox{mean}}{\underbrace{\hat{\mathbf{x}}_{t}+\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)}},\\ \underset{\mbox{variance}}{\underbrace{\hat{\Sigma}_{t}-\mathbf{K}_{t}\left[\mbox{Var}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)\right]^{-1}\mathbf{K}_{t}^{\top}}}\right).\end{align*}\]</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:Kalman"></span>
<img src="fig/Part4/Kalman.gif" alt="Kalman filter (Grey dots are observations)" width="100%"><!--
<p class="caption marginnote">-->Figure 16.4: Kalman filter (Grey dots are observations)<!--</p>-->
<!--</div>--></span>
</p>
<p>Roughly speaking, at each time step <span class="math inline">\(t\)</span>, the scheme of <em>Kalman’s filter</em> consists of two steps:</p>
<ol style="list-style-type: decimal">
<li><p>Estimate the mean and covariance of <span class="math inline">\(\mathbf{X}_t(\omega)\)</span> through the probability law that incorporates with the observation <span class="math inline">\(\mathbf{y}_t\)</span>, namely <span class="math inline">\(\Pr\left(\mathbf{X}_{t}(\omega)|\mathbf{y}_t, \mathcal{F}_{t-1}\right)\)</span>.</p></li>
<li><p>Predict the distribution of <span class="math inline">\(\mathbf{y}_{t+1}\)</span> by <span class="math inline">\(\hat{\mathbf{x}}_{t+1}\)</span> where <span class="math inline">\(\hat{\mathbf{X}}_{t+1}\)</span> follows the dynamical law <span class="math inline">\(\Pr\left(\mathbf{X}_{t+1}(\omega)|\mathcal{F}_{t}\right)\)</span>.</p></li>
</ol>
<p><strong>Kalman gain matrix</strong> <span class="math inline">\(\mathbf{K}_{t}\)</span> gives the cornerstone of this filtering procedure. The matrix is to form the projection, so it has to be chosen to minimize the distance (as the projection) between the errors under the information of <span class="math inline">\(\mathcal{F}_{t-1}\)</span> and <span class="math inline">\(\mathbf{y}_t\)</span>:<label for="tufte-sn-482" class="margin-toggle sidenote-number">482</label><input type="checkbox" id="tufte-sn-482" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">482</span> Here the <a href="">filtration set</a> <span class="math inline">\(\mathcal{F}_t\)</span> consists of the information of <span class="math inline">\(\mathcal{F}_{t-1}\)</span> and <span class="math inline">\(\mathbf{y}_{t}\)</span>. The space <span class="math inline">\(L_{2}(\Omega,\mathcal{F}_{t},\mathbb{P};\mathbb{R}^{m\times n})\)</span> shares the probability law(s) generating the information <span class="math inline">\(\mathcal{F}_{t-1}\)</span> and <span class="math inline">\(\mathbf{y}_{t}\)</span>, the range set of this linear operator space is a matrix in <span class="math inline">\(\mathbb{R}^{m\times n}\)</span>.</span>
<span class="math display">\[\min_{\mathbf{K}_{t}\in L_{2}(\Omega,\mathcal{F}_{t},\mathbb{P};\mathbb{R}^{m\times n})}\left\Vert \underset{\mbox{error of estimate}}{\underbrace{\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t}}}-\mathbf{K}_{t}\underset{\mbox{error of prediction}}{\underbrace{\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)}}\right\Vert _{\mathbb{P}}\]</span></p>
<p>The exact expression of <span class="math inline">\(\mathbf{K}_{t}\)</span> is given below.</p>
<div class="solution">
<p class="solution-begin">
Kalman gain matrix <span id="sol-start-197" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-197', 'sol-start-197')"></span>
</p>
<div id="sol-body-197" class="solution-body" style="display: none;">
<p>The mean and covariance matrix of dynamical law are
<span class="math display">\[\begin{align*}
\hat{\mathbf{x}}_{t}    &amp;=\mathbb{E}[\mathbf{X}_{t}(\omega)|\mathcal{F}_{t-1}]\\
\hat{\Sigma}_{t}    &amp;=\mathbb{E}\left[\left.(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})^{\top}\right|\mathcal{F}_{t-1}\right]\end{align*}
\]</span></p>
<p>Note that <span class="math display">\[\mathbb{E}\left[\left.\mathbf{B}\mathbf{X}_{t}(\omega)+\mathbf{W}_{2,t}(\omega)\right|\mathcal{F}_{t-1}\right]=\mathbf{B}\mathbb{E}\left[\left.\mathbf{X}_{t}(\omega)\right|\mathcal{F}_{t-1}\right]=\mathbf{B}\hat{\mathbf{x}}_{t}.\]</span>
Also
<span class="math display">\[\begin{align*}
\mbox{Variance of projection error}&amp;=   \mathbb{E}\left[\left.(\mathbf{y}-\mathbf{B}\hat{\mathbf{x}}_{t})(\mathbf{y}-\mathbf{B}\hat{\mathbf{x}}_{t})^{\top}\right|\mathcal{F}_{t-1}\right]\\
&amp;=  \mathbb{E}\left[\left.(\mathbf{B}\mathbf{X}_{t}(\omega)+\mathbf{W}_{2,t}(\omega)-\mathbf{B}\hat{\mathbf{x}}_{t})(\mathbf{B}\mathbf{X}_{t}(\omega)+\mathbf{W}_{2,t}(\omega)-\mathbf{B}\hat{\mathbf{x}}_{t})^{\top}\right|\mathcal{F}_{t-1}\right]\\
&amp;=  \mathbf{B}\underset{\hat{\Sigma}_{t}}{\underbrace{\mathbb{E}\left[\left.(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})^{\top}\right|\mathcal{F}_{t-1}\right]}}\mathbf{B}^{\top}+\underset{\mathbf{R}}{\underbrace{\mathbb{E}\left[\left.\mathbf{W}_{2,t}(\omega)\mathbf{W}_{2,t}(\omega)^{\top}\right|\mathcal{F}_{t-1}\right]}}\\
&amp;=  \mathbf{B}\hat{\Sigma}_{t}\mathbf{B}+\mathbf{R}.
\end{align*}\]</span></p>
<p>Now we can derive the Kalman gain matrix. Because the updating rule must follow the conditional expectation <span class="math display">\[\mathbb{E}[\mathbf{X}_{t}(\omega)|\mathbf{y}_{t}]=\mathbb{E}[\mathbf{X}_{t}(\omega)|\mathcal{F}_{t-1}]+\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right),\]</span>
or say <span class="math inline">\(\mathbb{E}[\mathbf{X}_{t}(\omega)|\mathbf{y}_{t}]=\hat{\mathbf{x}}_{t}+\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)\)</span>.
To obtain the “optimality”, <span class="math inline">\(\mathbf{K}_{t}\)</span> has to be chosen to minimize the error covariance of the projection under the information <span class="math inline">\(\mathcal{F}_{t-1}\)</span>. <span class="math display">\[\min_{\mathbf{K}_{t}}\left\Vert \mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t}-\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)\right\Vert _{\mathbb{P}}\]</span>
Recall that the minimization can be equivalently stated as the orthogonal condition. We have <span class="math display">\[\mbox{Cov}_{t-1}\left(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t}-\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right),\:\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)=0,\]</span>
which implies <span class="math display">\[\underset{(*)}{\underbrace{\mbox{Cov}_{t-1}\left(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t},\:\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)}}-\mathbf{K}_{t}\underset{\mbox{Variance of projection error}}{\underbrace{\mbox{Cov}_{t-1}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t},\:\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)}}=0.\]</span>
Note the <span class="math inline">\((*)\)</span> term can be simplified to<span class="math display">\[\begin{align*}&amp;\mbox{Cov}\left(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t},\:\mathbf{B}\mathbf{X}_{t}(\omega)+\mathbf{W}_{2,t}(\omega)-\mathbf{B}\hat{\mathbf{x}}_{t}\right)\\&amp;=\underset{\hat{\Sigma}_{t}}{\underbrace{\mathbb{E}\left[\left.(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})^{\top}\right|\mathcal{F}_{t-1}\right]}}\mathbf{B}^{\top}\\
&amp;+\mathbb{E}\left[\left.(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})\mathbf{W}_{2,t}(\omega)^{\top}\right|\mathcal{F}_{t-1}\right]=\hat{\Sigma}_{t}\mathbf{B}^{\top}.\end{align*}\]</span>
Then substitute the simplified <span class="math inline">\((*)\)</span>
back into the orthogonal condition, we have <span class="math display">\[\hat{\Sigma}_{t}\mathbf{B}^{\top}-\mathbf{K}_{t}\left(\mathbf{B}\hat{\Sigma}_{t}\mathbf{B}+\mathbf{R}\right)=0,\]</span>
namely <span class="math inline">\(\mathbf{K}_{t}=\hat{\Sigma}_{t}\mathbf{B}^{\top}(\mathbf{B}\hat{\Sigma}_{t}\mathbf{B}+\mathbf{R})^{-1}\)</span>
which is the <strong>Kalman gain</strong>.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>The <strong>Kalman gain</strong> <span class="math inline">\(\mathbf{K}_t(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t})\)</span> basically is a correction term, and it represents the amount by which to correct the propagated state estimate due to the measurement <span class="math inline">\(\mathbf{y}_{t}\)</span>. In other words, <strong>Kalman’s filter</strong> provides the best linear approximation of the observation vector <span class="math inline">\(\hat{\mathbf{y}}_{t}\)</span> when the projection <span class="math inline">\(\mathbf{K}_t\)</span> can minimize the distance between the error vectors <span class="math inline">\(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t}\)</span> and <span class="math inline">\(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\)</span>.</p>
<div class="solution">
<p class="solution-begin">
code <span id="sol-start-198" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-198', 'sol-start-198')"></span>
</p>
<div id="sol-body-198" class="solution-body" style="display: none;">
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb140-1" data-line-number="1">Kalman.filter =<span class="st"> </span><span class="cf">function</span>(A, C, B, Q, R, y, xhat, Sigma){</a>
<a class="sourceLine" id="cb140-2" data-line-number="2">  <span class="co"># x[t+1] = Ax[t] + Cv[t]</span></a>
<a class="sourceLine" id="cb140-3" data-line-number="3">  <span class="co"># y[t] = Bx[t] + w[t]</span></a>
<a class="sourceLine" id="cb140-4" data-line-number="4">  <span class="co"># v[t] ~ N(0, Q)</span></a>
<a class="sourceLine" id="cb140-5" data-line-number="5">  <span class="co"># w[t] ~ N(0, R)</span></a>
<a class="sourceLine" id="cb140-6" data-line-number="6">  <span class="co"># y = observation value at [t]</span></a>
<a class="sourceLine" id="cb140-7" data-line-number="7">  <span class="co"># xhat = prior state estimates at [t-1]</span></a>
<a class="sourceLine" id="cb140-8" data-line-number="8">  <span class="co"># Sigma    = posteriror state variance at [t-1]</span></a>
<a class="sourceLine" id="cb140-9" data-line-number="9">  <span class="co"># prior and posterior</span></a>
<a class="sourceLine" id="cb140-10" data-line-number="10">  <span class="co"># xpri = xpri[t+1], xpost = xpost[t], </span></a>
<a class="sourceLine" id="cb140-11" data-line-number="11">  <span class="co"># Sigmapri = Sigmapri[t+1], Sigmapost = Sigmapost[t],</span></a>
<a class="sourceLine" id="cb140-12" data-line-number="12">  <span class="co"># K = Kalman gain at t</span></a>
<a class="sourceLine" id="cb140-13" data-line-number="13">  y =<span class="st"> </span><span class="kw">matrix</span>(y, <span class="dt">ncol=</span><span class="dv">1</span>); xhat =<span class="st"> </span><span class="kw">matrix</span>(xhat, <span class="dt">ncol=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb140-14" data-line-number="14">  <span class="co"># error/innovataion term</span></a>
<a class="sourceLine" id="cb140-15" data-line-number="15">  error =<span class="st"> </span>y <span class="op">-</span><span class="st"> </span>B <span class="op">%*%</span><span class="st"> </span>xhat</a>
<a class="sourceLine" id="cb140-16" data-line-number="16">  V.error =<span class="st"> </span>B <span class="op">%*%</span><span class="st"> </span>Sigma <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(B) <span class="op">+</span><span class="st"> </span>R</a>
<a class="sourceLine" id="cb140-17" data-line-number="17">  <span class="co"># Kalman gain</span></a>
<a class="sourceLine" id="cb140-18" data-line-number="18">  K =<span class="st"> </span>Sigma <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(B) <span class="op">%*%</span><span class="st"> </span><span class="kw">solve</span>(V.error)</a>
<a class="sourceLine" id="cb140-19" data-line-number="19">  <span class="co"># filtering step</span></a>
<a class="sourceLine" id="cb140-20" data-line-number="20">   <span class="cf">if</span>(<span class="op">!</span><span class="kw">any</span>(<span class="kw">is.na</span>(error))){</a>
<a class="sourceLine" id="cb140-21" data-line-number="21">    xpost =<span class="st"> </span>xhat <span class="op">+</span><span class="st"> </span>K <span class="op">%*%</span><span class="st"> </span>error</a>
<a class="sourceLine" id="cb140-22" data-line-number="22">    Sigmapost =<span class="st"> </span>Sigma <span class="op">-</span><span class="st"> </span>K <span class="op">%*%</span><span class="st"> </span>V.error <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(K) <span class="co"># variance</span></a>
<a class="sourceLine" id="cb140-23" data-line-number="23">  } <span class="cf">else</span>{</a>
<a class="sourceLine" id="cb140-24" data-line-number="24">    <span class="co"># NA</span></a>
<a class="sourceLine" id="cb140-25" data-line-number="25">    xpost =<span class="st"> </span>xhat</a>
<a class="sourceLine" id="cb140-26" data-line-number="26">    Sigmapost =<span class="st"> </span>Sigma</a>
<a class="sourceLine" id="cb140-27" data-line-number="27">  }</a>
<a class="sourceLine" id="cb140-28" data-line-number="28"> </a>
<a class="sourceLine" id="cb140-29" data-line-number="29">  <span class="co"># one-step-ahead forecast (prior in the next period)</span></a>
<a class="sourceLine" id="cb140-30" data-line-number="30">  xpred =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>xpost; Sigmapred =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>Sigmapost <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(A) <span class="op">+</span><span class="st"> </span><span class="kw">t</span>(C) <span class="op">%*%</span><span class="st"> </span>Q <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(C) </a>
<a class="sourceLine" id="cb140-31" data-line-number="31">  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">xpost=</span>xpost, <span class="dt">xpred=</span>xpred, <span class="dt">Sigmapost=</span>Sigmapost, <span class="dt">Sigmapred=</span>Sigmapred, <span class="dt">K=</span>K))</a>
<a class="sourceLine" id="cb140-32" data-line-number="32">}</a>
<a class="sourceLine" id="cb140-33" data-line-number="33"></a>
<a class="sourceLine" id="cb140-34" data-line-number="34"></a>
<a class="sourceLine" id="cb140-35" data-line-number="35"><span class="co">## Data and Model</span></a>
<a class="sourceLine" id="cb140-36" data-line-number="36">N      =<span class="st"> </span><span class="dv">30</span></a>
<a class="sourceLine" id="cb140-37" data-line-number="37"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb140-38" data-line-number="38">y =<span class="st"> </span><span class="kw">arima.sim</span>(<span class="dt">model=</span><span class="kw">list</span>(<span class="dt">ar=</span><span class="fl">0.5</span>, <span class="dt">ma=</span><span class="fl">0.2</span>),<span class="dt">n=</span>N, <span class="dt">innov=</span><span class="kw">rnorm</span>(N))</a>
<a class="sourceLine" id="cb140-39" data-line-number="39">y =<span class="st"> </span>y <span class="op">+</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="dv">1</span><span class="op">:</span>N</a>
<a class="sourceLine" id="cb140-40" data-line-number="40"></a>
<a class="sourceLine" id="cb140-41" data-line-number="41"><span class="co"># data y</span></a>
<a class="sourceLine" id="cb140-42" data-line-number="42"></a>
<a class="sourceLine" id="cb140-43" data-line-number="43">y =<span class="st"> </span><span class="kw">matrix</span>(y, <span class="dt">nrow=</span>N, <span class="dt">ncol=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb140-44" data-line-number="44"></a>
<a class="sourceLine" id="cb140-45" data-line-number="45"><span class="co"># system: x[t+1] = Ax[t] + Cv[t]</span></a>
<a class="sourceLine" id="cb140-46" data-line-number="46"><span class="co"># obs:    y[t]   = Bx[t] + e[t]</span></a>
<a class="sourceLine" id="cb140-47" data-line-number="47"><span class="co"># v[t] ~ NID(0,Q), e[t] ~ NID(0, R)</span></a>
<a class="sourceLine" id="cb140-48" data-line-number="48"></a>
<a class="sourceLine" id="cb140-49" data-line-number="49"></a>
<a class="sourceLine" id="cb140-50" data-line-number="50">A =<span class="st">  </span><span class="kw">matrix</span>(<span class="kw">c</span>(.<span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb140-51" data-line-number="51">               <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb140-52" data-line-number="52">               <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb140-53" data-line-number="53">               <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">nrow=</span><span class="dv">4</span>, <span class="dt">ncol=</span><span class="dv">4</span>, <span class="dt">byrow =</span> T)</a>
<a class="sourceLine" id="cb140-54" data-line-number="54">B =<span class="st">  </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="dt">nrow=</span><span class="dv">1</span>, <span class="dt">ncol=</span><span class="dv">4</span>, <span class="dt">byrow =</span> T)</a>
<a class="sourceLine" id="cb140-55" data-line-number="55">C =<span class="st">  </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb140-56" data-line-number="56">               <span class="fl">.5</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb140-57" data-line-number="57">               <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb140-58" data-line-number="58">               <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="dt">nrow=</span><span class="dv">4</span>, <span class="dt">ncol=</span><span class="dv">4</span>, <span class="dt">byrow =</span> T)</a>
<a class="sourceLine" id="cb140-59" data-line-number="59">Q =<span class="st"> </span><span class="kw">diag</span>(<span class="dv">1</span>, <span class="dv">4</span>); R =<span class="st"> </span><span class="kw">diag</span>(<span class="dv">0</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb140-60" data-line-number="60"></a>
<a class="sourceLine" id="cb140-61" data-line-number="61"><span class="co"># Recurisve filtering the data y</span></a>
<a class="sourceLine" id="cb140-62" data-line-number="62"></a>
<a class="sourceLine" id="cb140-63" data-line-number="63">recursive.kalman =<span class="st"> </span><span class="cf">function</span>(A, C, B, Q, R, y, xini, Sigmaini){</a>
<a class="sourceLine" id="cb140-64" data-line-number="64">  N =<span class="st"> </span><span class="kw">nrow</span>(y); err =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,N)</a>
<a class="sourceLine" id="cb140-65" data-line-number="65">  xpri =<span class="st"> </span>xpost  =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>N, <span class="dt">ncol=</span><span class="kw">nrow</span>(A))</a>
<a class="sourceLine" id="cb140-66" data-line-number="66">  Sigma =<span class="st"> </span>K =<span class="st"> </span><span class="kw">list</span>(<span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span><span class="kw">nrow</span>(A), <span class="dt">ncol=</span><span class="kw">nrow</span>(A)))</a>
<a class="sourceLine" id="cb140-67" data-line-number="67">  <span class="co"># Initial state</span></a>
<a class="sourceLine" id="cb140-68" data-line-number="68">  xpri[<span class="dv">1</span>, ] =<span class="st"> </span>xini</a>
<a class="sourceLine" id="cb140-69" data-line-number="69">  Sigma[[<span class="dv">1</span>]]    =<span class="st"> </span>Sigmaini</a>
<a class="sourceLine" id="cb140-70" data-line-number="70">  <span class="co"># Run the filter at every time t</span></a>
<a class="sourceLine" id="cb140-71" data-line-number="71">  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N){</a>
<a class="sourceLine" id="cb140-72" data-line-number="72">    temp     =<span class="st"> </span><span class="kw">Kalman.filter</span>(<span class="dt">A =</span> A, <span class="dt">C =</span> C, <span class="dt">B =</span> B, <span class="dt">Q =</span> Q, <span class="dt">R =</span> R,</a>
<a class="sourceLine" id="cb140-73" data-line-number="73">                         <span class="dt">y =</span> y[i, ], <span class="dt">xhat =</span> xpri[i, ], <span class="dt">Sigma =</span> Sigma[[i]])</a>
<a class="sourceLine" id="cb140-74" data-line-number="74">    xpost[i,] =<span class="st"> </span>temp<span class="op">$</span>xpost</a>
<a class="sourceLine" id="cb140-75" data-line-number="75">    <span class="cf">if</span>(i <span class="op">&lt;</span><span class="st"> </span>N)</a>
<a class="sourceLine" id="cb140-76" data-line-number="76">      xpri[i<span class="op">+</span><span class="dv">1</span>,] =<span class="st"> </span>temp<span class="op">$</span>xpred; Sigma[[i<span class="op">+</span><span class="dv">1</span>]] =<span class="st"> </span>temp<span class="op">$</span>Sigmapred; K[[i]] =<span class="st"> </span>temp<span class="op">$</span>K; </a>
<a class="sourceLine" id="cb140-77" data-line-number="77">  }</a>
<a class="sourceLine" id="cb140-78" data-line-number="78">  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">xpost=</span>xpost, <span class="dt">xpri=</span>xpri, <span class="dt">Sigma=</span>Sigma, <span class="dt">K=</span>K))</a>
<a class="sourceLine" id="cb140-79" data-line-number="79">}</a>
<a class="sourceLine" id="cb140-80" data-line-number="80"></a>
<a class="sourceLine" id="cb140-81" data-line-number="81"><span class="co">#</span></a>
<a class="sourceLine" id="cb140-82" data-line-number="82">result =<span class="st"> </span><span class="kw">recursive.kalman</span>(A, C, B, Q, R, y, <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), <span class="kw">diag</span>(<span class="fl">1.5</span>, <span class="dv">4</span>))</a></code></pre></div>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
</div>
<div id="example-global-optimization" class="section level2">
<h2>
<span class="header-section-number">16.4</span> Example: Global Optimization</h2>
<p>Systems with many degrees of freedom are by necessity stochastic. In this kind of system, stochastic events are de facto (hierarchical) interplays of a great number of units. The freedom allows the system’s entities, such as white noises, to undergo interactions on the basis of some simple dynamics. Then at the phenomenological level, where some collective properties describe the system, a few principal variables such as the <a href="ch-CalUn.html#sub:ex">moments</a> of a (conditional) probability function disclose more complex trajectories.
These collective trajectories, however, could be indistinguishable from some deterministic counterparts.<label for="tufte-sn-483" class="margin-toggle sidenote-number">483</label><input type="checkbox" id="tufte-sn-483" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">483</span> For example, consider a <a href="ch-randomization.html#sub:SumDecomp">weakly stationary</a> <a href="ch-eigen.html#sub:matNorms">AR(1)</a> model <span class="math display">\[X_t(\omega)=c X_{t-1}(\omega)+\varepsilon_t(\omega)\]</span> with <span class="math inline">\(\mathbb{E}[X_t]=\mu\)</span> for all <span class="math inline">\(t\)</span>, the phenomenological dynamics of the system is around the deterministic trajectory <span class="math inline">\(c^{t}\mu\)</span>.</span> In other words, if one only cares about the intrinsic dynamics of this system, i.e., the projection of random elements onto some determined space, then one can simply focus on the deterministic trend and throw the “illusive” randomness away. You may then ask what the benefit of randomizing a deterministic system and the cost to pay for such randomization is? Let’s respond to these questions by comparing some deterministic and stochastic techniques.</p>
<p>Consider the following optimization problem of searching the optimal <span class="math inline">\(\mathbf{x}\in\mathbb{R}^{n}\)</span> of some objection function <span class="math inline">\(f(\cdot):\mathbb{R}^{n}\rightarrow \mathbb{R}\)</span> A simple <a href="sub-inferknow.html#sub:comAlgo">algorithm</a> in optimization is to iterate the argument of the objective function by the <a href="ch-vecMat.html#sub:linearity">gradient</a>. Suppose that we seek to minimize a functional <span class="math inline">\(f\)</span>, and suppose that an initial point <span class="math inline">\(\mathbf{x}_{0}\)</span> is given. The iteration follows <a href="ch-DE.html#sub:EulerScheme">Euler’s scheme</a>
<span class="math display">\[\mathbf{x}_{t}=\mathbf{x}_{t-1}-\epsilon\nabla f(\mathbf{x}_{t-1})\]</span>
where <span class="math inline">\(\epsilon\)</span> is a scalar in <span class="math inline">\(\mathbb{R}\)</span> and <span class="math inline">\(\nabla f(\mathbf{x})\)</span> is a (directional) vector so that
<span class="math display">\[f(\mathbf{x}_{t})=f(\mathbf{x}_{t-1}-\epsilon\nabla f(\mathbf{x}_{t-1}))&lt;f(\mathbf{x}_{t-1})\]</span>
holds when <span class="math inline">\(\mathbf{x}_{t-1}\)</span> moves in the direction of <span class="math inline">\(-\nabla f(\mathbf{x})\)</span>.<label for="tufte-sn-484" class="margin-toggle sidenote-number">484</label><input type="checkbox" id="tufte-sn-484" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">484</span> It is easy to see that if some <span class="math inline">\(\mathbf{x}^{*}\)</span> satisfies the condition <span class="math inline">\(\nabla f(\mathbf{x}^{*})=0\)</span>, the <a href="ch-DE.html#sub:EulerScheme">Euler’s scheme</a> arrives at the <a href="ch-DE.html#sub:ode">equilibrium</a> or <a href="ch-DE.html#sub:ode">stationary</a> state, namely <span class="math display">\[\lim_{n\rightarrow\infty}\mathbf{x}_{t}=\mathbf{x}^{*}.\]</span> In addition, because the gradient of the continuous function equals zero at the <a href="#sub:optimization">optimal point</a>, this scheme’s <a href="ch-DE.html#sub:ode">equilibrium point</a> is also the solution of the optimization problem <span class="math display">\[\min_{\mathbf{x}}f(\mathbf{x}).\]</span></span> We can randomize this algorithm by adding some Gaussian white noise <span class="math inline">\(\varepsilon(\omega) \sim \mathcal{N}(0,\sigma^2)\)</span> at each iteration: <span class="math display">\[\mathbf{X}_{t}(\omega)=\mathbf{X}_{t-1}(\omega)-\epsilon\nabla f(\mathbf{X}_{t-1}(\omega)) + \varepsilon_{t-1}(\omega).\]</span>
As you can see, the randomized algorithm is similar to the <a href="ch-UnMulti.html#sub:rw">random walk</a> model with a stochastic gradient term.</p>
<p>The following numerical illustration performs the minimization of <span class="math inline">\(x^2 -\sin(x)\)</span> using the above two schemes. The <a href="ch-optApp.html#sub:Proj2">objective function</a> of this problem has a unique minimum between zero and one. (See figure <a href="ch-randomization.html#fig:simpleObj">16.5</a>).</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:simpleObj"></span>
<img src="fig/Part4/simpleObj.png" alt="Objective function and its derivative   " width="100%"><!--
<p class="caption marginnote">-->Figure 16.5: Objective function and its derivative <!--</p>-->
<!--</div>--></span>
</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb141-1" data-line-number="1"><span class="co"># objective function and its gradient</span></a>
<a class="sourceLine" id="cb141-2" data-line-number="2">ftn =<span class="st"> </span><span class="cf">function</span>(x) {fx =<span class="st"> </span>x<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="kw">sin</span>(x); dfx =<span class="st"> </span><span class="dv">2</span><span class="op">*</span>x <span class="op">-</span><span class="st"> </span><span class="kw">cos</span>(x); <span class="kw">return</span>(<span class="kw">c</span>(fx, dfx))}</a>
<a class="sourceLine" id="cb141-3" data-line-number="3"><span class="co"># optimization</span></a>
<a class="sourceLine" id="cb141-4" data-line-number="4"><span class="kw">set.seed</span>(<span class="dv">2021</span>); x =<span class="st"> </span>x.s =<span class="st"> </span><span class="dv">1</span> ; fx =<span class="st"> </span>fx.s =<span class="st"> </span><span class="kw">ftn</span>(x); epsilon =<span class="st"> </span>sigma =<span class="st"> </span><span class="fl">0.1</span>; </a>
<a class="sourceLine" id="cb141-5" data-line-number="5">iter =<span class="st"> </span>iter.s =<span class="st"> </span><span class="dv">0</span>; max.iter =<span class="st"> </span><span class="dv">1000</span>; tol =<span class="st"> </span><span class="fl">1e-3</span>; <span class="co"># try epsilon = sigma = 0.01; tol=1e-5; max.iter=10000</span></a>
<a class="sourceLine" id="cb141-6" data-line-number="6"><span class="cf">while</span> ((<span class="kw">abs</span>(fx[<span class="dv">2</span>]) <span class="op">&gt;</span><span class="st"> </span>tol) <span class="op">&amp;&amp;</span><span class="st"> </span>(iter <span class="op">&lt;</span><span class="st"> </span>max.iter)) {</a>
<a class="sourceLine" id="cb141-7" data-line-number="7">    x =<span class="st"> </span>x <span class="op">-</span><span class="st"> </span>epsilon<span class="op">*</span>fx[<span class="dv">2</span>]; <span class="co"># gradient search</span></a>
<a class="sourceLine" id="cb141-8" data-line-number="8">    fx =<span class="st"> </span><span class="kw">ftn</span>(x); iter =<span class="st"> </span>iter <span class="op">+</span><span class="st"> </span><span class="dv">1</span>}</a>
<a class="sourceLine" id="cb141-9" data-line-number="9"><span class="cf">while</span> ((<span class="kw">abs</span>(fx.s[<span class="dv">2</span>]) <span class="op">&gt;</span><span class="st"> </span>tol) <span class="op">&amp;&amp;</span><span class="st"> </span>(iter.s <span class="op">&lt;</span><span class="st"> </span>max.iter)) {</a>
<a class="sourceLine" id="cb141-10" data-line-number="10">    x.s =<span class="st"> </span>x.s <span class="op">-</span><span class="st"> </span>epsilon<span class="op">*</span>fx.s[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>sigma<span class="op">*</span><span class="kw">rnorm</span>(<span class="dv">1</span>,<span class="dv">0</span>); <span class="co"># stochastic gradient search</span></a>
<a class="sourceLine" id="cb141-11" data-line-number="11">    fx.s =<span class="st"> </span><span class="kw">ftn</span>(x.s); iter.s =<span class="st"> </span>iter.s <span class="op">+</span><span class="st"> </span><span class="dv">1</span>}</a>
<a class="sourceLine" id="cb141-12" data-line-number="12"></a>
<a class="sourceLine" id="cb141-13" data-line-number="13"><span class="cf">if</span> ((<span class="kw">abs</span>(fx[<span class="dv">2</span>]) <span class="op">&gt;</span><span class="st"> </span>tol) <span class="op">|</span><span class="st"> </span>(<span class="kw">abs</span>(fx.s[<span class="dv">2</span>]) <span class="op">&gt;</span><span class="st"> </span>tol)) {<span class="kw">return</span>(<span class="ot">NULL</span>)} <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb141-14" data-line-number="14">  <span class="kw">cat</span>(<span class="st">"Deterministic x:"</span>, x,  <span class="st">"converges at"</span>, iter, <span class="st">"iteration </span><span class="ch">\n</span><span class="st">"</span>)</a>
<a class="sourceLine" id="cb141-15" data-line-number="15">  <span class="kw">cat</span>(<span class="st">"Randomized X:"</span>, x.s, <span class="st">"converges at"</span>, iter.s, <span class="st">"iteration </span><span class="ch">\n</span><span class="st">"</span>)}</a></code></pre></div>
<pre><code>## Deterministic x: 0.4505276 converges at 26 iteration 
## Randomized X: 0.4498334 converges at 482 iteration</code></pre>
<p>In this example, given the same degree of accuracy for the convergences (<span class="math inline">\(0.001\)</span>), we can see that the number of iterations in the randomized algorithm is about <span class="math inline">\(19\)</span> times larger than the deterministic one. That’s to say, the stochastic algorithm is less efficient than the deterministic counterpart for this optimization problem.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:stochOpt"></span>
<img src="fig/Part4/stochOpt.png" alt="Objective function with many local optima" width="100%"><!--
<p class="caption marginnote">-->Figure 16.6: Objective function with many local optima<!--</p>-->
<!--</div>--></span>
</p>
<p>Now let’s consider another optimization problem with a slightly more complex objective function. Figure <a href="ch-randomization.html#fig:stochOpt">16.6</a> is about maximizing <span class="math inline">\(f(x_1, x_2)\)</span> where <span class="math inline">\(f(\cdot,\cdot)\)</span> has many <strong>local optima</strong>. Let the initial point be on a pseudo plateau in the central basin (the yellow square). The deterministic and randomized search algorithms are similar to the previous setting. However, this time, the deterministic algorithm (after around <span class="math inline">\(1500\)</span> iterations) gets stuck on the pseudo plateau (the red triangle). In contrast, the stochastic algorithm’s search trajectory (the blue path), although it does not seem to converge, explores several peaks that are much higher than the heaps in the central valley.<label for="tufte-sn-485" class="margin-toggle sidenote-number">485</label><input type="checkbox" id="tufte-sn-485" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">485</span> he algorithm fails to converge due to “unbounded” numerical gradients generated by the peaks.</span></p>
<div class="solution">
<p class="solution-begin">
code <span id="sol-start-199" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-199', 'sol-start-199')"></span>
</p>
<div id="sol-body-199" class="solution-body" style="display: none;">
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb143-1" data-line-number="1">obj=<span class="cf">function</span>(x){(x[<span class="dv">1</span>]<span class="op">*</span><span class="kw">sin</span>(<span class="dv">20</span><span class="op">*</span>x[<span class="dv">2</span>])<span class="op">+</span>x[<span class="dv">2</span>]<span class="op">*</span><span class="kw">sin</span>(<span class="dv">20</span><span class="op">*</span>x[<span class="dv">1</span>]))<span class="op">^</span><span class="dv">2</span><span class="op">*</span><span class="kw">cosh</span>(<span class="kw">sin</span>(<span class="dv">10</span><span class="op">*</span>x[<span class="dv">1</span>])<span class="op">*</span>x[<span class="dv">1</span>])<span class="op">+</span></a>
<a class="sourceLine" id="cb143-2" data-line-number="2">(x[<span class="dv">1</span>]<span class="op">*</span><span class="kw">cos</span>(<span class="dv">10</span><span class="op">*</span>x[<span class="dv">2</span>])<span class="op">+</span>x[<span class="dv">2</span>]<span class="op">*</span><span class="kw">sin</span>(<span class="dv">10</span><span class="op">*</span>x[<span class="dv">1</span>]))<span class="op">^</span><span class="dv">2</span><span class="op">*</span><span class="kw">cosh</span>(<span class="kw">cos</span>(<span class="dv">20</span><span class="op">*</span>x[<span class="dv">2</span>])<span class="op">*</span>x[<span class="dv">2</span>])}</a>
<a class="sourceLine" id="cb143-3" data-line-number="3"></a>
<a class="sourceLine" id="cb143-4" data-line-number="4">init=<span class="kw">c</span>(.<span class="dv">65</span>,.<span class="dv">8</span>); xvec =<span class="st"> </span>xvec.s =<span class="st"> </span><span class="kw">matrix</span>(init,<span class="dt">ncol=</span><span class="dv">2</span>); epsilon=<span class="fl">0.01</span>; sigma =<span class="fl">0.1</span>;</a>
<a class="sourceLine" id="cb143-5" data-line-number="5">grad =<span class="st"> </span>grad.s =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.1</span>,<span class="fl">0.1</span>),<span class="dt">ncol=</span><span class="dv">2</span>); grad.norm =<span class="st"> </span>grad.norm.s =iter=iter.s=<span class="dv">1</span>; max.iter =<span class="st"> </span><span class="dv">2000</span>;</a>
<a class="sourceLine" id="cb143-6" data-line-number="6">obj.cur=obj.val =<span class="st"> </span>obj.cur.s =<span class="st"> </span>obj.val.s =<span class="kw">obj</span>(init);</a>
<a class="sourceLine" id="cb143-7" data-line-number="7"></a>
<a class="sourceLine" id="cb143-8" data-line-number="8"><span class="cf">while</span> ((grad.norm<span class="op">&gt;</span><span class="fl">10e-3</span>) <span class="op">&amp;&amp;</span><span class="st"> </span>(iter <span class="op">&lt;</span><span class="st"> </span>max.iter)){</a>
<a class="sourceLine" id="cb143-9" data-line-number="9">  obj.cur =<span class="st"> </span><span class="kw">obj</span>(xvec[iter, ]);</a>
<a class="sourceLine" id="cb143-10" data-line-number="10">  x =<span class="st"> </span>xvec[iter,] <span class="op">+</span><span class="st">  </span>epsilon<span class="op">*</span>grad ; <span class="co"># deterministic search</span></a>
<a class="sourceLine" id="cb143-11" data-line-number="11">  xvec=<span class="kw">rbind</span>(xvec,x); obj.val =<span class="st"> </span><span class="kw">rbind</span>(obj.val, <span class="kw">obj</span>(x));</a>
<a class="sourceLine" id="cb143-12" data-line-number="12">  grad =<span class="st"> </span>epsilon<span class="op">*</span>(<span class="kw">obj</span>(x)<span class="op">-</span>obj.cur)<span class="op">/</span>(x <span class="op">-</span><span class="st"> </span>xvec[iter,]);</a>
<a class="sourceLine" id="cb143-13" data-line-number="13">  grad.norm=<span class="kw">sqrt</span>(<span class="kw">norm</span>(grad));</a>
<a class="sourceLine" id="cb143-14" data-line-number="14">  iter=iter<span class="op">+</span><span class="dv">1</span>;</a>
<a class="sourceLine" id="cb143-15" data-line-number="15">  } </a>
<a class="sourceLine" id="cb143-16" data-line-number="16"></a>
<a class="sourceLine" id="cb143-17" data-line-number="17"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb143-18" data-line-number="18"><span class="cf">while</span> ((grad.norm.s<span class="op">&gt;</span><span class="fl">10e-3</span>) <span class="op">&amp;&amp;</span><span class="st"> </span>(iter.s <span class="op">&lt;</span><span class="st"> </span>max.iter)){</a>
<a class="sourceLine" id="cb143-19" data-line-number="19">  obj.cur.s =<span class="st"> </span><span class="kw">obj</span>(xvec.s[iter.s, ]);</a>
<a class="sourceLine" id="cb143-20" data-line-number="20">  x.s =<span class="st"> </span>xvec.s[iter.s,] <span class="op">+</span><span class="st">  </span>epsilon<span class="op">*</span>grad.s <span class="op">+</span><span class="st"> </span>sigma<span class="op">*</span><span class="kw">rnorm</span>(<span class="dv">2</span>); <span class="co"># stochastic search</span></a>
<a class="sourceLine" id="cb143-21" data-line-number="21">  xvec.s=<span class="kw">rbind</span>(xvec.s,x.s); obj.val.s =<span class="st"> </span><span class="kw">rbind</span>(obj.val.s, <span class="kw">obj</span>(x.s));</a>
<a class="sourceLine" id="cb143-22" data-line-number="22">  grad.s =<span class="st"> </span>epsilon<span class="op">*</span>(<span class="kw">obj</span>(x.s)<span class="op">-</span>obj.cur.s)<span class="op">/</span>(x.s <span class="op">-</span><span class="st"> </span>xvec.s[iter.s,]);</a>
<a class="sourceLine" id="cb143-23" data-line-number="23">  grad.norm.s=<span class="kw">sqrt</span>(<span class="kw">norm</span>(grad.s))</a>
<a class="sourceLine" id="cb143-24" data-line-number="24">  iter.s=iter.s<span class="op">+</span><span class="dv">1</span>;</a>
<a class="sourceLine" id="cb143-25" data-line-number="25">  <span class="cf">if</span> (<span class="kw">is.nan</span>(grad.norm.s)){<span class="cf">break</span>}</a>
<a class="sourceLine" id="cb143-26" data-line-number="26">  <span class="cf">if</span> (<span class="kw">abs</span>(<span class="kw">max</span>(x.s)<span class="op">&gt;</span><span class="dv">3</span>)){<span class="cf">break</span>}</a>
<a class="sourceLine" id="cb143-27" data-line-number="27">} </a>
<a class="sourceLine" id="cb143-28" data-line-number="28"></a>
<a class="sourceLine" id="cb143-29" data-line-number="29"><span class="co"># remove all rows with non-finite values</span></a>
<a class="sourceLine" id="cb143-30" data-line-number="30">obj.val.s=obj.val.s[<span class="op">!</span><span class="kw">rowSums</span>(<span class="op">!</span><span class="kw">is.finite</span>(obj.val.s)),];</a>
<a class="sourceLine" id="cb143-31" data-line-number="31"></a>
<a class="sourceLine" id="cb143-32" data-line-number="32">grad.norm<span class="op">&lt;</span><span class="fl">10e-3</span></a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb145-1" data-line-number="1">grad.norm.s<span class="op">&lt;</span><span class="fl">10e-3</span></a></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb147-1" data-line-number="1"><span class="kw">max</span>(obj.val) </a></code></pre></div>
<pre><code>## [1] 0.05264206</code></pre>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb149-1" data-line-number="1"><span class="kw">max</span>(obj.val.s)</a></code></pre></div>
<pre><code>## [1] 113.4229</code></pre>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb151-1" data-line-number="1"><span class="co"># plot</span></a>
<a class="sourceLine" id="cb151-2" data-line-number="2"></a>
<a class="sourceLine" id="cb151-3" data-line-number="3">x=y=<span class="kw">seq</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="dt">le=</span><span class="dv">435</span>);</a>
<a class="sourceLine" id="cb151-4" data-line-number="4">plotU=<span class="cf">function</span>(x,y){(x<span class="op">*</span><span class="kw">sin</span>(<span class="dv">20</span><span class="op">*</span>y)<span class="op">+</span>y<span class="op">*</span><span class="kw">sin</span>(<span class="dv">20</span><span class="op">*</span>x))<span class="op">^</span><span class="dv">2</span><span class="op">*</span><span class="kw">cosh</span>(<span class="kw">sin</span>(<span class="dv">10</span><span class="op">*</span>x)<span class="op">*</span>x)<span class="op">+</span></a>
<a class="sourceLine" id="cb151-5" data-line-number="5">(x<span class="op">*</span><span class="kw">cos</span>(<span class="dv">10</span><span class="op">*</span>y)<span class="op">+</span>y<span class="op">*</span><span class="kw">sin</span>(<span class="dv">10</span><span class="op">*</span>x))<span class="op">^</span><span class="dv">2</span><span class="op">*</span><span class="kw">cosh</span>(<span class="kw">cos</span>(<span class="dv">20</span><span class="op">*</span>y)<span class="op">*</span>y)}</a>
<a class="sourceLine" id="cb151-6" data-line-number="6">z=<span class="kw">outer</span>(x,y,plot)</a>
<a class="sourceLine" id="cb151-7" data-line-number="7"><span class="kw">par</span>(<span class="dt">bg =</span> <span class="st">"white"</span>,<span class="dt">mar=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb151-8" data-line-number="8">res=<span class="kw">persp</span>(x, y, z, <span class="dt">theta=</span><span class="dv">155</span>, <span class="dt">phi=</span><span class="dv">30</span>, <span class="dt">col=</span><span class="st">"green"</span>, <span class="dt">ltheta=</span><span class="op">-</span><span class="dv">120</span>, </a>
<a class="sourceLine" id="cb151-9" data-line-number="9">          <span class="dt">shade=</span><span class="fl">0.75</span>, <span class="dt">border =</span><span class="ot">NA</span>, <span class="dt">xlab=</span><span class="st">"x1"</span>, <span class="dt">ylab=</span><span class="st">"x2"</span>, <span class="dt">zlab=</span><span class="st">"objective function"</span>, <span class="dt">box=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb151-10" data-line-number="10"></a>
<a class="sourceLine" id="cb151-11" data-line-number="11"><span class="co"># initial point for both searches</span></a>
<a class="sourceLine" id="cb151-12" data-line-number="12">routexy0=<span class="kw">expand.grid</span>(init,init)</a>
<a class="sourceLine" id="cb151-13" data-line-number="13"><span class="kw">points</span>(<span class="kw">trans3d</span>(routexy0[,<span class="dv">1</span>],routexy0[,<span class="dv">2</span>],maxd,<span class="dt">pmat=</span>res), <span class="dt">col=</span><span class="st">"yellow"</span>, <span class="dt">pch=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb151-14" data-line-number="14"></a>
<a class="sourceLine" id="cb151-15" data-line-number="15"><span class="co"># local optimal point for the deterministic search </span></a>
<a class="sourceLine" id="cb151-16" data-line-number="16">maxd =<span class="st"> </span><span class="kw">max</span>(obj.val); position.d =<span class="st"> </span><span class="kw">which</span>(obj.val <span class="op">==</span><span class="st"> </span>maxd, <span class="dt">arr.ind =</span> <span class="ot">TRUE</span>); position.d =<span class="st"> </span>xvec[position.d[<span class="dv">1</span>],]</a>
<a class="sourceLine" id="cb151-17" data-line-number="17">route1=(position.d); routexy1=<span class="kw">expand.grid</span>(route1,route1)</a>
<a class="sourceLine" id="cb151-18" data-line-number="18"><span class="kw">points</span>(<span class="kw">trans3d</span>(routexy1[,<span class="dv">1</span>],routexy1[,<span class="dv">2</span>],maxd,<span class="dt">pmat=</span>res), <span class="dt">col=</span><span class="st">"red"</span>, <span class="dt">pch=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb151-19" data-line-number="19"></a>
<a class="sourceLine" id="cb151-20" data-line-number="20"><span class="co"># climbing path for the stochastic search</span></a>
<a class="sourceLine" id="cb151-21" data-line-number="21">xvec.s=xvec.s[<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(obj.val.s),];</a>
<a class="sourceLine" id="cb151-22" data-line-number="22"><span class="kw">lines</span>(<span class="kw">trans3d</span>(xvec.s[,<span class="dv">1</span>], xvec.s[,<span class="dv">2</span>], obj.val.s, <span class="dt">pmat =</span> res), <span class="dt">col =</span> <span class="st">"blue"</span>)</a></code></pre></div>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>The above two examples reveal some contrasts between the deterministic and stochastic approaches. In terms of the <a href="ch-DE.html#sub:stab">stability</a> of performance, the deterministic method surpasses its opponent in two exercises. Also, the deterministic iteration effectively converges, while the stochastic one either does in a slower manner or completely fails. Nevertheless, one aspect of the stochastic approach overwhelms the opponent: the escapes from the <strong>local optima</strong>. Such a feature is indispensable when one needs to optimize an objective <em>globally</em>.<label for="tufte-sn-486" class="margin-toggle sidenote-number">486</label><input type="checkbox" id="tufte-sn-486" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">486</span> In the first example, the minimum point is unique; thus, locally searching this minimum on <span class="math inline">\([0,1]\)</span> is the same as the search on <span class="math inline">\(\mathbb{R}\)</span>. That is, a global ordering feature is granted by <a href="ch-optApp.html#sub:Optimization">convexity</a>. The deterministic method fully exploits this feature while adding noises somehow “misguides” the search and leads to inefficiency. In the second example, <a href="ch-optApp.html#sub:Optimization">convexity</a> is no longer a global feature. Some local maxima (around the central pseudo plateaux) are nearly the minima from a global perspective. In this case, the noises effectively pull out the search and “guide”" the algorithm to explore further possibilities.</span></p>
<p>For a <a href="ch-optApp.html#sub:Optimization">convex</a> function, the conditions from calculus are necessary and sufficient to characterize a point’s optimality. Nevertheless, in the <strong>global optimization</strong>, many optima are quite likely lacking <a href="ch-optApp.html#sub:Optimization">convexity</a>, i.e., pseudo plateaux. In these elusive situations, enumerating all possibilities and examining each one (a “grand tour”) will soonly confront the problem of far too many candidates and will exhaust the computational resources. Random search algorithms based on <a href="ch-CalUn.html#sub:divRV">Monte Carlo</a> methods, i.e., using many random points during the iterations and incoporating them with some deterministic structures, emerges in the <strong>global optimization</strong>. The simulated points can guide the optimizers to find out states with good values of irregular objective functions, i.e., non-convex functions. In addition, these algorithms often set up some correspondences with physical, biological, or economic, and social systems.</p>
<p>It is time to reconsider the problem of randomization.
One essential issue of the previous randomized algorithm is the instability (non-convergence) caused by the <a href="ch-randomization.html#sub:RHilbert">white noises</a>. One rescue attempt is to alter for a more stable type of iterations. Recall that a differential equation <span class="math inline">\(\mbox{d}x(t)/\mbox{d}t=-\epsilon x(t)\)</span> always converges (or say <em>reverts</em>) to zero for any <span class="math inline">\(\epsilon&gt;0\)</span>. In addition, if we modify the equation to<label for="tufte-sn-487" class="margin-toggle sidenote-number">487</label><input type="checkbox" id="tufte-sn-487" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">487</span> We can verify that <span class="math inline">\(x(t)=\mbox{e}^{-\epsilon t}\)</span> is the (fundamental) solution of <span class="math inline">\(\mbox{d}x(t)/\mbox{d}t=-\epsilon x(t)\)</span>. So if we add a constant <span class="math inline">\(\mu\)</span> to all <span class="math inline">\(x(t)\)</span>, then the differentiation structure must adapt to this addition by using <span class="math inline">\(x(t)-\mu\)</span>.</span> <span class="math display">\[\frac{\mbox{d}x(t)}{\mbox{d}t}=-\epsilon (x(t)-\mu),\]</span> then we can have such a <strong>reversion</strong> <span class="math inline">\(\lim_{t\rightarrow\infty}x(t)=\mu\)</span> for any constant <span class="math inline">\(\mu\in\mathbb{R}\)</span>.</p>
<p>This deterministic <strong>reversion</strong> structure can help us to stabilize the random algorithm. Let’s make <span class="math inline">\(\mbox{d}x(t)/\mbox{d}t\)</span> become a <em>stochastic differential equation</em>
<span class="math display">\[\frac{\mbox{d}X(t,\omega)}{\mbox{d}t}=\underset{\mbox{drift}}{\underbrace{-\epsilon(X(t,\omega)-\mu_{X})}}+\underset{\mbox{diffusion}}{\underbrace{\sigma\varepsilon(t,\omega)}},\]</span>
where <span class="math inline">\(\mu_{X}\)</span> plays the role as the constant mean of the <a href="ch-randomization.html#sub:SumDecomp">weakly stationary process</a> <span class="math inline">\(X(t,\omega)\)</span>, and <span class="math inline">\(\varepsilon(t,\omega)\)</span> is the <a href="ch-randomization.html#sub:RHilbert">Gaussian white noise</a>.</p>
<p>This <strong>stochastic differential equation</strong> can be solved by integrating <span class="math inline">\(X(s,\omega)\)</span> over <span class="math inline">\([0,t]\)</span>:<span class="math display">\[X(t,\omega)=X(0,\omega)-\epsilon\int_{0}^{t}(X(s,\omega)-\mu_X)\mbox{d}s+\sigma B(t,\omega)\]</span>
where <span class="math inline">\(B(t,\omega)=\int_{0}^{t}\varepsilon(s,\omega)\mbox{d}s\)</span> is the <a href="ch-randomization.html#sub:RHilbert">Brownian motion</a>.<label for="tufte-sn-488" class="margin-toggle sidenote-number">488</label><input type="checkbox" id="tufte-sn-488" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">488</span> For any <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathcal{H}\)</span>-value random variables</a> <span class="math inline">\(X(s,\omega)\)</span> and <span class="math inline">\(\varepsilon(s,\omega)\)</span>, the <a href="ch-randomization.html#sub:RHilbert">mean square integrals</a> are well-defined.</span> This solution <span class="math inline">\(X(t,\omega)\)</span> is also a stochastic process known as the <em>Ornstein-Uhlenbeck</em> process. The a <a href="ch-UnMulti.html#sub:rw">stationar</a> probability of <strong>Ornstein-Uhlenbeck</strong> process is: <span class="math display">\[\lim_{t\rightarrow\infty}X(t,\omega)\sim\mathcal{N}\left(\mu_{X},\frac{\sigma^{2}}{2\epsilon}\right).\]</span></p>
<p>For the <strong>Ornstein-Uhlenbeck</strong> process, if the <a href="ch-DE.html#sub:pde">diffusion coefficient</a> <span class="math inline">\(\sigma\)</span> does not completely suppress the original (deterministic) structure, one can expect that the equation’s dynamics will evolve around the deterministic path, and the <a href="ch-randomization.html#sub:RHilbert">mean function</a> of <span class="math inline">\(X(t,\omega)\)</span> will converge to the constant <span class="math inline">\(\mu_{X}\)</span>. Figure <a href="ch-randomization.html#fig:OU">16.7</a> shows that the long-term behavior of the <strong>Ornstein-Uhlenbeck</strong> process has a similar characteristic as its deterministic counterpart: along with the increase of <span class="math inline">\(t\)</span>, the process tends to <strong>revert</strong> to the constant mean <span class="math inline">\(\mu_{X}\)</span>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:OU"></span>
<img src="fig/Part4/OU.png" alt="Simulate Ornstein-Uhlenbeck process by the improved Euler's method " width="100%"><!--
<p class="caption marginnote">-->Figure 16.7: Simulate Ornstein-Uhlenbeck process by the improved Euler’s method <!--</p>-->
<!--</div>--></span>
</p>
<p>The <strong>Ornstein-Uhlenbeck</strong> equation (or simulating the <strong>Ornstein-Uhlenbeck</strong> process) can be solved by an improved Euler’s scheme that counts for the <a href="ch-randomization.html#sub:RHilbert">increments of the Brownian motion</a>.<label for="tufte-sn-489" class="margin-toggle sidenote-number">489</label><input type="checkbox" id="tufte-sn-489" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">489</span> When we discretize the time interval into <span class="math inline">\(n\)</span> subintervals, Euler’s algorithm needs to appropriately adjust the variance of the <a href="ch-randomization.html#sub:RHilbert">increments of the Brownian motion</a>. Otherwise, the variance will blow up. Recall that the increment between time <span class="math inline">\(t\)</span> and <span class="math inline">\(s\)</span> is distributed to <span class="math inline">\(\mathcal{N}(0,t-s)\)</span>, so when we model the <a href="ch-randomization.html#sub:RHilbert">Brownian motion</a> on a <span class="math inline">\(T\)</span>-length time interval with <span class="math inline">\(n\)</span> slices, the time increment for each slice is <span class="math inline">\(T/n\)</span>. So the <a href="ch-randomization.html#sub:RHilbert">increment of Brownian motion</a> on this slice is drawn from <span class="math inline">\(\mathcal{N}(0,T/n)\)</span>. This is often called the <em>Euler-Maruyama method</em>.</span> The specific algorithm of the simulation is given below.</p>
<div class="solution">
<p class="solution-begin">
Simulate the Ornstein-Uhlenbeck process <span id="sol-start-200" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-200', 'sol-start-200')"></span>
</p>
<div id="sol-body-200" class="solution-body" style="display: none;">
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb152-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb152-2" data-line-number="2">T =<span class="st"> </span><span class="dv">200</span>; n.iter =<span class="st"> </span><span class="dv">2000</span>; epsilon =<span class="st"> </span><span class="fl">0.1</span>; sigma =<span class="st"> </span><span class="fl">0.1</span>; mu=<span class="dv">1</span>;</a>
<a class="sourceLine" id="cb152-3" data-line-number="3">x =<span class="st"> </span>x.s =<span class="st"> </span>x.ss =<span class="st"> </span>time =<span class="kw">rep</span>(<span class="dv">0</span>,n.iter); step.size =<span class="st"> </span>T<span class="op">/</span>n.iter; </a>
<a class="sourceLine" id="cb152-4" data-line-number="4">x[<span class="dv">1</span>] =<span class="st"> </span>x.s[<span class="dv">1</span>] =<span class="st"> </span>x.ss[<span class="dv">1</span>] =<span class="st"> </span><span class="dv">0</span>; time[<span class="dv">1</span>] =<span class="st"> </span><span class="dv">0</span>; </a>
<a class="sourceLine" id="cb152-5" data-line-number="5">e=<span class="kw">rnorm</span>(n.iter,<span class="dv">0</span>,<span class="kw">sqrt</span>(step.size)<span class="op">*</span>sigma); <span class="co"># Increments of the Brownian motion at n.iter time points</span></a>
<a class="sourceLine" id="cb152-6" data-line-number="6"></a>
<a class="sourceLine" id="cb152-7" data-line-number="7"><span class="cf">for</span>(t <span class="cf">in</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span>(n.iter<span class="dv">-1</span>))){</a>
<a class="sourceLine" id="cb152-8" data-line-number="8">    x[t<span class="op">+</span><span class="dv">1</span>] =<span class="st"> </span>x[t] <span class="op">-</span><span class="st"> </span>epsilon <span class="op">*</span><span class="st"> </span>(x[t]<span class="op">-</span>mu)<span class="op">*</span>step.size;</a>
<a class="sourceLine" id="cb152-9" data-line-number="9">    x.s[t<span class="op">+</span><span class="dv">1</span>] =<span class="st"> </span>x.s[t] <span class="op">-</span><span class="st"> </span>epsilon <span class="op">*</span><span class="st"> </span>(x.s[t]<span class="op">-</span>mu)<span class="op">*</span>step.size <span class="op">+</span><span class="st"> </span>e[t];</a>
<a class="sourceLine" id="cb152-10" data-line-number="10">    x.ss[t<span class="op">+</span><span class="dv">1</span>] =<span class="st"> </span>x.ss[t] <span class="op">-</span><span class="st"> </span>epsilon <span class="op">*</span><span class="st"> </span>(x.ss[t]<span class="op">-</span>mu)<span class="op">*</span>step.size <span class="op">+</span><span class="st"> </span><span class="fl">0.1</span><span class="op">*</span>e[t]; <span class="co"># smaller diffusion</span></a>
<a class="sourceLine" id="cb152-11" data-line-number="11">    time[t<span class="op">+</span><span class="dv">1</span>] =<span class="st"> </span>time[t] <span class="op">+</span><span class="st"> </span>step.size</a>
<a class="sourceLine" id="cb152-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb152-13" data-line-number="13"></a>
<a class="sourceLine" id="cb152-14" data-line-number="14">stationary.x =<span class="st"> </span>x.s[(n.iter<span class="op">/</span><span class="dv">2</span>)<span class="op">:</span>n.iter]</a>
<a class="sourceLine" id="cb152-15" data-line-number="15"><span class="co"># stationary mean : mu </span></a>
<a class="sourceLine" id="cb152-16" data-line-number="16"><span class="kw">mean</span>(stationary.x)</a></code></pre></div>
<pre><code>## [1] 1.140312</code></pre>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb154-1" data-line-number="1"><span class="co"># stationary variance : sigma^2/epsilon </span></a>
<a class="sourceLine" id="cb154-2" data-line-number="2"><span class="kw">var</span>(stationary.x)</a></code></pre></div>
<pre><code>## [1] 0.09210787</code></pre>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb156-1" data-line-number="1"><span class="kw">library</span>(ggplot2); <span class="kw">library</span>(reshape2)</a>
<a class="sourceLine" id="cb156-2" data-line-number="2">dat =<span class="st"> </span><span class="kw">data.frame</span>(time,x,x.s,x.ss)</a>
<a class="sourceLine" id="cb156-3" data-line-number="3"><span class="kw">names</span>(dat)[<span class="dv">2</span>]=<span class="st">"x(t)"</span>; <span class="kw">names</span>(dat)[<span class="dv">3</span>]=<span class="st">"X(t,w)"</span>; <span class="kw">names</span>(dat)[<span class="dv">4</span>]=<span class="st">"X(t,w) smaller sigma"</span></a>
<a class="sourceLine" id="cb156-4" data-line-number="4"><span class="kw">ggplot</span>( <span class="kw">melt</span>(dat, <span class="dt">id.vars=</span><span class="st">"time"</span>), <span class="kw">aes</span>(time, value, <span class="dt">colour=</span>variable, <span class="dt">group=</span>variable)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()   <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</a></code></pre></div>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>The <strong>reversion</strong> property is a special feature of differential equations attached to some <a href="ch-vecMat.html#sub:linearity">gradient</a> structure, called <em>gradient flow</em>. The <strong>flow</strong> refers to a specific set of solutions of the <a href="ch-DE.html#sub:ode">autonomous system</a> of (systems of) differential equations. For any vector <span class="math inline">\(\mathbf{x}\in\mathbb{F}^n\)</span>, let <span class="math display">\[\psi^{t}(\mathbf{x}_{0})=\underset{t\mbox{-times composition}}{\underbrace{\psi\circ\cdots\circ\psi(\mathbf{x}_{0})}}=\mathbf{x}(t)\]</span>
be the solution(s) of <span class="math display">\[\frac{\mbox{d}\mathbf{x}(t)}{\mbox{d}t}=-\mathbf{F}(\mathbf{x}(t))\]</span>
with <span class="math inline">\(\mathbf{x}(0)=\mathbf{x}_{0}\)</span>.<label for="tufte-sn-490" class="margin-toggle sidenote-number">490</label><input type="checkbox" id="tufte-sn-490" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">490</span> The real-valued scalar <span class="math inline">\(t\)</span> is a generalization of the function <a href="sub-axioms.html#sub:rec">iteration</a> (called <em>functional power</em>).</span> The <em>flow</em> satisfies the following two conditions:</p>
<ol style="list-style-type: decimal">
<li><p>Identity function at the initial: <span class="math inline">\(\psi^{0}(\mathbf{x}) = \mathbf{x}\)</span></p></li>
<li><p><em>Group law</em> : <span class="math inline">\((\psi^{s}\circ\psi^{t})(\mathbf{x}) = \psi^{(s+t)}(\mathbf{x})\)</span>.</p></li>
</ol>
<p>The <strong>group law</strong> says that the <a href="sub-axioms.html#sub:add">addition</a> property of the scalar arguments (the power) can be obtained via the <a href="sub-set-theory.html#sub:func">composition</a> of the functions, just like the exponential function<label for="tufte-sn-491" class="margin-toggle sidenote-number">491</label><input type="checkbox" id="tufte-sn-491" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">491</span> The set of any <a href="sub-calculus.html#sub:diffInt">differentiable</a> function satisfying these two conditions is also called a <em>one-parameter group</em> (or a <em>one-parameter semigroup</em> if <span class="math inline">\(t\geq0\)</span>). It gives a general mathematical model of the autonomous, deterministic motion of a system in time.</span> <span class="math display">\[\mbox{e}^{tx}\mbox{e}^{sx}=\mbox{e}^{(t+s)x}.\]</span> Roughly speaking, the <strong>gradient flow</strong> is the <strong>flow</strong> whose gradients satisfy the <a href="ch-DE.html#sub:ode">law of conservation</a>; namely, the total change of the gradients of the <strong>flow</strong> is invariant.<label for="tufte-sn-492" class="margin-toggle sidenote-number">492</label><input type="checkbox" id="tufte-sn-492" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">492</span> We will come back to this conservative field in ch[?].</span></p>
<p>The <strong>gradient flow</strong> can be described by the following equation <span class="math display">\[\frac{\mbox{d}x(t)}{\mbox{d}t}=-\epsilon\nabla_{x}U(x(t))\]</span>
where <span class="math inline">\(\nabla_{x}U(\cdot)\)</span> is the <a href="ch-vecMat.html#sub:linearity">gradient</a> of the function <span class="math inline">\(U(\cdot)\)</span> with respect to the input argument <span class="math inline">\(x\)</span> .<label for="tufte-sn-493" class="margin-toggle sidenote-number">493</label><input type="checkbox" id="tufte-sn-493" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">493</span> If we let <span class="math inline">\(U(x)=x^{2}/2\)</span>, then we return to the <strong>mean reverting</strong> case <span class="math display">\[\frac{\mbox{d}x(t)}{\mbox{d}t}=-\epsilon x(t)\]</span> with <span class="math inline">\(\nabla_{x}U(x)=2 x/2\)</span>. </span></p>
<p>Analytically solving the <strong>gradient flow</strong> equation is not of our current interest. However, it is worth noticing that when the function <span class="math inline">\(U(\cdot)\)</span> is <a href="ch-optApp.html#sub:Optimization">convex</a>, the <strong>gradient flow</strong> is the continuous counterpart of the previous gradient descend/ascend algorithms. We can see that at an optimal point <span class="math inline">\(x^{*}\)</span>, <span class="math inline">\(\nabla_{x}U(x^{*})=0\)</span>, so the equation or the algorithm reaches the equilibrium at <span class="math inline">\(x^{*}\)</span>.</p>
<p>The randomization of the <strong>gradient flow problem</strong> leads to the following <strong>stochastic differential equation</strong> called the <em>Langevin equation</em> <span class="math display">\[\frac{\mbox{d}X(t,\omega)}{\mbox{d}t}=-\epsilon\nabla_{x}U(X(t,\omega))+\sigma\varepsilon(t,\omega).\]</span>
It is obvious to see that when <span class="math inline">\(U(x)=x^2/2\)</span>, the equation returns to the <strong>Ornstein-Uhlenbeck</strong> case.
Because the <strong>Ornstein-Uhlenbeck</strong> equation is a special case of the <strong>Langevin equation</strong>, we may expect that the <strong>Langevin equation</strong> will also converge/revert to some <a href="ch-randomization.html#sub:SumDecomp">weakly stationary</a> <a href="ch-randomization.html#sub:RHilbert">Gaussian process</a> in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{Y},\mathbb{P};\mathcal{H})\)</span>-space</a>. Indeed, the <strong>Langevin equation</strong> converges to a <a href="ch-randomization.html#sub:RHilbert">Gaussian process</a> whose <a href="ch-randomization.html#sub:SumDecomp">stationary distribution</a> is often written in the following form, called <em>Gibbs’ form</em>: <span class="math display">\[\mathbb{P}\left(X_{\infty}(\omega)\right)=\frac{\mbox{e}^{-U(x)/2\sigma_{X}^{2}}}{\int_{\mathbb{R}}\mbox{e}^{-U(x)/2\sigma_{X}^{2}}\mbox{d}x}\]</span>
where <span class="math inline">\(\sigma_{X}^{2}=\sigma^{2}/2\epsilon\)</span> and <span class="math inline">\(\lim_{t\rightarrow\infty}X(t,\omega)=X_{\infty}(\omega)\)</span>.<label for="tufte-sn-494" class="margin-toggle sidenote-number">494</label><input type="checkbox" id="tufte-sn-494" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">494</span> Note that if we let <span class="math inline">\(U(x)=(x-\mu_{X})^{2}\)</span>, then a Gaussian/normal distribution is <span class="math display">\[\begin{align*} &amp; \frac{1}{\sqrt{2\pi\sigma_{X}^{2}}}\mbox{e}^{-U(x)/2\sigma_{X}^{2}}\\
\mbox{where } &amp; \sqrt{2\pi\sigma_{X}^{2}}=\int_{\mathbb{R}}\mbox{e}^{-\frac{(x-\mu_{X})^{2}}{2\sigma_{X}^{2}}}\mbox{d}x\\
&amp;=\int_{\mathbb{R}}\mbox{e}^{-U(x)/2\sigma_{X}^{2}}\mbox{d}x.\end{align*}\]</span></span> In physics, <span class="math inline">\(\sigma_{X}^{2}\)</span> is interpreted as the temperature and <span class="math inline">\(U(x)\)</span> is interpreted as the <em>energy</em> at the state <span class="math inline">\(x\)</span>. In economics and biology, <span class="math inline">\(\sigma_{X}^{2}\)</span> often represents some measure of the risk or the taste, and <span class="math inline">\(-U(x)\)</span> represents the <a href="sub-axioms.html#sub:zorn">utility</a> of choosing <span class="math inline">\(x\)</span>.</p>
<p>The advantage of using <strong>Gibbs’ form</strong> is to have a straightforward and compact expression of the probability ratio between any two states <span class="math inline">\(x^{*}\)</span> and <span class="math inline">\(x\)</span>
<span class="math display">\[\frac{\mathbb{P}\left(X_{\infty}(\omega)=x^{*}\right)}{\mathbb{P}\left(X_{\infty}(\omega)=x\right)}=\frac{\mbox{e}^{-U(x^{*})/2\sigma_{X}^{2}}}{\mbox{e}^{-U(x)/2\sigma_{X}^{2}}}=\exp\left(\frac{U(x)-U(x^{*})}{2\sigma_{X}^{2}}\right),\]</span>
where <span class="math inline">\(U(x)-U(x^{*})\)</span> stands for the energy difference or utility difference. If <span class="math inline">\(x^{*}\)</span> is the minimal point such that <span class="math inline">\(U(x^{*})&lt;U(x)\)</span> for any <span class="math inline">\(x\neq x^{*}\)</span> and <span class="math inline">\(x\in\mathcal{H}\)</span>, we have<label for="tufte-sn-495" class="margin-toggle sidenote-number">495</label><input type="checkbox" id="tufte-sn-495" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">495</span> For <span class="math inline">\(U(x^{*})-U(x)&lt;0\)</span>, one has <span class="math display">\[\exp\left(-\frac{U(x^*)-U(x)}{2\sigma_{X}^{2}}\right)&gt;1\]</span> for <span class="math inline">\(U(x^{*})&lt;U(x)\)</span> which means <span class="math display">\[\frac{\mathbb{P}\left(X_{\infty}(\omega)=x^{*}\right)}{\mathbb{P}\left(X_{\infty}(\omega)=x\right)}&gt;1.\]</span></span>
<span class="math display">\[\mathbb{P}\left(X_{\infty}(\omega)=x^{*}\right)&gt;\mathbb{P}\left(X_{\infty}(\omega)=x\right).\]</span>
The inequality says that the probability of arriving at <span class="math inline">\(x^{*}\)</span> is strictly larger than reaching any other state <span class="math inline">\(x\)</span>. That is, if you draw a sample from the distribution <span class="math inline">\(\mathbb{P}\left(X_{\infty}(\omega)\right)\)</span>, the maximal probability of realizing any value of <span class="math inline">\(X_{\infty}(\omega)\)</span> is to confront the global optimal <span class="math inline">\(x^{*}\)</span>. In other words, <span class="math inline">\(\mathbb{P}\left(X_{\infty}(\omega)=x^{*}\right)\)</span> attains the maximal likelihood of the draws.</p>
<p>Understanding the <strong>Langevin equation</strong> will make us appreciate some important improvements in designing the <strong>global optimization</strong> algorithms. Suppose that one is only interested in a deterministic problem of optimizing a (non-convex) function <span class="math inline">\(U(\cdot)\)</span> with the minimal value at point <span class="math inline">\(x^{*}\)</span>. We knew that when the <strong>Langevin equation</strong> converges to the weakly stationary process <span class="math inline">\(X_{\infty}(\omega)\)</span>, the optimal point <span class="math inline">\(x^{*}\)</span>, rather than any other points <span class="math inline">\(x\)</span>, is more likely to be simulated according to the probability law <span class="math inline">\(\mathbb{P}\left(X_{\infty}(\omega)\right)\)</span>. In this case, instead of solving the whole <strong>Langevin equation</strong> by the <strong>Euler-Maruyama method</strong>, one can simply simulate random variables from probability law <span class="math inline">\(\mathbb{P}\left(X_{\infty}(\omega)\right)\)</span> to see which <span class="math inline">\(x\)</span> appears more often than the others. This motivation brings us to an algorithm called <em>simulated annealing</em>.</p>
<p>Without jumping into the details, let’s call for <strong>simulated annealing</strong> (as a built-in optimizer) to find the function’s <strong>global optimum</strong> in figure <a href="ch-randomization.html#fig:mixObj">16.8</a>. The global optimum is covered-up by many local optima. Starting from the same initial, all local searches (gradient- or higher order-based methods) fail, but <strong>simulated annealing</strong> does reach the point.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:mixObj"></span>
<img src="fig/Part4/mixObj.png" alt="Non-convex objective function " width="100%"><!--
<p class="caption marginnote">-->Figure 16.8: Non-convex objective function <!--</p>-->
<!--</div>--></span>
</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb157-1" data-line-number="1">p =<span class="st"> </span><span class="fl">0.4</span>; <span class="co"># a mixture objective function</span></a>
<a class="sourceLine" id="cb157-2" data-line-number="2">U =<span class="st"> </span><span class="cf">function</span>(x)</a>
<a class="sourceLine" id="cb157-3" data-line-number="3">     p     <span class="op">*</span><span class="st"> </span>(x<span class="dv">-20</span>)<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="dv">8</span> <span class="op">+</span></a>
<a class="sourceLine" id="cb157-4" data-line-number="4"><span class="st">     </span>(<span class="dv">1</span><span class="op">-</span>p) <span class="op">*</span><span class="st"> </span>(<span class="kw">sin</span>(x)<span class="op">-</span><span class="dv">5</span>)<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb157-5" data-line-number="5"></a>
<a class="sourceLine" id="cb157-6" data-line-number="6"><span class="co"># Local deterministic search</span></a>
<a class="sourceLine" id="cb157-7" data-line-number="7">NM =<span class="st"> </span><span class="kw">optim</span>(<span class="dv">8</span>,U)<span class="op">$</span>par; CG =<span class="st"> </span><span class="kw">optim</span>(<span class="dv">8</span>,U, <span class="dt">method=</span><span class="st">"CG"</span>)<span class="op">$</span>par; LBFGS =<span class="st"> </span><span class="kw">optim</span>(<span class="dv">8</span>,U, <span class="dt">method=</span><span class="st">"L-BFGS-B"</span>)<span class="op">$</span>par; BFGS =<span class="st"> </span><span class="kw">optim</span>(<span class="dv">8</span>,U,<span class="dt">method=</span><span class="st">"BFGS"</span>)<span class="op">$</span>par;</a>
<a class="sourceLine" id="cb157-8" data-line-number="8"><span class="kw">set.seed</span>(<span class="dv">2021</span>); SANN =<span class="st"> </span><span class="kw">optim</span>(<span class="dv">8</span>,U,<span class="dt">method=</span><span class="st">"SANN"</span>)<span class="op">$</span>par <span class="co"># SANN a variant of simulated annealing </span></a>
<a class="sourceLine" id="cb157-9" data-line-number="9"></a>
<a class="sourceLine" id="cb157-10" data-line-number="10"><span class="kw">cat</span>(<span class="st">"Local methods - NM:"</span>, NM, <span class="st">" CG:"</span>, CG, <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</a>
<a class="sourceLine" id="cb157-11" data-line-number="11"><span class="kw">cat</span>(<span class="st">"Local methods - BFGS:"</span>, BFGS, <span class="st">" LBFGS:"</span>, LBFGS, <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</a>
<a class="sourceLine" id="cb157-12" data-line-number="12"><span class="kw">cat</span>(<span class="st">"Simulated Annealing - SANN:"</span>, SANN, <span class="st">"</span><span class="ch">\n</span><span class="st">"</span>)</a></code></pre></div>
<pre><code>## Local methods - NM: 8.345313  CG: 8.345172</code></pre>
<pre><code>## Local methods - BFGS: 8.345155  LBFGS: 8.345173</code></pre>
<pre><code>## Simulated Annealing - SANN: 20.40368</code></pre>
<p><strong>Simulated annealing</strong> is a simple and general algorithm for finding the <strong>global optimum</strong>. Its concept is analogous to the physical annealing process (the reverse direction in figure <a href="ch-randomization.html#fig:annealing">16.1</a>): physical systems occupy only the states with the lowest energy as the temperature is lowered to absolute zero. One simulates the cooling of a fictitious physical system whose possible <strong>energies</strong> correspond to the values of the <a href="ch-optApp.html#sub:Proj2">objective function</a> being minimized. The scheme of <strong>simulated annealing</strong> is listed below.</p>
<p><code>1 :  Criteria:</code> <span class="math inline">\(k =\)</span> <code>Num of max iteration</code> <br><code>2 :</code> <span class="math inline">\(\,\,\)</span> <span class="math inline">\(n_{k} =\)</span> <code>Num of samples generated at k-th iteration</code> <br><code>3 :  Variables:</code> <span class="math inline">\(x\leftarrow x_0\)</span>, <span class="math inline">\(\sigma_X\leftarrow\sigma_X(0)\)</span>, <span class="math inline">\(U\leftarrow U(x_0)\)</span> <br><code>4 :  WHILE</code> <span class="math inline">\(k &lt; K\)</span> <code>DO</code> <br><code>5 :</code> <span class="math inline">\(\qquad\)</span> <code>FOR</code> <span class="math inline">\(i \in \{1, 2, \dots, n_k\}\)</span> <code>DO</code> <br><code>6 :</code> <span class="math inline">\(\qquad\)</span> <code>Propose</code> <span class="math inline">\(x_p\)</span> <code>from</code> <span class="math inline">\(\mathcal{N}(x, 1)\)</span> <br><code>7 :</code> <span class="math inline">\(\qquad\)</span> <code>IF</code> <span class="math inline">\(U(x_{p})\leq U(x)\)</span> <code>THEN</code> <span class="math inline">\(x \leftarrow x_p\)</span> <br><code>8 :</code> <span class="math inline">\(\qquad\)</span> <code>ELSE</code> <br><code>9 :</code> <span class="math inline">\(\qquad\)</span> <span class="math inline">\(\qquad\)</span> <code>IF</code> <span class="math inline">\(\mbox{e}^{\left(\frac{U(x)- U(x_p)}{\sigma_X^2}\right)}&gt;\mbox{a random number in }[0,1]\)</span> <br><code>10:</code> <span class="math inline">\(\qquad\)</span> <span class="math inline">\(\qquad\)</span> <code>THEN</code> <span class="math inline">\(x \leftarrow x_p\)</span> <br><code>11:</code> <span class="math inline">\(\qquad\)</span> <code>END FOR LOOP</code> <br><code>12:</code> <span class="math inline">\(\qquad\)</span> <span class="math inline">\(k \leftarrow k+1\)</span>, <code>Decrease</code> <span class="math inline">\(\sigma_X(k)\)</span> <br><code>13:</code> <code>END WHILE LOOP</code> <br></p>
<p>The simulation in step 6 can be drawn from other distributions that do not violate the <strong>Gibbs form</strong>. For example, if the state space <span class="math inline">\(\mathcal{H}\)</span> of the process is believed to be discrete, one can consider <a href="ch-CalUn.html#sub:divRV">Poisson distribution</a> or a distribution called multinomial (see ch[?]). Step 7 is to accept any move that can lower the objective value.
That is, if <span class="math inline">\(U(x')\leq U(x)\)</span>, the simulated state <span class="math inline">\(x'\)</span> is preferable to the current state <span class="math inline">\(x\)</span>, so the process will approach <span class="math inline">\(x'\)</span> (moving downhill). Nevertheless, even if the simulated state <span class="math inline">\(x'\)</span> gives a higher objective value, it could drive the process to a <strong>global minimum</strong>, so the algorithm should also accept some moves that are going up. Step 8 to 10 is to implement this motivation. They are also known as the <em>Metropolis criterion</em>. In fact, from step 5 to 12 (inside the WHILE loop), the algorithm is called the <em>Metropolis algorithm</em>. It is currently one of the best-known algorithms. Basically, it is the <strong>simulated annealing</strong> algorithm with the fixed temperature/utility. While the <strong>simulated annealing</strong> gradually reduces the probability of moving uphill by lowering the temperature <span class="math inline">\(\sigma_X\)</span> as the algorithm proceeds (in each iteration <span class="math inline">\(k\)</span>).<label for="tufte-sn-496" class="margin-toggle sidenote-number">496</label><input type="checkbox" id="tufte-sn-496" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">496</span> When the temperature goes down to zero, <span class="math inline">\(\sigma_X=0\)</span>, then randomization terminates, everything is determined.</span></p>
<p>In sum, the <strong>simulated annealing</strong> (and <strong>Metropolis</strong>) algorithm always accepts downhill moves while only sometimes accepting uphill moves with the hope of avoiding getting trapped in local minima. The following example shows the performance of the <strong>Metropolis algorithm</strong> under different temperatures <span class="math inline">\(\sigma_X\)</span>.
The green, blue, and red processes in figure <a href="ch-randomization.html#fig:Metropolis">16.9</a> correspond to <span class="math inline">\(\sigma_X^2=3\)</span>, <span class="math inline">\(2\)</span> and <span class="math inline">\(1\)</span>, respectively. A larger temperature makes the algorithm more volatile, so the algorithm seems to discover the <strong>global optimum</strong> faster. However, a lower temperature makes the algorithm more stable, so the algorithm spends more time at the reachable optima.
Within two thousand realizations of <span class="math inline">\(X_{\infty}(\omega)\)</span>, a large portion of the simulated samples clearly surrounds the global optimum.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:Metropolis"></span>
<img src="fig/Part4/Metropolis.png" alt="Sampling the global optimum by the Metropolis algorithm   " width="100%"><!--
<p class="caption marginnote">-->Figure 16.9: Sampling the global optimum by the Metropolis algorithm <!--</p>-->
<!--</div>--></span>
</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb161-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb161-2" data-line-number="2">sampling =<span class="st"> </span><span class="cf">function</span>(x, U, n.iter, sigmaX2) {</a>
<a class="sourceLine" id="cb161-3" data-line-number="3">    data.vec =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n.iter)</a>
<a class="sourceLine" id="cb161-4" data-line-number="4">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_len</span>(n.iter)){</a>
<a class="sourceLine" id="cb161-5" data-line-number="5">        <span class="co">## Propose a new x</span></a>
<a class="sourceLine" id="cb161-6" data-line-number="6">        xp =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>,x)</a>
<a class="sourceLine" id="cb161-7" data-line-number="7">        <span class="co">## Accept new point xp, if U(xp) &lt; U(x)</span></a>
<a class="sourceLine" id="cb161-8" data-line-number="8">        <span class="cf">if</span> (<span class="kw">U</span>(xp) <span class="op">&lt;</span><span class="st"> </span><span class="kw">U</span>(x)) { x =<span class="st"> </span>xp }</a>
<a class="sourceLine" id="cb161-9" data-line-number="9">        <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb161-10" data-line-number="10">          <span class="co">## Metropolis criterion: </span></a>
<a class="sourceLine" id="cb161-11" data-line-number="11">          Mc =<span class="st"> </span><span class="kw">exp</span>((<span class="kw">U</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">U</span>(xp))<span class="op">/</span>sigmaX2)</a>
<a class="sourceLine" id="cb161-12" data-line-number="12">          <span class="cf">if</span> (Mc <span class="op">&gt;</span><span class="st"> </span><span class="kw">runif</span>(<span class="dv">1</span>) ){ x =<span class="st"> </span>xp }</a>
<a class="sourceLine" id="cb161-13" data-line-number="13">        }</a>
<a class="sourceLine" id="cb161-14" data-line-number="14">        data.vec[i] =<span class="st"> </span>x}</a>
<a class="sourceLine" id="cb161-15" data-line-number="15">    <span class="kw">return</span>(data.vec)</a>
<a class="sourceLine" id="cb161-16" data-line-number="16">}</a>
<a class="sourceLine" id="cb161-17" data-line-number="17"></a>
<a class="sourceLine" id="cb161-18" data-line-number="18">result =<span class="st"> </span><span class="kw">sampling</span>(<span class="dv">0</span>, U, <span class="dt">n.iter=</span><span class="dv">2000</span>, <span class="dt">sigmaX2=</span><span class="dv">2</span>) <span class="co"># blue</span></a>
<a class="sourceLine" id="cb161-19" data-line-number="19">result.fast =<span class="st"> </span><span class="kw">sampling</span>(<span class="dv">0</span>, U, <span class="dt">n.iter=</span><span class="dv">2000</span>, <span class="dt">sigmaX2=</span><span class="dv">3</span>) <span class="co"># green</span></a>
<a class="sourceLine" id="cb161-20" data-line-number="20">result.slow =<span class="st"> </span><span class="kw">sampling</span>(<span class="dv">0</span>, U, <span class="dt">n.iter=</span><span class="dv">2000</span>, <span class="dt">sigmaX2=</span><span class="dv">1</span>) <span class="co"># red</span></a></code></pre></div>
<p>Let’s turn to the other side of global optimization. We saw that global optimization relies on the idea of randomization, while randomization means destabilizing the deterministic structure. It seems that, given the current computational layout of <a href="ch-representation.html#sub:pseudoRandom">pseudo-randomness</a>, instability is somehow an intrinsic feature in the global optimization procedure.</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb162-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2019</span>); <span class="kw">optim</span>(<span class="dv">8</span>,U,<span class="dt">method=</span><span class="st">"SANN"</span>)<span class="op">$</span>par  </a></code></pre></div>
<pre><code>## [1] 14.37226</code></pre>
<p>The algorithm is, by nature, unstable under the changes of the <a href="ch-representation.html#sub:pseudoRandom">pseudo-random</a> structure.<label for="tufte-sn-497" class="margin-toggle sidenote-number">497</label><input type="checkbox" id="tufte-sn-497" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">497</span> When the random seed changes, the new <a href="ch-representation.html#sub:pseudoRandom">pseudo-random numbers</a> do not generate enough randomness to escape the local optimum. The root of this problem is due to the lack of real “randomness” in the computational equipmens.</span> It is up to the user to decide whether the cost of this instability can be compensated by the benefit of discovering a better-off objective.<label for="tufte-sn-498" class="margin-toggle sidenote-number">498</label><input type="checkbox" id="tufte-sn-498" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">498</span> The board game GO was considered much more difficult for computers to win than other games such as chess because of its large state space and the requirement of the global strategies from the openning to the end-game. However, the current Monte-Carlo search algorithm has successfully made the computers as being “an entity that cannot be defeated” by their human opponents.</span></p>
<p>But this interesting cut-in may be a good moment to meditate on our understanding of <a href="sub-inferknow.html#determinism">determinism</a>. To conclude this chapter, let’s have a glimpse on the so-called <em>dilemma of determinism</em>, “the conflict between freedom and universal determination” (<span class="citation">James (<a href="bibliography.html#ref-James1884">1956</a>)</span>), a philosophical question of whether and to what extent the course of natural events is predetermined. This question inevitably binds to another question of whether and to what extent acquiring human freedom is a simultaneous state of chasing spiritual <a href="">rationality</a> (with respect to mind) and material <a href="sub-axioms.html#sub:zorn">utility</a> (with respect to body). It seems to me that any attempt of answering this question is equivalent to reconciling freedom of the will and moral law with a mechanical order of the universe, which is not my ambition.</p>
<p>However, I think the previous examples may help us recognize some deeper functions inside <a href="sub-inferknow.html#determinism">determinism</a>. The current fundamental scientific paradigm has shifted its focus to stochastics in the wake of quantum mechanics. But in fact, reducing the (material) world into a single observation of combined elements, e.g., a statistical mean, through the projection operator is definitely back to the realm of determinism. That is, the end of this causal dynamics randomly locates at a state space, where the values are predetermined. The causal structure is neither globally nor locally very different between the randomized and deterministic models: the skeleton dynamics is deterministic, random things may flow around the deterministic tracks. Free particles participating in the stochastic flow are lead by some predetermined gradient type structure. The disordered randomness, like the freedom of the will, belongs to an experiential category, and it helps explore the unknowns ahead and escape from the unwanted traps.</p>
</div>
</div></body></html>

<p style="text-align: center;">
<a href="ch-optApp.html"><button class="btn btn-default">Previous</button></a>
<a href="bibliography.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2021-03-31
</p>
</div>
</div>



</body>
</html>
