<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="16 Randomization | Project XXII" />
<meta property="og:type" content="book" />





<meta name="author" content="Zhengyuan Gao" />

<meta name="date" content="2021-03-20" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="16 Randomization | Project XXII">

<title>16 Randomization | Project XXII</title>

<link href="libs/tufte-css/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding/datatables.js"></script>
<link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>



</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Project XXII<p><p class="author">Zhengyuan Gao</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BA; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Preface</a>
<a href="part-i-inference-based-upon-logic-and-logical-computation.html">PART I: Inference Based upon Logic and Logical Computation</a>
<a href="sub-logic.html"><span class="toc-section-number">1</span> Logic</a>
<a href="sub-set-theory.html"><span class="toc-section-number">2</span> Set Theory</a>
<a href="sub-axioms.html"><span class="toc-section-number">3</span> Axioms</a>
<a href="sub-inferknow.html"><span class="toc-section-number">4</span> Inference and Knowledge</a>
<a href="sub-incomplete.html"><span class="toc-section-number">5</span> Incompleteness</a>
<a href="part-ii-infinitesimal-changes-and-their-consequences.html">PART II: Infinitesimal Changes and their Consequences</a>
<a href="sub-continuity.html"><span class="toc-section-number">6</span> Continuity</a>
<a href="sub-calculus.html"><span class="toc-section-number">7</span> Calculus</a>
<a href="ch-DE.html"><span class="toc-section-number">8</span> Differential Equations</a>
<a href="ch-CalUn.html"><span class="toc-section-number">9</span> Calculus under Uncertainty</a>
<a href="part-iii-emergence-of-abstract-interactions.html">PART III: Emergence of Abstract Interactions</a>
<a href="ch-vecMat.html"><span class="toc-section-number">10</span> Vector and Matrix</a>
<a href="ch-MatComp.html"><span class="toc-section-number">11</span> Matrix Computation</a>
<a href="ch-eigen.html"><span class="toc-section-number">12</span> Eigenvalues and Eigenvectors</a>
<a href="ch-UnMulti.html"><span class="toc-section-number">13</span> Uncertainty in Multiple Dimensions</a>
<a href="part-iv-three-masons-to-illuminate-the-dual-world.html">PART IV: Three Masons to Illuminate the Dual World</a>
<a href="ch-representation.html"><span class="toc-section-number">14</span> Representation</a>
<a href="ch-optApp.html"><span class="toc-section-number">15</span> Optimum Approximation</a>
<a id="active-page" href="ch-randomization.html"><span class="toc-section-number">16</span> Randomization</a><ul class="toc-sections">
<li class="toc"><a href="#sub:RHilbert"> Randomized Hilbert Space</a></li>
<li class="toc"><a href="#sub:SumDecomp"> Direct Sums and Decompositions</a></li>
<li class="toc"><a href="#sub:Kalman"> Miscellaneous: Sample Average, Reduction Principle, and Kalman’s Filter</a></li>
</ul>
<a href="bibliography.html">Bibliography</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="ch:randomization" class="section level1">
<h1>
<span class="header-section-number">16</span> Randomization</h1>
<p>Plato, in his work <em><em>the Republic</em></em>, describes a cave in which the residents are chained to a wall so they can’t see the real world; the best they can perceive are shadows reflected on the wall of the cave by some light outside. The residents have to make up their own interpretations of the outside world according to these shadows. All residents lack the omniscient knowledge of the real world, so they perceive the world differently based upon what is going on inside their minds and their “mental” projections of the shadows. That’s to say, facing the same situation, different people may consciously react differently; the different actions consecutively create different life paths. An observer from outside may find the evolutions of these paths somehow <a href="ch-CalUn.html#sub:rv">random</a>.</p>
<p>The <a href="ch-CalUn.html#sub:rv">randomness</a> emerges when the residents construct various elaborate ideas towards what reality is. The variety of these elaborate “mental” projections breaks down the unitarity and generates random outcomes. Such a construction is called <em>randomization</em>, a process of endowing unpredictable patterns to the events.<label for="tufte-sn-421" class="margin-toggle sidenote-number">421</label><input type="checkbox" id="tufte-sn-421" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">421</span> <strong>Randomization</strong> is particularly important in designing a game. Nobody wants to play a deterministic scissors-paper-stone game. This game is interesting because of the random feature - no one knows the opponent’s action. Similarly, suppose that many residents are characters immersing inside some kind of giant, massively “game platform” that is so well rendered that none of them can detect the artificiality; the way of keeping the game continue, I guess, is to create enough <a href="ch-CalUn.html#sub:rv">randomness</a> to prevent awakening the players. (Although attributing few random events a “non-random” feature to some players would make the game more “addictive,” this trick is non-applicable to all the events and all the players.)</span></p>
<div id="sub:RHilbert" class="section level2">
<h2>
<span class="header-section-number">16.1</span> Randomized Hilbert Space</h2>
<p>One way to think about <a href="ch-randomization.html#ch:randomization">randomization</a> is to consider the physical process of heating the crystals. The cooling crystals of a solid locate statically on some perfect lattice. During the heating process, the free energy accumulates so that the solid begins to melt, and the previous static crystals start to move randomly.<label for="tufte-sn-422" class="margin-toggle sidenote-number">422</label><input type="checkbox" id="tufte-sn-422" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">422</span> The inverse of this physical process, namely cooling down a melting solid, is called annealing.</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:annealing"></span>
<img src="fig/Part4/annealing.gif" alt="Heating the particles" width="100%"><!--
<p class="caption marginnote">-->Figure 16.1: Heating the particles<!--</p>-->
<!--</div>--></span>
</p>
<p>In a technical sense, if we restrict our attention to some objects in a <a href="ch-representation.html#sub:conjugacy">Hilbert space</a>, <span class="math inline">\(\mathcal{H}\)</span>, we can define the <a href="ch-randomization.html#ch:randomization">randomization</a> as mapping these objects into a <strong>Hilbert space</strong> associated with a certain <a href="sub-incomplete.html#sub:beyond2">probability space</a>.</p>
<ul>
<li>
<strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space, and mean square completeness</strong> : Let <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> be a <a href="sub-incomplete.html#sub:beyond2">probability space</a>, and let <span class="math inline">\(\mathcal{H}\)</span> be a (non-random) Hilbert space with the <a href="ch-vecMat.html#sub:vec">inner product</a> <span class="math inline">\(\left\langle \cdot,\cdot\right\rangle\)</span>. The <a href="ch-randomization.html#ch:randomization">randomization</a> of this Hilbert space on <span class="math inline">\((\Omega,\mathcal{F},\mathbb{P})\)</span> gives some measurable maps <span class="math inline">\(X(\omega):\Omega\rightarrow\mathcal{H}\)</span>, <span class="math inline">\(Y(\omega):\Omega\rightarrow\mathcal{H}\)</span>, and the <em><span class="math inline">\(\mathbb{P}\)</span>-inner product</em> <span class="math inline">\(\left\langle \cdot,\cdot\right\rangle _{\mathbb{P}}\)</span> such that:
<span class="math display">\[\left\langle X(\omega),Y(\omega)\right\rangle _{\mathbb{P}}=\int\left\langle X(\omega),Y(\omega)\right\rangle \mathbb{P}(\mbox{d}\omega)=\mathbb{E}\left[\left\langle X(\omega),Y(\omega)\right\rangle \right].\]</span>
In particular, any random element <span class="math inline">\(X(\omega)\)</span> in this randomized Hilbert space has the finite second moment, e.g., <span class="math display">\[\mathbb{E}\left[\left\langle X(\omega),X(\omega)\right\rangle \right]^{2}=\mathbb{E}[\|X(\omega)\|^{2}]&lt;\infty.\]</span>
These random elements are known as <em>mean-square integrable <span class="math inline">\(\mathcal{H}\)</span>-value random variables</em>. The space is denoted by <span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>.
A sequence <span class="math inline">\(\{X_{n}(\omega)\}_{n\in\mathbb{N}}\)</span> of the <em><span class="math inline">\(\mathcal{H}\)</span>-valued random variables</em> is said to converge to <span class="math inline">\(X(\omega)\)</span> in <em>mean square</em> if
<span class="math display">\[\lim_{n\rightarrow\infty}\mathbb{E}\left[\left|X_{n}(\omega)-X(\omega)\right|^{2}\right]=0\]</span> with <span class="math inline">\(\mathbb{E}[|X_{n}(\omega)|^{2}]&lt;\infty\)</span> for all <span class="math inline">\(n\)</span>.
The <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span> space</strong> is <a href="ch-representation.html#sub:conjugacy">complete</a> with respect to the <strong>mean square</strong>.<label for="tufte-sn-423" class="margin-toggle sidenote-number">423</label><input type="checkbox" id="tufte-sn-423" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">423</span> Recall that the Hilbert space is complete with respect to the <a href="ch-representation.html#sub:innerProd"><span class="math inline">\(L_{2}\)</span>-norm</a>.</span>
</li>
</ul>
<p>Here are three examples of the <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span> space</strong>.
First, suppose that the <a href="ch-representation.html#sub:conjugacy">Hilbert space</a> <span class="math inline">\(\mathcal{H}\)</span> is just a real-valued <a href="ch-MatComp.html#sub:vecSpaces">vector space</a> in the finite dimension, i.e., <span class="math inline">\(\mathcal{H}=\mathbb{R}^{n}\)</span>, the <strong><span class="math inline">\(\mathcal{H}\)</span>-valued random variables</strong> are standard random vectors, i.e., <span class="math display">\[\begin{align*}\mathbf{\mathbf{X}}(\omega)=\left[\begin{array}{c}
X_{1}(\omega)\\
\vdots\\
X_{n}(\omega)
\end{array}\right]&amp;,\: \mathbf{\mathbf{Y}}(\omega)=\left[\begin{array}{c}
Y_{1}(\omega)\\
\vdots\\
Y_{n}(\omega)
\end{array}\right],\\
\left\langle \mathbf{X}(\omega),\mathbf{Y}(\omega)\right\rangle _{\mathbb{P}}=&amp;\mathbb{E}\left[\sum_{i=1}^{n}X_i(\omega)Y_i(\omega)\right],\end{align*}\]</span>
where the <span class="math inline">\(\mathbb{P}\)</span>-inner product <span class="math inline">\(\left\langle \mathbf{X}(\omega),\mathbf{Y}(\omega)\right\rangle _{\mathbb{P}}\)</span> can also be written as
<span class="math display">\[\left\langle \mathbf{X}(\omega),\mathbf{Y}(\omega)\right\rangle _{\mathbb{P}} = \int_{\mathbb{R}^{n}}\int_{\mathbb{R}^{n}}\langle \mathbf{x},\mathbf{y}\rangle p(\mathbf{x},\mathbf{y})\mbox{d}\mathbf{x}\mbox{d}\mathbf{y}=\int\int  (\mathbf{x}^\top\mathbf{y} p(\mathbf{x},\mathbf{y}))\mbox{d}\mathbf{x}\mbox{d}\mathbf{y},\]</span>
where <span class="math inline">\(p(\cdot,\cdot)\)</span> is the <a href="ch-CalUn.html#sub:conProb">joint probability</a> density function of <span class="math inline">\(\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}\)</span>.<label for="tufte-sn-424" class="margin-toggle sidenote-number">424</label><input type="checkbox" id="tufte-sn-424" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">424</span> If we assume that the set <span class="math inline">\(\Omega\)</span> in the probability space <span class="math inline">\((\Omega, \mathcal{F}, \mathbf{P})\)</span> only contains <a href="ch-CalUn.html#sub:rv">discrete states</a>, the <span class="math inline">\(\mathbb{P}\)</span>-inner product becomes
<span class="math display">\[\begin{align*}\left\langle \mathbf{X}(\omega),\mathbf{Y}(\omega)\right\rangle _{\mathbb{P}}&amp;= \mathbb{E}[X(\omega)Y(\omega)]\\&amp;=\sum_{i=1}^{n}x_{i}y_{i}p(x_{i},y_{i}).\end{align*}\]</span></span></p>
<p>Second, suppose that the <a href="ch-representation.html#sub:conjugacy">Hilbert space</a> <span class="math inline">\(\mathcal{H}\)</span> is an infinite-dimensional space of real-valued functions on the domain <span class="math inline">\(t\in[0,T]\)</span>, i.e., <span class="math inline">\(\mathcal{H}=L_2[0,T]\)</span>, then the <strong><span class="math inline">\(\mathcal{H}\)</span>-valued random variables</strong> are <a href="ch-UnMulti.html#sub:Markov">stochastic processes</a>, i.e., <span class="math inline">\(X(t,\omega)\)</span> and <span class="math inline">\(Y(t,\omega)\)</span>. Informally, we can think to extend the previous example to the infinite dimension.<label for="tufte-sn-425" class="margin-toggle sidenote-number">425</label><input type="checkbox" id="tufte-sn-425" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">425</span> A <a href="ch-UnMulti.html#sub:Markov">stochastic processes</a> <span class="math inline">\(X(t,\omega)\)</span> can be thought of as a collection of infinite many random variables at infinite time points <span class="math inline">\(t_1, t_2,\dots\)</span>. So we have an infinite-length vector <span class="math inline">\(X_{t_1}(\omega),\dots X_{t_n}(\omega)\dots\)</span>. Also, the probability space of the <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong> is a <a href="ch-UnMulti.html#sub:Markov">filtered space</a> <span class="math inline">\((\Omega,\mathcal{F},\{\mathcal{F}_{t}\}_{t&gt;0},\mathbb{P})\)</span> where <span class="math inline">\(\mathcal{F}_1\)</span> is for <span class="math inline">\(X_{t_1}\)</span>, etc.</span> That is, given a fixed <span class="math inline">\(\omega\in\Omega\)</span> (the sample path), the <strong><span class="math inline">\(\mathcal{H}\)</span>-valued</strong> <span class="math inline">\(\{X(\cdot,\omega):t\in[0,T]\}\)</span> is a deterministic function, say <span class="math inline">\(X(\cdot,\omega):[0,t]\rightarrow L_{2}[0,T]\)</span>.
The <span class="math inline">\(\mathbb{P}\)</span>-inner product of these stochastic processes is given by
<span class="math display">\[\begin{align*}&amp; \left\langle X(t,\omega),Y(t,\omega)\right\rangle _{\mathbb{P}}=\int\int X(s,\omega)Y(s,\omega)\mbox{d}s\mathbb{P}(\mbox{d}\omega)\\&amp;=\mathbb{E}\left[\left\langle X(s,\omega),Y(s,\omega)\right\rangle \right]=\mathbb{E}\left[\int_{0}^{t}X(s,\omega)Y(s,\omega)\mbox{d}s\right].\end{align*}\]</span></p>
<p>Third, one fundamental <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> in <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong> is the <em>Gaussian Hilbert space</em>, a <a href="ch-CalUn.html#sub:divRV">complete</a> space consisting of zero-mean <a href="ch-CalUn.html#sub:divRV">Gaussian random variables</a> from the probability space <span class="math inline">\((\Omega,\mathcal{F},\mathcal{N})\)</span> where <span class="math inline">\(\mathcal{N}\)</span> stands for the <a href="ch-CalUn.html#sub:divRV">Gaussian probability law</a>. For example, for any <span class="math inline">\(n\)</span> independent identical <span class="math inline">\(\varepsilon_i \sim \mathcal{N}(0,\sigma^2)\)</span>, the <a href="ch-MatComp.html#sub:vecSpaces">span</a> of these <a href="ch-CalUn.html#sub:divRV">Gaussian random variables</a> <span class="math display">\[\mbox{span}\left\{\sum_{i=1}^{n} c_i\varepsilon_i\,:\, \sum_{i=1}^n|c_i|^2\leq \infty\right\}\]</span> is a <strong>Gaussian Hilbert space</strong>.</p>
<p>In the <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong>, we can formalize the randomness for infinite-dimensional objects.
One example is to randomize the (deterministic) <a href="sub-inferknow.html#sub:dyn">dynamics</a>. Consider the following <a href="ch-DE.html#sub:ode">differential equation</a>
<span class="math display">\[\frac{\mbox{d}x(t)}{\mbox{d}t} =f(x(t)),  \,\, \mbox{with }
x(0) =x_{0}.\]</span>
If the solution of the system exists, say <span class="math inline">\(x(t)\in\mathcal{H}\)</span>, it must be a function <span class="math inline">\(x(\cdot):[0,T]\rightarrow\mathbb{R}\)</span>.<label for="tufte-sn-426" class="margin-toggle sidenote-number">426</label><input type="checkbox" id="tufte-sn-426" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">426</span> Suppose the function <span class="math inline">\(f(\cdot)\)</span> is <a href="sub-continuity.html#sub:continuity">Lipschitz continuous</a>; the solution exists and is unique.</span> <a href="ch-randomization.html#ch:randomization">Randomization</a> is to parameterize the function <span class="math inline">\(f(\cdot)\)</span> by a random variable <span class="math inline">\(\varepsilon_{t}\)</span> at time <span class="math inline">\(t\)</span> with <span class="math inline">\(\mathbb{E}[|\varepsilon_t|^2]\leq\infty\)</span>. The system then becomes <span class="math display">\[
\frac{\mbox{d}X(t,\omega)}{\mbox{d}t} =f\left(X(t,\omega),\varepsilon_t\right),\,\, \mbox{with }
X(0,\omega) =X_{0},\]</span>
whose solution, if it exists, is a <a href="ch-UnMulti.html#sub:Markov">stochastic process</a> <span class="math display">\[X(\cdot,\cdot):[0,T]\times(\Omega,\mathcal{F},\{\mathcal{F}_{t}\}_{t\in[0,T]},\mathbb{P})\rightarrow\mathbb{R}.\]</span> Given <span class="math inline">\(\omega\)</span>, the deterministic function <span class="math inline">\(X(\cdot,\omega)\)</span> is an <span class="math inline">\(\mathcal{H}\)</span>-valued element.</p>
<p>The above differentiation notation of the stochastic process <span class="math inline">\(X(t,\omega)\)</span> may look awkward because the stochastic processes often have zigzags and because we have seen the “non-differentiability” natural of zigzags in chapter <a href="sub-calculus.html#sub:noDiff">7.2</a>. The fact is that the differentiation <span class="math inline">\(\mbox{d}X(t,\omega)/\mbox{d}t\)</span> refers to a “differentiation” in the <strong>mean square</strong> sense. For the <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space’s</strong> processes, we can introduce this new differentiation by some standard calculus conditions on the <a href="ch-CalUn.html#sub:ex">means</a> and the <a href="ch-UnMulti.html#sub:MultiVar">covariances</a>.<label for="tufte-sn-427" class="margin-toggle sidenote-number">427</label><input type="checkbox" id="tufte-sn-427" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">427</span> Notice that for a <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong> stochastic process, the first- and second-order densities (if they exist) can answer most (if not all) important probabilistic questions about the process. In other words, all the probability laws of the process are given by the first- and second-order densities. So it would be quite straightforward to consider the parameters associated with these densities, namely the <a href="ch-CalUn.html#sub:ex">means</a> and the <a href="ch-UnMulti.html#sub:MultiVar">covariance</a>.</span></p>
<p>One can define an equivalence class of stochastic processes having prescribed first and second-order <a href="ch-CalUn.html#sub:ex">moments</a>. This class is normally labeled as the <em>second-order process</em>. Let <span class="math inline">\(\{X(t,\omega)\}_{t\in\mathbb{R}^{+}}\)</span> be a stochastic process and let <span class="math inline">\(\{X_t(\omega)\}_{t\in\mathbb{N}}\)</span> be a time series.</p>
<ul>
<li>
<em>Mean value function</em> of the process or the time series:
<span class="math display">\[\mu_X(t) =\mathbb{E}[X(t,\omega)] = \mathbb{E}[X_t(\omega)].\]</span>
</li>
<li>
<em>(Auto)-covariance function</em> of the process or the time series:
<span class="math display">\[\begin{align*}\mbox{Cov}_X (t,s) &amp;=\mathbb{E}\left[(X(t,\omega)- \mu_{X}(t))(X(s,\omega)- \mu_{X}(s))\right], 
\\&amp;=\mathbb{E}[X(t,\omega)X(s,\omega)]- \mu_{X}(t)\mu_{X}(s);
\end{align*}\]</span>
</li>
</ul>
<p>Many interesting models can be characterized by the <strong>mean and covariance functions</strong>.
For example, consider the <a href="ch-eigen.html#sub:matNorms">AR(1)</a> model <span class="math inline">\(X_{t+1}=\phi X_{t}+\varepsilon_{t}\)</span> where <span class="math inline">\(\varepsilon_{t}\sim\mathcal{N}(0,\sigma_{\epsilon}^{2})\)</span> is <a href="ch-CalUn.html#sub:divRV">independent</a> of <span class="math inline">\(X_t\)</span>. The mean and covariance functions follow the dynamic law:<label for="tufte-sn-428" class="margin-toggle sidenote-number">428</label><input type="checkbox" id="tufte-sn-428" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">428</span> The law comes from
<span class="math display">\[\begin{align*}
\mu_{X}(t+1)&amp;=\mathbb{E}[X_{t+1}]\\&amp;=\mathbb{E}[\phi X_{t}+\varepsilon_{t}]\\
&amp;=\phi\mu_{X}(t)\\
\mbox{Cov}_{X}(t+1,t)   &amp;=\mbox{Cov}(X_{t+1},X_{t})\\
&amp;=\mbox{Cov}(\phi X_{t}+\epsilon_{t},X_{t})
    \\&amp;=\phi\mbox{Cov}(X_{t},X_{t})+\mbox{Cov}(\epsilon_{t},X_{t})
    \\&amp;=\phi\mbox{Cov}_{X}(t,t)\end{align*}\]</span>
Sometimes people are interested in AR models with invariant variances, namely <span class="math display">\[\mbox{Var}(X_t(\omega))=\cdots=\mbox{Var}(X_1(\omega))=\sigma^2.\]</span><br>
Then there is
<span class="math display">\[\sigma^{2} = \phi^{2} \sigma^{2} + \sigma_{\epsilon}^{2}\]</span> which implies <span class="math inline">\(\sigma=\sigma_{\epsilon}^{2}/(1-\phi)\)</span>. Thus the <strong>covariance function</strong> of this AR(1) model can be simplified into <span class="math display">\[\mbox{Cov}_{X}(t+1,t)=\frac{\sigma_{\epsilon}^{2}\phi}{1-\phi}.\]</span></span>
<span class="math display">\[\mu_{X}(t+1)=\phi\mu_{X}(t),\,\,\mbox{Cov}_{X}(t+1,t)=\phi\mbox{Cov}_{X}(t,t).\]</span></p>
<p>Another essential sub-class of the <strong>second-order processes</strong> is the <em>Gaussian process</em>, whose probability law is uniquely determined by the following specification <span class="math display">\[X(\cdot,\omega)\sim \mathcal{N}(\mu_X(\cdot),\mbox{Cov}_X(\cdot,\cdot)).\]</span> To understand the above specification, let’s consider a real-valued random vector <span class="math inline">\(\mathbf{X}(\omega)\in\mathbb{R}^{n}\)</span> with <a href="ch-CalUn.html#sub:divRV">Gaussian distribution</a> <span class="math inline">\(\mathbf{X}(\omega)\sim \mathcal{N}(\mathbf{\mu},\Sigma)\)</span>.
The first-order information is given by the mean vector <span class="math inline">\(\mathbb{E}[\mathbf{X}(\omega)]=\mathbf{\mu}\)</span>; the finite second-order information is contained in the <a href="ch-UnMulti.html#sub:MultiVar">covariance matrix</a> <span class="math inline">\(\mbox{Var}(\mathbf{X}(\omega))=\Sigma\)</span>. Now let’s turn to a <strong>Gaussian process</strong> <span class="math inline">\(X(t,\omega)\in\mathbb{R}\)</span> with mean function <span class="math inline">\(\mu_X(\cdot)\)</span> and <span class="math inline">\(\mbox{Cov}_X(\cdot,\cdot)\)</span>. If we visualize the process at any <span class="math inline">\(n\)</span> time points <span class="math inline">\((t_1,\dots,t_n)\)</span>, then the process looks like a random vector of the time series <span class="math inline">\(X_{t_1}(\omega),\dots,X_{t_n}(\omega)\)</span>:
<span class="math display">\[\underset{\mathbf{X}(\omega)}{\underbrace{\left[\begin{array}{c}
X(t_{1},\omega)\\
\vdots\\
X(t_{n},\omega)
\end{array}\right]}}\sim\mathcal{N}\left(\underset{\mathbf{\mu}}{\underbrace{\left[\begin{array}{c}
\mu_{X}(t_{1})\\
\vdots\\
\mu_{X}(t_{n})
\end{array}\right]}},\underset{\Sigma}{\underbrace{\left[\begin{array}{ccc}
\mbox{Cov}_{X}(t_{1},t_{1}) &amp; \cdots &amp; \mbox{Cov}_{X}(t_{1},t_{n})\\
\vdots &amp; \ddots &amp; \vdots\\
\mbox{Cov}_{X}(t_{n},t_{1}) &amp; \cdots &amp; \mbox{Cov}_{X}(t_{n},t_{n})
\end{array}\right]}}\right).\]</span>
Extending this <span class="math inline">\(n\)</span>-length random vector to infinity, we are supposed to have the <strong>Gaussian process</strong>:<label for="tufte-sn-429" class="margin-toggle sidenote-number">429</label><input type="checkbox" id="tufte-sn-429" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">429</span> The discretization of a continuous time <strong>Gaussian process</strong> has an interesting implication. The <a href="ch-UnMulti.html#sub:MultiVar">covariance matrices</a> as the <a href="ch-UnMulti.html#sub:MultiVar">positive (semi)-definite matrices</a> may have their corresponding underlying functions. Moreover, these functions may also have corresponding underlying <a href="ch-MatComp.html#sub:vecSpaces">operators</a> (the functions’ infinite-dimensional counterparts). In practice, the discretized <strong>Gaussian process</strong>’s <a href="ch-UnMulti.html#sub:MultiVar">covariance matrices</a> come from some “kernels” or called “positive operators.” We will come back to these concepts in sec[?].</span> <span class="math display">\[X(\cdot,\omega)\sim \mathcal{N}(\mu_X(\cdot),\mbox{Cov}_X(\cdot,\cdot)).\]</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:GP"></span>
<img src="fig/Part4/GP.png" alt="Gaussian processes with zero mean" width="100%"><!--
<p class="caption marginnote">-->Figure 16.2: Gaussian processes with zero mean<!--</p>-->
<!--</div>--></span>
</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:GPKern"></span>
<img src="fig/Part4/GPKern.png" alt="The corresponding covariance matrces" width="100%"><!--
<p class="caption marginnote">-->Figure 16.3: The corresponding covariance matrces<!--</p>-->
<!--</div>--></span>
</p>
<div class="solution">
<p class="solution-begin">
Simulation of Gaussian processes <span id="sol-start-99" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-99', 'sol-start-99')"></span>
</p>
<div id="sol-body-99" class="solution-body" style="display: none;">
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">20210228</span>)</a>
<a class="sourceLine" id="cb124-2" data-line-number="2"><span class="kw">library</span>(MASS) <span class="co"># call for multivariate normal sampling function</span></a>
<a class="sourceLine" id="cb124-3" data-line-number="3">n =<span class="st"> </span><span class="dv">100</span>;</a>
<a class="sourceLine" id="cb124-4" data-line-number="4">CovMatrix1 =<span class="st"> </span><span class="cf">function</span>(s, t) {<span class="kw">min</span>(s, t)}; <span class="co"># Brownian motion</span></a>
<a class="sourceLine" id="cb124-5" data-line-number="5">CovMatrix2 =<span class="st"> </span><span class="cf">function</span>(s,t){ <span class="kw">exp</span>(<span class="op">-</span><span class="dv">10</span><span class="op">*</span><span class="kw">abs</span>(s<span class="op">-</span>t))}; <span class="co"># OU</span></a>
<a class="sourceLine" id="cb124-6" data-line-number="6">CovMatrix3 =<span class="st"> </span><span class="cf">function</span>(s,t){ <span class="kw">as.numeric</span>(s<span class="op">==</span>t)}; <span class="co"># Gaussian white noise</span></a>
<a class="sourceLine" id="cb124-7" data-line-number="7">t =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">0</span>, <span class="dt">to =</span> <span class="dv">1</span>, <span class="dt">length.out =</span> n); <span class="co"># t_1=0,..., t_n=1</span></a>
<a class="sourceLine" id="cb124-8" data-line-number="8"></a>
<a class="sourceLine" id="cb124-9" data-line-number="9"><span class="co"># Fill in entities of the covariance matrix</span></a>
<a class="sourceLine" id="cb124-10" data-line-number="10">CovMatrix1 =<span class="st"> </span><span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">1</span>){<span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">2</span>){</a>
<a class="sourceLine" id="cb124-11" data-line-number="11">     <span class="kw">CovMatrix1</span>(argument_<span class="dv">1</span>, argument_<span class="dv">2</span>)})})</a>
<a class="sourceLine" id="cb124-12" data-line-number="12">CovMatrix2 =<span class="st"> </span><span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">1</span>){<span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">2</span>){</a>
<a class="sourceLine" id="cb124-13" data-line-number="13">     <span class="kw">CovMatrix2</span>(argument_<span class="dv">1</span>, argument_<span class="dv">2</span>)})})</a>
<a class="sourceLine" id="cb124-14" data-line-number="14">CovMatrix3 =<span class="st"> </span><span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">1</span>){<span class="kw">sapply</span>(t, <span class="cf">function</span>(argument_<span class="dv">2</span>){</a>
<a class="sourceLine" id="cb124-15" data-line-number="15">     <span class="kw">CovMatrix3</span>(argument_<span class="dv">1</span>, argument_<span class="dv">2</span>)})})</a>
<a class="sourceLine" id="cb124-16" data-line-number="16"><span class="co"># Sampling</span></a>
<a class="sourceLine" id="cb124-17" data-line-number="17">samplePath1 =<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dt">mu =</span> <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dt">times =</span> n), <span class="dt">Sigma =</span> CovMatrix1);</a>
<a class="sourceLine" id="cb124-18" data-line-number="18">samplePath2 =<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dt">mu =</span> <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dt">times =</span> n), <span class="dt">Sigma =</span> CovMatrix2);</a>
<a class="sourceLine" id="cb124-19" data-line-number="19">samplePath3 =<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dt">mu =</span> <span class="kw">rep</span>(<span class="dv">0</span>, <span class="dt">times =</span> n), <span class="dt">Sigma =</span> CovMatrix3);</a>
<a class="sourceLine" id="cb124-20" data-line-number="20"></a>
<a class="sourceLine" id="cb124-21" data-line-number="21"><span class="co"># plot</span></a>
<a class="sourceLine" id="cb124-22" data-line-number="22"><span class="kw">library</span>(ggplot2); <span class="kw">library</span>(reshape2); <span class="kw">library</span>(expm)</a>
<a class="sourceLine" id="cb124-23" data-line-number="23">dat =<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">c</span>(t,samplePath1, samplePath2, samplePath3), <span class="dt">ncol=</span><span class="dv">4</span>))</a>
<a class="sourceLine" id="cb124-24" data-line-number="24"><span class="kw">names</span>(dat)=<span class="kw">c</span>(<span class="st">"time"</span>,<span class="st">"Brownian motion"</span>,<span class="st">"Ornstein-Uhlenbeck"</span>,<span class="st">"Gaussian white noise"</span>);</a>
<a class="sourceLine" id="cb124-25" data-line-number="25"><span class="kw">ggplot</span>( <span class="kw">melt</span>(dat, <span class="dt">id.vars=</span><span class="st">"time"</span>), <span class="kw">aes</span>(time, value, <span class="dt">group=</span>variable)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_line</span>()   <span class="op">+</span></a>
<a class="sourceLine" id="cb124-26" data-line-number="26"><span class="op">+</span><span class="st">   </span><span class="kw">facet_grid</span>(.<span class="op">~</span>variable)<span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>()</a>
<a class="sourceLine" id="cb124-27" data-line-number="27">m1=<span class="kw">Matrix</span>(CovMatrix1, <span class="dt">sparse=</span><span class="ot">TRUE</span>); m2=<span class="kw">Matrix</span>(CovMatrix2, <span class="dt">sparse=</span><span class="ot">TRUE</span>); m3=<span class="kw">Matrix</span>(CovMatrix3, <span class="dt">sparse=</span><span class="ot">TRUE</span>);</a>
<a class="sourceLine" id="cb124-28" data-line-number="28"><span class="kw">print</span>(<span class="kw">image</span>(m1, <span class="dt">main=</span><span class="st">"Brownian motion"</span>), <span class="dt">split=</span><span class="kw">c</span>(<span class="dt">x=</span><span class="dv">1</span>,<span class="dt">y=</span><span class="dv">1</span>,<span class="dt">nx=</span><span class="dv">3</span>, <span class="dt">ny=</span><span class="dv">1</span>), <span class="dt">more=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb124-29" data-line-number="29"><span class="kw">print</span>(<span class="kw">image</span>(m2, <span class="dt">main=</span><span class="st">"Ornstein-Uhlenbeck"</span>), <span class="dt">split=</span><span class="kw">c</span>(<span class="dt">x=</span><span class="dv">2</span>,<span class="dt">y=</span><span class="dv">1</span>,<span class="dt">nx=</span><span class="dv">3</span>, <span class="dt">ny=</span><span class="dv">1</span>), <span class="dt">more=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb124-30" data-line-number="30"><span class="kw">print</span>(<span class="kw">image</span>(m3, <span class="dt">main=</span><span class="st">"Gaussian white noise"</span>, <span class="dt">Imult=</span><span class="fl">0.1</span>), <span class="dt">split=</span><span class="kw">c</span>(<span class="dt">x=</span><span class="dv">3</span>,<span class="dt">y=</span><span class="dv">1</span>,<span class="dt">nx=</span><span class="dv">3</span>, <span class="dt">ny=</span><span class="dv">1</span>))</a></code></pre></div>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Now, we come back to the discussion of the differentiation issue for the stochastic process <span class="math inline">\(X(t,\omega)\)</span>. The issue is resolvable with defining a meaningful calculus towards the randomness.
The following definition gives the <em>mean square calculus</em> for the processes in <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong>.</p>
<ul>
<li><p><strong>Mean square continuity</strong>: When <span class="math display">\[\lim_{h\rightarrow0}\mathbb{E}\left[\left|X(t+h, \omega)-X(t, \omega)\right|^{2}\right]=0,\]</span>
the random variable <span class="math inline">\(X(t,\omega)\)</span> is <em>mean square continous</em> at <span class="math inline">\(t\)</span>.</p></li>
<li><p><strong>Mean square differentiability</strong> : When the second order derivative <span class="math display">\[\frac{\partial^{2}\mathbb{E}[X(t,\omega)X(s,\omega)]}{\partial t\partial s}\]</span>
exists for any <span class="math inline">\(t,s \in [a,b]\)</span>, <span class="math inline">\(X(t,\omega)\)</span> is <em>mean square differentiable</em> such that <span class="math display">\[\lim_{h\rightarrow0}\mathbb{E}\left[\left|\frac{X(t+h,\omega)-X(t,\omega)}{h}-\frac{\mbox{d}X(t,\omega)}{\mbox{d}t}\right|^{2}\right]=0.\]</span></p></li>
<li><p><strong>Mean square integrability</strong> : When the (Riemann) <a href="sub-calculus.html#sub:diffInt">integral</a> <span class="math display">\[\int_{a}^{b}\int_{a}^{b}\mathbb{E}[X(t,\omega)X(s,\omega)]\mbox{d}t\mbox{d}s\]</span> is well defined over the interval <span class="math inline">\([a,b]\)</span>, then <span class="math inline">\(X(t,\omega)\)</span> is <em>mean square integrable</em> over <span class="math inline">\([a,b]\)</span> with the <em>mean square integral</em> <span class="math inline">\(\int_{a}^{b} X(t,\omega) \mbox{d}t\)</span>.<label for="tufte-sn-430" class="margin-toggle sidenote-number">430</label><input type="checkbox" id="tufte-sn-430" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">430</span> Here is a more formal definition of the <em>mean square integral</em>. For an interval <span class="math inline">\([a,b]\)</span>, we make a partition <span class="math inline">\(a=t_{0}&lt;t_{1}&lt;\cdots&lt;t_{n}=b\)</span> and
let <span class="math inline">\(\kappa=\max_{i}(t_{i+1}-t_{i})\)</span>, <span class="math inline">\(t_{i}\leq m_{i}\leq t_{i+1}\)</span>. The <strong>mean square integral</strong> of <span class="math inline">\(X(t,\omega)\)</span> is given by the <strong>mean square limit</strong> of <span class="math inline">\(\sum_{i=0}^{n-1}X_{m_{i}}(\omega)(t_{i+1}-t_{i})\)</span> that is
<span class="math display">\[\begin{align*}\lim_{\kappa\rightarrow0, n\rightarrow\infty}\mathbb{E}\left[\left|\sum_{i=0}^{n-1}X_{m_{i}}(\omega)(t_{i+1}-t_{i})-\\
\int_{a}^{b}X(t,\omega)\mbox{d}t\right|^{2}\right]=0.\end{align*}\]</span></span></p></li>
<li><p><em>Fundamental theorem of mean square calculus</em> : <span class="math display">\[\Pr\left\{ \left|X(t,\omega)-X(a,\omega)=\int_{a}^{t}\left(\frac{\mbox{d}X(\tau,\omega)}{\mbox{d}\tau}\right)\mbox{d}\tau\right|\right\} =1.\]</span></p></li>
</ul>
<p>The term <span class="math inline">\(\mathbb{E}[X(t,\omega)X(s,\omega)]\)</span> connects to the <strong>mean square</strong> criterion:
<span class="math display">\[\begin{align*}\mathbb{E}\left[\left|X(t+h)-X(t)\right|^{2}\right]&amp;=\mathbb{E}\left[X(t+h)X(t+h)\right]-\mathbb{E}\left[X(t+h)X(t)\right]\\
&amp;-\mathbb{E}\left[X(t)X(t+h)\right]+\mathbb{E}\left[X(t)X(t)\right].\end{align*}\]</span>
That is to say, the <strong>mean square continuity</strong> is all about continuity of <span class="math inline">\(\mathbb{E}[X(t,\omega)X(s,\omega)]\)</span> for any <span class="math inline">\(t,s\)</span>. In addition, both <strong>mean square differentiability and integrability</strong> relate to the calculus of <span class="math inline">\(\mathbb{E}[X(t,\omega)X(s,\omega)]\)</span>.<label for="tufte-sn-431" class="margin-toggle sidenote-number">431</label><input type="checkbox" id="tufte-sn-431" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">431</span> To have a vague idea about this relationship, recall that the definitions of <a href="sub-calculus.html#sub:diffInt">differentiation</a> and <a href="sub-calculus.html#sub:diffInt">integration</a> are both based on the continuity of some criteria of <a href="sub-continuity.html#sub:rational">infinitesimal changes</a>. Thus, the <strong>mean square differentiability and integrability</strong> must be based on the continuity of some <strong>mean square</strong> criteria of <a href="sub-continuity.html#sub:rational">infinitesimal changes</a>. As these <strong>mean square</strong> criteria all relate to the terms of <span class="math inline">\(\mathbb{E}[X(t,\omega)X(s,\omega)]\)</span>, we can see that the essence of defining <strong>mean square differentiability and integrability</strong> is to define the differentiability and integrability of <span class="math inline">\(\mathbb{E}[X(t,\omega)X(s,\omega)]\)</span> for any <span class="math inline">\(t,s\)</span>.</span>
By the equality
<span class="math display">\[\mathbb{E}[X(t,\omega)X(s,\omega)]=\mbox{Cov}_X (t,s)+\mu_{X}(t)\mu_{X}(s),\]</span> one can induce that the <strong>mean square continuity, differentiability, and integrability</strong> actually relate to the continuity and the calculus of <strong>mean and covariance functions</strong>.</p>
<div class="solution">
<p class="solution-begin">
Some remark about the mean square criterion <span id="sol-start-100" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-100', 'sol-start-100')"></span>
</p>
<div id="sol-body-100" class="solution-body" style="display: none;">
<p>Let’s discretize the stochastic process <span class="math inline">\(X(t,\omega)\)</span> into a stochastic sequence <span class="math inline">\(\{X_{n}(\omega)\}_{n\in\mathbb{N}}\)</span>.</p>
<ol style="list-style-type: decimal">
<li>The <strong>mean square convergence</strong> implies probabilistic convergence in section <a href="ch-UnMulti.html#sub:WLLN">13.4</a>.</li>
</ol>
<p>Proof: For any <span class="math inline">\(\epsilon&gt;0\)</span> and a random variable <span class="math inline">\(Y\)</span> with a finite second moment, we have
<span class="math display">\[\begin{align*}\mathbb{E}[|Y|^{2}]=&amp;   \int|y|^{2}p(y)\mbox{d}y\\ &amp;=\int_{-\infty}^{-\epsilon}|y|^{2}p(y)\mbox{d}y+\int_{-\epsilon}^{\infty}|y|^{2}p(y)\mbox{d}y\\
&amp;\geq   \epsilon^{2}\left(\int_{-\infty}^{-\epsilon}p(y)\mbox{d}y+\int_{-\epsilon}^{\infty}p(y)\mbox{d}y\right)\\
&amp;=\epsilon^{2}\Pr\left\{ |y|\geq\epsilon\right\}.\end{align*}\]</span>
Substituting <span class="math inline">\(X-X_{n}\)</span> for <span class="math inline">\(Y\)</span>, we have
<span class="math display">\[\Pr\left\{ |X-X_{n}|\geq\epsilon\right\} \leq\frac{\mathbb{E}\left[\left|X_{n}-X\right|^{2}\right]}{\epsilon^{2}}\]</span>
by the <a href="ch-UnMulti.html#sub:WLLN">Markov inequality</a> (see the proof of <a href="ch-UnMulti.html#sub:WLLN">weak law of large numbers</a>).</p>
<ol start="2" style="list-style-type: decimal">
<li><p>The <a href="sub-continuity.html#sub:Cauchy">Cauchy criterion</a> in the <strong>mean square</strong> sense means
<span class="math display">\[\lim_{n,m\rightarrow\infty}\mathbb{E}\left[X_{n}-X_{m}\right]^{2}=0.\]</span>
If the <a href="sub-continuity.html#sub:Cauchy">Cauchy criterion</a> holds, we say that the stochastic sequence <span class="math inline">\(\{X_{n}\}\)</span> <strong>converges in mean square</strong> to some <span class="math inline">\(X\)</span>,
<span class="math display">\[\lim_{n\rightarrow\infty}\mathbb{E}\left[X_{n}-X\right]^{2}=0.\]</span></p></li>
<li><p>If <span class="math inline">\(X_n\)</span> <strong>converges in mean square</strong> to <span class="math inline">\(X\)</span>, then
<span class="math display">\[\mathbb{E}[X]=\lim_{n\rightarrow\infty}\mathbb{E}[X_n].\]</span></p></li>
</ol>
<p>Proof: Notice that the following equality <span class="math display">\[\lim_{n\rightarrow\infty}\mathbb{E}\left[\left|X_{n}-X\right|^{2}\right]=\lim_{n\rightarrow\infty}\left|\mathbb{E}[X_{n}-X]\right|^{2}=0\]</span>
can be verified by the <a href="ch-vecMat.html#sub:vec">Schwarz inequality</a>: <span class="math display">\[\left|\mathbb{E}[X_{n}-X]\right|^{2}\leq\mathbb{E}\left[\left|X_{n}-X\right|^{2}\right].\]</span>
Because <span class="math inline">\(\lim_{n\rightarrow\infty}\left|\mathbb{E}[X_{n}-X]\right|^{2}=0\)</span> means
<span class="math display">\[\lim_{n\rightarrow\infty}\mathbb{E}[X_{n}-X]=0,\]</span> the result follows.</p>
<ol start="4" style="list-style-type: decimal">
<li>If <span class="math inline">\(X_n\)</span> and <span class="math inline">\(Y_n\)</span> <strong>converge in mean square</strong> to <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, then <span class="math display">\[\mathbb{E}[XY]=\lim_{n\rightarrow\infty}\mathbb{E}[X_{n}Y_{n}].\]</span>
</li>
</ol>
<p>Proof: Note that if <span class="math inline">\(X_n\)</span> and <span class="math inline">\(Y_n\)</span> <strong>converge in mean square</strong> to <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, we have
<span class="math display">\[\lim_{n\rightarrow\infty}\mathbb{E}[X_{n}-X]=\lim_{n\rightarrow\infty}\mathbb{E}[Y_{n}-Y]=0.\]</span></p>
<p>To see this, we make the following inequality
<span class="math display">\[\begin{align*}\left|\mathbb{E}[XY]-\mathbb{E}[X_{n}Y_{n}]\right|  =\left|\mathbb{E}[(Y-Y_{n})X_{n}]+\mathbb{E}[(X-X_{t})Y_{n}]-\mathbb{E}[(X-X_{t})(Y-Y_{n})]\right|\\
\leq    \left|\mathbb{E}[(Y-Y_{n})X_{n}]\right|+\left|\mathbb{E}[(X-X_{n})Y_{n}]\right|+\left|\mathbb{E}[(X-X_{n})(Y-Y_{n})]\right|.\end{align*}\]</span>
Then we apply <a href="ch-vecMat.html#sub:vec">Schwarz inequality</a> to the last three terms and take the limit
<span class="math display">\[\begin{align*}
&amp;\lim_{n\rightarrow\infty}\left|\mathbb{E}[XY]-\mathbb{E}[X_{n}Y_{n}]\right|    \leq
\lim_{n\rightarrow\infty}   \mathbb{E}\left|Y-Y_{n}|\mathbb{E}|X_{n}\right|\\
+&amp;\mathbb{E}\left|X-X_{n}| \mathbb{E}|Y_{n}\right|+\mathbb{E}\left|X-X_{n}|\mathbb{E}|Y-Y_{n}\right|\rightarrow0.\end{align*}\]</span>
The result follows.</p>
<ol start="5" style="list-style-type: decimal">
<li>In the view of the <a href="sub-continuity.html#sub:Cauchy">Cauchy criterion</a>, the <strong>mean square differentiability</strong> is about the convergence of the infinitesimal stochastic sequence <span class="math inline">\(\left\{\frac{X_{t+h}-X_{t}}{h}\right\}\)</span> such that <span class="math display">\[\lim_{h,s\rightarrow0}\mathbb{E}\left[\left|\frac{X_{t+h}-X_{t}}{h}-\frac{X_{t+s}-X_{t}}{s}\right|^{2}\right]=0.\]</span> We can show that the convergence holds if <span class="math inline">\(\frac{\partial^{2}\mathbb{E}[X_{t}(\omega)X_{s}(\omega)]}{\partial t\partial s}\)</span> exists.</li>
</ol>
<p>Proof: Because <span class="math display">\[\begin{align*}\mathbb{E}\left[\left|\frac{X_{t+h}-X_{t}}{h}-\frac{X_{t+s}-X_{t}}{s}\right|^{2}\right]&amp;=\mathbb{E}\left[\frac{X_{t+h}-X_{t}}{h}\frac{X_{t+h}-X_{t}}{h}\right]\\-2\mathbb{E}\left[\frac{X_{t+h}-X_{t}}{h}\frac{X_{t+s}-X_{t}}{s}\right]&amp;+\mathbb{E}\left[\frac{X_{t+s}-X_{t}}{s}\frac{X_{t+s}-X_{t}}{s}\right].\end{align*}\]</span></p>
<p>Notice that <span class="math display">\[\lim_{h,s\rightarrow0}\mathbb{E}\left[\frac{X_{t+h}-X_{t}}{h}\times\frac{X_{t+s}-X_{t}}{s}\right] =\lim_{h,s\rightarrow0}\frac{1}{hs}\left[\mathbb{E}[X_{t+h}X_{t+s}]\\-\mathbb{E}[X_{t}X_{t+s}]-(\mathbb{E}[X_{t+h}X_{t}]-\mathbb{E}[X_{t}X_{t}])\right]=\frac{\partial^{2}\mathbb{E}[X_{t}X_{t}]}{\partial t\partial t}.\]</span></p>
<p>Similarly,
<span class="math display">\[\begin{align*}\lim_{h\rightarrow0}\mathbb{E}\left[\frac{X_{t+h}-X_{t}}{h}\frac{X_{t+h}-X_{t}}{h}\right]&amp;=\lim_{s\rightarrow0}\mathbb{E}\left[\frac{X_{t+h}-X_{t}}{h}\frac{X_{t+h}-X_{t}}{h}\right]\\&amp;=\frac{\partial^{2}\mathbb{E}[X_{t}X_{t}]}{\partial t\partial t}.\end{align*}\]</span>
So when the second derivatives of <span class="math inline">\(\mathbb{E}[X_{t}X_{t}]\)</span> exists, the sequence <span class="math inline">\(\left\{\frac{X_{t+h}-X_{t}}{h}\right\}\)</span> <strong>converges in mean square</strong> to <span class="math inline">\(\mbox{d}X_{t}/\mbox{d}t\)</span>.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>With the <strong>mean square calculus</strong>, we can define a <strong>white noise process</strong>, a baseline of modeling unpredictable fluctuation in the <strong>second-order processes</strong>. (See figure <a href="ch-randomization.html#fig:GP">16.2</a>.)</p>
<p>Let <span class="math inline">\(\varepsilon(t,\omega)\)</span> denote the <strong>white noise</strong>. The process has zero <strong>mean function</strong> and its <strong>covariance function</strong> is <span class="math display">\[\mbox{Cov}_{\varepsilon}(t,s)=\sigma^{2}\delta(t-s)\]</span> where <span class="math inline">\(\delta(\cdot)\)</span> is called the <em>delta function</em> such that <span class="math display">\[\begin{align*}\delta(x)=\begin{cases}
\infty, &amp; x=0,\\
0, &amp; x\neq0,
\end{cases} &amp; \; \mbox{ and }\\ 
\int_{-\infty}^{\infty}\delta(x)\mbox{d}x=1,&amp; \,\, \int_{-\infty}^{\infty}f(x)\delta(x)\mbox{d}x=f(0)
\end{align*}\]</span>
for any continuous function <span class="math inline">\(f(\cdot)\)</span>. Basically, a <strong>delta function</strong> is an infinite “spike” above a single point.<label for="tufte-sn-432" class="margin-toggle sidenote-number">432</label><input type="checkbox" id="tufte-sn-432" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">432</span> One can shift the origin to any point <span class="math inline">\(t\)</span>
by setting the argument to <span class="math inline">\(x-t\)</span> such that <span class="math display">\[\delta(x-t)=\begin{cases}
\infty, &amp; x=t,\\
0, &amp; x\neq t.
\end{cases}\]</span></span>
The <strong>delta function</strong> can be thought of as the derivative of the step function <span class="math inline">\(\mathbf{1}_{\{x&gt;0\}}(x)\)</span> that has zero everywhere and goes to infinity at <span class="math inline">\(0\)</span>.<label for="tufte-sn-433" class="margin-toggle sidenote-number">433</label><input type="checkbox" id="tufte-sn-433" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">433</span> Note that for any <span class="math inline">\(\epsilon&gt;0\)</span>, the derivative of the step function is <span class="math display">\[\begin{align*}
&amp;\lim_{\epsilon\rightarrow0}\frac{\mathbf{1}_{\{x&gt;0\}}(x+\epsilon)-\mathbf{1}_{\{x&gt;0\}}(x)}{\epsilon}=\\
&amp;\begin{cases}
\lim_{\epsilon\rightarrow0}\frac{1}{\epsilon}\rightarrow\infty &amp; x=0\\
\lim_{\epsilon\rightarrow0}\frac{0}{\epsilon}\rightarrow 0 &amp; x\neq 0
\end{cases}.\end{align*}\]</span></span></p>
<p>For a <strong>white noise process</strong>, if the probability law in the <strong><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</strong> is Gaussian, then the <strong>white noise</strong> is called <em>Gaussian white noise</em>,<label for="tufte-sn-434" class="margin-toggle sidenote-number">434</label><input type="checkbox" id="tufte-sn-434" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">434</span> Notice that the inner product <span class="math inline">\(\langle f,\delta \rangle=f(0)\)</span> is well-defined. So the <strong>delta function</strong> is somehow similar to the <a href="sub-set-theory.html#sub:func">indicator function</a> at the origin as <span class="math display">\[\int_{-\infty}^{\infty}f(x)\mathbf{1}_{\{x=0\}}(x)\mbox{d}x=f(0).\]</span> In practice, one implements <span class="math inline">\(\mathbf{1}_{\{x=t\}}(x)\)</span>
instead of <span class="math inline">\(\delta(x-t)\)</span> as a (numerical) <strong>delta function</strong>, because the infinite value of the <strong>delta function</strong> is impossible for the implementation.</span> <span class="math display">\[\varepsilon(t,\omega)\sim\mathcal{N}(0,\sigma^{2}\delta(x-t)).\]</span></p>
<p>The <strong>Gaussian white noise</strong> <span class="math inline">\(\varepsilon(t,\omega)\)</span> corresponds to the <strong>mean square derivative</strong> of a <a href="ch-DE.html#sub:pde">Brownian motion</a> <span class="math inline">\(X(t,\omega)\)</span>:
<span class="math display">\[\varepsilon(t,\omega)=\frac{\mbox{d}X(t,\omega)}{\mbox{d}t}.\]</span>
To see the connection, we analyze the <strong>mean and covariance functions</strong> of both processes. The <strong>mean functions</strong> of both white noise and Brownian motion are zero. So the <strong>mean square derivative</strong> does not change anything for the mean. The <strong>covariance function</strong> of the <a href="ch-DE.html#sub:pde">Brownian motion</a> is <span class="math inline">\(\mbox{Cov}_{X}(t,s)=\sigma^{2}\min(t,s)\)</span>. According to the <strong>mean square differentiability</strong>, we should check the second derivative of this function, and we can see it equals to the <strong>covariance function</strong> of the <strong>white noise</strong><label for="tufte-sn-435" class="margin-toggle sidenote-number">435</label><input type="checkbox" id="tufte-sn-435" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">435</span> Note that<span class="math display">\[\min(t,s)=\begin{cases}
s, &amp; s&lt;t\\
t, &amp; s&gt;t
\end{cases},\\
\frac{\partial\min(t,s)}{\partial t}=\mathbf{1}_{\{s&gt;t\}}(s)=\begin{cases}
0, &amp; s&lt;t\\
1, &amp; s&gt;t
\end{cases}.\]</span>
That is, the first derivative of <span class="math inline">\(\min(t,s)\)</span> is the step function <span class="math inline">\(\mathbf{1}_{\{s&gt;t\}}(s)\)</span>. As we know, the derivative of a step function is the <strong>delta function</strong>. The result follows.</span> <span class="math display">\[\mbox{Cov}_{\varepsilon}(t,s)=\frac{\partial^{2}\mbox{Cov}_{B}(t,s)}{\partial t\partial s}=\sigma^{2}\frac{\partial^{2}\min(t,s)}{\partial t\partial s}=\sigma^{2}\delta(t-s).\]</span>
Thus, we have the result that the <strong>mean square differentiation</strong> of a <a href="ch-DE.html#sub:pde">Brownian motion</a> is a <strong>white noise process</strong>.<label for="tufte-sn-436" class="margin-toggle sidenote-number">436</label><input type="checkbox" id="tufte-sn-436" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">436</span> The discretization of the covariance functions of Brownian motion and white noise can be found in figure <a href="ch-randomization.html#fig:GPKern">16.3</a>.</span></p>
<p>Conversely, if we consider <span class="math inline">\(\int_{s}^{t}\varepsilon(\tau,\omega)\mbox{d}\tau\)</span> to be a “proper” integral, by the <strong>fundamental theorem of mean square calculus</strong>, we expect to have
<span class="math display">\[\int_{s}^{t}\varepsilon(\tau,\omega)\mbox{d}\tau=X(t,\omega)-X(s,\omega)\mbox{ with probability }1,\]</span>
where <span class="math inline">\(X(t,\omega)-X(s,\omega)\)</span> is also called the <em>increment</em> of <a href="ch-DE.html#sub:pde">Brownian motion</a>.<label for="tufte-sn-437" class="margin-toggle sidenote-number">437</label><input type="checkbox" id="tufte-sn-437" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">437</span> The “integral” <span class="math inline">\(\int_{s}^{t}\varepsilon(\tau,\omega)\mbox{d}\tau\)</span>, however, is not technically well defined over here. A precise definition of the integral is not trivial, because some quantities that are finite sums in the finite discrete case must be replaced by integrals that may not converge in the <strong>mean square</strong> sense. Thus, instead of dealing with the rigorous definition, here we only consider a vague presentation of the result.</span>
The <strong>covariance function</strong> of the <strong>increment of Brownian motion</strong> <span class="math inline">\(X(t,\omega)-X(s,\omega)\)</span> follows the probability law <span class="math inline">\(\mathcal{N}(0,(t-s)\sigma^{2})\)</span>, which is what we expect to see on the “integral” of the <strong>white noise process</strong>.<label for="tufte-sn-438" class="margin-toggle sidenote-number">438</label><input type="checkbox" id="tufte-sn-438" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">438</span> The fact is that <a href="ch-DE.html#sub:pde">Brownian motion</a> is a mathematically tractable version of the idea of the <a href="ch-UnMulti.html#sub:rw">random walk</a> in the continuous time setting. Intuitively, one can think of <a href="ch-UnMulti.html#sub:rw">random walks</a> <span class="math inline">\(X_{s}=\varepsilon_{1}+\cdots+\varepsilon_{s}\)</span>
and <span class="math inline">\(X_{t}=\varepsilon_{1}+\cdots+\varepsilon_{t}\)</span>. Then <span class="math display">\[X_{t}-X_{s}=\underset{(t-s)-\mbox{entities}}{\underbrace{\varepsilon_{t}+\varepsilon_{t-1}+\cdots+\varepsilon_{s}}}.\]</span> For each <span class="math inline">\(\varepsilon_{i}\sim\mathcal{N}(0,\sigma^{2})\)</span>, the <a href="ch-CalUn.html#sub:divRV">Gaussian property</a> tells that <span class="math display">\[\sum_{i=s}^{t}\varepsilon_{i}\sim\mathcal{N}(0,(t-s)\sigma^{2}).\]</span></span></p>
</div>
<div id="sub:SumDecomp" class="section level2">
<h2>
<span class="header-section-number">16.2</span> Direct Sums and Decompositions</h2>
<p>When we compare the <a href="ch-representation.html#ch:representation">representation</a> of a <a href="ch-UnMulti.html#sub:rw">random walk</a> <span class="math inline">\(X_{t}(\omega)\in\mathbb{R}\)</span> and that of a vector <span class="math inline">\(\mathbf{x}\in\mathbb{R}^{t}\)</span>, we can find some interesting similarities:
<span class="math display">\[\begin{align*}X_{t}(\omega) &amp;=    \varepsilon_{1}(\omega)+\cdots+\varepsilon_{t}(\omega), \, \varepsilon_{i} \sim \mathcal{N}(0,1), \\
\left\langle \varepsilon_{i}(\omega),\varepsilon_{j}(\omega)\right\rangle _{\mathbb{P}}&amp;=\mathbb{E}\left[\left\langle \varepsilon_{i}(\omega),\varepsilon_{j}(\omega)\right\rangle \right]  =\begin{cases}
0, &amp; i\neq j\\
1, &amp; i=j
\end{cases},\\
\mathbf{x} &amp;=   c_{1}\mathbf{e}_{1}+\cdots+c_{n}\mathbf{e}_{t},\\
\left\langle \mathbf{e}_{i},\mathbf{e}_{j}\right\rangle &amp;=\begin{cases}
0, &amp; i\neq j\\
1, &amp; i=j
\end{cases}.\end{align*}\]</span>
The (discrete time) <a href="ch-randomization.html#sub:RHilbert">Gaussian white noise</a> <span class="math inline">\(\varepsilon_i(\omega)\)</span> with unit variance looks like a “whitened” <a href="ch-vecMat.html#sub:vec">unit vector</a> <span class="math inline">\(\mathbf{e}_i\)</span>. Moreover, the collection of white noises seems to play the same role as the <a href="ch-UnMulti.html#sub:MultiVar">orthonormal basis</a> to the random walk <span class="math inline">\(X_{t}(\omega)\)</span> in a <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a>.</p>
<p>Given this result, a natural question is to ask whether we can represent any <a href="ch-randomization.html#sub:RHilbert">second-order</a> <span class="math inline">\(\mathcal{H}\)</span>-valued time series or stochastic process in terms of white noises? If this is the case, then we can think the <a href="ch-randomization.html#sub:RHilbert">randomization</a> of the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a> is about randomizing the Hilbert space <span class="math inline">\(\mathcal{H}\)</span> through “whitening” the <a href="ch-representation.html#sub:innerProd">basis</a> of <span class="math inline">\(\mathcal{H}\)</span>.</p>
<p>The route of our exposition will start with defining how to construct a space by many <a href="ch-MatComp.html#sub:vecSpaces">subspaces</a>; then we will study the subspaces generated by white noises and will see what kind of processes can live in the “combination” of those subspaces. The switch of the focus from white noises to the space of white noises will help us to bridge the difference between the <a href="ch-representation.html#ch:representation">representations</a> of the stochastic objects and of the deterministic ones.</p>
<p>Now we start with introducing the way of “combining” the <a href="ch-MatComp.html#sub:vecSpaces">subspaces</a>.</p>
<ul>
<li>
<strong>Direct sums, decompositions</strong> and <strong>complements</strong> : Consider two <a href="ch-MatComp.html#sub:vecSpaces">vector spaces</a> <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> over the same <a href="ch-MatComp.html#sub:vecSpaces">scalar field</a> <span class="math inline">\(\mathbb{F}\)</span>. When the two spaces can be combined to form a new vector space <span class="math inline">\(\mathcal{V}\)</span>, then <span class="math inline">\(\mathcal{V}\)</span> is called the <em>direct sum</em> of <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span>, denoted by <span class="math display">\[\mathcal{V}=\mathcal{V}_{1}+\mathcal{V}_{2}.\]</span>
If <span class="math inline">\(\mathcal{V}\)</span> is the <strong>direct sum</strong> of <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span>, then all objects in <span class="math inline">\(\mathcal{V}\)</span> can be <em>decompose</em> into those from <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> such that for any <span class="math inline">\(f\in\mathcal{V}\)</span>, there is <span class="math inline">\(f=f_{1}+f_{2}\)</span> where <span class="math inline">\(f_{1}\in\mathcal{V}_{1}\)</span> and <span class="math inline">\(f_{2}\in\mathcal{V}_{2}\)</span>, and we say <span class="math inline">\(\mathcal{V}_1\)</span> and <span class="math inline">\(\mathcal{V}_2\)</span> are <em>complements</em> of each other. In addition, if <span class="math inline">\(\mathcal{V}_{1}\)</span>
and <span class="math inline">\(\mathcal{V}_{2}\)</span> are <a href="ch-representation.html#sub:conjugacy">Hilbert spaces</a>, and they are <em>disjoint</em><label for="tufte-sn-439" class="margin-toggle sidenote-number">439</label><input type="checkbox" id="tufte-sn-439" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">439</span> The disjoint property tells that the <strong>direct sum</strong> can be uniquely <strong>decomposed</strong>. The proof is given below.</span> and <em>orthogonal</em><label for="tufte-sn-440" class="margin-toggle sidenote-number">440</label><input type="checkbox" id="tufte-sn-440" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">440</span> The orthogonality of the subspaces <span class="math inline">\(\mathcal{V}_1 \bot \mathcal{V}_2\)</span> are defined as <span class="math display">\[\langle f,g\rangle=0\]</span> for all <span class="math inline">\(f\in\mathcal{V}_1\)</span> and <span class="math inline">\(g\in\mathcal{V}_2\)</span>.</span>, we say <span class="math inline">\(\mathcal{V}\)</span> is the <em>orthogonal direct sum</em> of <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span>, denoted by <span class="math display">\[\mathcal{V}=\mathcal{V}_{1}\oplus\mathcal{V}_{2},\]</span> where <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> are <em>orthogonal complements</em> of one another.<label for="tufte-sn-441" class="margin-toggle sidenote-number">441</label><input type="checkbox" id="tufte-sn-441" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">441</span> The <strong>orthogonal complement</strong> of <span class="math inline">\(\mathcal{V}_1\subset\mathcal{V}\)</span> is denoted by <span class="math inline">\(\mathcal{V}_1^{\bot}\)</span>. If <span class="math inline">\(\mathcal{V}=\mathcal{V}_{1}\oplus\mathcal{V}_{2}\)</span>, then <span class="math inline">\(\mathcal{V}_2 = \mathcal{V}_1^{\bot}\)</span>.</span>
</li>
</ul>
<div class="solution">
<p class="solution-begin">
Proof <span id="sol-start-101" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-101', 'sol-start-101')"></span>
</p>
<div id="sol-body-101" class="solution-body" style="display: none;">
<p>For two subspaces <span class="math inline">\(\mathcal{V}_1\)</span> and <span class="math inline">\(\mathcal{V}_2\)</span>, the following two statements are equivalent:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(f\in \mathcal{V}_{1} + \mathcal{V}_{2}\)</span>, and <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> are disjoint.</p></li>
<li><p>There is a unique <span class="math inline">\(f_{1}\in\mathcal{V}_{1}\)</span> and a unique <span class="math inline">\(f_{2}\in\mathcal{V}_{2}\)</span> to <strong>decompose</strong> <span class="math inline">\(f\)</span>: <span class="math display">\[f=f_{1}+f_{2}.\]</span></p></li>
</ol>
<p>Proof:</p>
<ol style="list-style-type: decimal">
<li>Suppose that <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> are disjoint. Let <span class="math display">\[f=f_{1}+f_{2}=g_{1}+g_{2},\]</span> where <span class="math inline">\(f_{1},g_{1}\in\mathcal{V}_{1}\)</span> and <span class="math inline">\(f_{2},g_{2}\in\mathcal{V}_{2}\)</span>. So we have <span class="math inline">\(f_{1}-g_{1}=g_{2}-f_{2}\)</span>. By the <a href="ch-MatComp.html#sub:vecSpaces">subspace property</a>, we have <span class="math inline">\(f_{1}-g_{1}\in\mathcal{V}_{1}\)</span> and <span class="math inline">\(f_{2}-g_{2}\in\mathcal{V}_{2}\)</span>. By the disjoint condition <span class="math inline">\(\mathcal{V}_{1}\cap\mathcal{V}_{2}=\{0\}\)</span>, one can induce that <span class="math display">\[f_{1}-g_{1}=g_{2}-f_{2}=0,\]</span> which implies <span class="math inline">\(f_{1}=g_{1}\)</span> and <span class="math inline">\(g_{2}=f_{2}\)</span>.</li>
</ol>
<p>2: Now suppose that <span class="math inline">\(f_{1}\)</span> and <span class="math inline">\(f_{2}\)</span> are uniquely determined <span class="math inline">\(f=f_{1}+f_{2}\)</span> in <span class="math inline">\(\mathcal{V}_{1}+\mathcal{V}_{2}\)</span>, we need to show that <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> are disjoint. Suppose that <span class="math inline">\(\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\)</span> are not disjoint, then there exists <span class="math inline">\(g\neq0\)</span> in <span class="math inline">\(\mathcal{V}_{1}\cap\mathcal{V}_{2}\)</span>. Then for any <span class="math inline">\(f=f_{1}+f_{2}\)</span>, we have
<span class="math display">\[f=(f_{1}+cg)+(f_{2}-cg)\]</span>
for any scalar <span class="math inline">\(c\)</span>. But then <span class="math inline">\(f\)</span> is not uniquely determined, a contradiction.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Let’s consider three examples of “summing” the subspaces or “decomposing” a space.</p>
<p><span class="newthought">Range space and null space </span></p>
<p>For a non-square matrix <span class="math inline">\(\mathbf{A}\in\mathbb{F}^{n\times m}\)</span>, the <em>column space</em> (or called <em>range space</em> when we consider the operator rather than the matrix) of <span class="math inline">\(\mathbf{A}\)</span> is defined by:<label for="tufte-sn-442" class="margin-toggle sidenote-number">442</label><input type="checkbox" id="tufte-sn-442" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">442</span> Notice that the <strong>column/range space</strong> <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span> is the <a href="sub-set-theory.html#sub:func">image</a> of the linear transformation <span class="math inline">\(\mathbf{A}\)</span>.</span>
<span class="math display">\[\mbox{Range}(\mathbf{A}):=\left\{ \mathbf{y}\in\mathbb{F}^{n}:\,\mathbf{y}=\mathbf{A}\mathbf{x},\mathbf{x}\in\mathbb{F}^{m},\mathbf{A}\in\mathbb{F}^{n\times m}\right\}.\]</span>
The solution of the system <span class="math inline">\(\mathbf{b}=\mathbf{A}\mathbf{x}\)</span> exists only if <span class="math inline">\(\mathbf{b}\)</span> is in <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span>, i.e., <span class="math inline">\(\mathbf{b}\in\mbox{Range}(\mathbf{A})\)</span>. The <strong>column space</strong> <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span> contains all the <a href="ch-vecMat.html#sub:vec">linear combinations</a> of the columns of <span class="math inline">\(\mathbf{A}\)</span>. It is a <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> of <span class="math inline">\(\mathbb{F}^{n}\)</span>.</p>
<p>The <em>null space</em> is <a href="ch-vecMat.html#sub:vec">orthogonal</a> to <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span><label for="tufte-sn-443" class="margin-toggle sidenote-number">443</label><input type="checkbox" id="tufte-sn-443" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">443</span> To see the orthogonality, for any <span class="math inline">\(\mathbf{x}\in\mbox{Range}(\mathbf{A})\)</span>, there is
<span class="math display">\[\left\langle \mathbf{A}\mathbf{x},\,\mathbf{y}\right\rangle =\mathbf{x}^{\mbox{H}}\mathbf{A}^{\mbox{H}}\mathbf{y}=0\]</span> for any <span class="math inline">\(\mathbf{y}\in\mbox{Null}(\mathbf{A}^{\mbox{H}})\)</span>.</span>
<span class="math display">\[\mbox{Null}(\mathbf{A}^{\mbox{H}}):=\left\{ \mathbf{y}\in\mathbb{F}^{n}:\mathbf{A}^{\mbox{H}}\mathbf{y}=0,\,\mathbf{A}^{\mbox{H}}\in\mathbb{F}^{m\times n}\right\}\]</span> The <strong>null space</strong> of <span class="math inline">\(\mathbf{A}^{\mbox{H}}\)</span> consists of all solutions <span class="math inline">\(\mathbf{A}^{\mbox{H}}\mathbf{y}=0\)</span>, it is also a <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> of <span class="math inline">\(\mathbb{F}^{n}\)</span>.</p>
<p>The <strong>null space</strong> and <strong>column space</strong> are <strong>orthogonal complements</strong> such that:
<span class="math display">\[\mbox{Range}(\mathbf{A})\oplus\mbox{Null}(\mathbf{A}^{\mbox{H}})=\mathbb{F}^{n}.\]</span>
This result is a generalization of the <a href="ch-optApp.html#sub:Proj1">Farkas’ lemma</a>. Basically it reveals a dichotomy about the sandwich form <span class="math inline">\(\left\langle \mathbf{x},\,\mathbf{A}^{\mbox{H}}\mathbf{y}\right\rangle\)</span> for a given system <span class="math inline">\(\mathbf{y}=\mathbf{A}\mathbf{x}\)</span>: either the sandwich form is zero or non-zero.<label for="tufte-sn-444" class="margin-toggle sidenote-number">444</label><input type="checkbox" id="tufte-sn-444" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">444</span> That is, for any <span class="math inline">\(\mathbf{x},\mathbf{y}\in\mathbb{F}^{n}\)</span>, either <span class="math inline">\(\left\langle \mathbf{x},\,\mathbf{A}^{\mbox{H}}\mathbf{y}\right\rangle =0\)</span> or
<span class="math display">\[\begin{align*}\left\langle \mathbf{x},\,\mathbf{A}^{\mbox{H}}\mathbf{y}\right\rangle &amp;=\left\langle \mathbf{A}\mathbf{x},\,\mathbf{y}\right\rangle\\
&amp;=\left\langle \mathbf{y},\,\mathbf{y}\right\rangle =\|\mathbf{y}\|^{2}\neq0.\end{align*}\]</span></span> The dichotomy is also known as the <em>Fredholm alternative</em>.</p>
<div class="solution">
<p class="solution-begin">
Fredholm alternative <span id="sol-start-102" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-102', 'sol-start-102')"></span>
</p>
<div id="sol-body-102" class="solution-body" style="display: none;">
<p>The <strong>Fredholm alternative</strong> states as follows:</p>
<p>For any <span class="math inline">\(\mathbf{y}\in\mathbb{R}^{n}\)</span> such that <span class="math inline">\(\mathbf{A}^{\top}\mathbf{y}=0\)</span>, then <span class="math inline">\(\mathbf{b}=\mathbf{A}\mathbf{x}\)</span> has a solution if and only if
<span class="math display">\[\langle\mathbf{A}\mathbf{x},\mathbf{y}\rangle=\langle\mathbf{b},\mathbf{y}\rangle=0.\]</span></p>
<p>The statement can be summarized as: the system <span class="math inline">\(\mathbf{b}=\mathbf{A}\mathbf{x}\)</span> has a solution (<span class="math inline">\(\mathbf{b}\in\mbox{Range}(\mathbf{A})\)</span> if and only if <span class="math inline">\(\langle\mathbf{b},\mathbf{y}\rangle=0\)</span> for any <span class="math inline">\(\mathbf{y}\in \mbox{Null}(\mathbf{A}^{\top})\)</span> (<span class="math inline">\(\mathbf{b}\in(\mbox{Null}(\mathbf{A}^{\top}))^{\bot}\)</span>).</p>
<p>Notice that <span class="math inline">\(\mbox{Range}(\mathbf{A})=(\mbox{Null}(\mathbf{A}^{\top}))^{\bot}\)</span>.</p>
<p>Thus, the <strong>Fredholm alternative</strong> essentially tells the dichotomy: <span class="math inline">\(\mathbf{b}\in\mbox{Range}(\mathbf{A})\)</span> (has a solution) or <span class="math inline">\(\mathbf{b}\in\mbox{Null}(\mathbf{A}^{\top})\)</span> (no solution).</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>The <strong>complementary subspaces</strong> tell that the “dimension” of the <strong>column space</strong> is not only a property of <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span> but also the property of <span class="math inline">\(\mbox{Null}(\mathbf{A}^{\mbox{H}})\)</span>.<label for="tufte-sn-445" class="margin-toggle sidenote-number">445</label><input type="checkbox" id="tufte-sn-445" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">445</span> The dimension of the space refers to the cardinal number of the <a href="ch-MatComp.html#sub:vecSpaces">basis</a> of that space.</span> That is, if the “dimension” of <span class="math inline">\(\mbox{Range}(\mathbf{A})\)</span> grows, then the “size” of <span class="math inline">\(\mbox{Null}(\mathbf{A}^{\mbox{H}})\)</span> will shrink, and vice versa. In the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a>, the zero mean <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathcal{H}\)</span>-valued random variables</a> belong to the <strong>null space</strong> of a <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a>. It turns out that the size of this <strong>null space</strong> is only one dimensional lower than the (non-randomized) <span class="math inline">\(\mathcal{H}\)</span>.<label for="tufte-sn-446" class="margin-toggle sidenote-number">446</label><input type="checkbox" id="tufte-sn-446" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">446</span> For a <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathcal{H}\)</span>-valued random variable</a>, its expectation can be thought of as a <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a>, <span class="math display">\[\begin{align*}\mathbb{E}[X(\omega)]=\int_{\Omega}x\mathbb{P}(\mbox{d}x)\\=\int_{\Omega}xf(x)\mbox{d}x=\langle x,f\rangle\end{align*}\]</span>
where <span class="math inline">\(\mathbb{P}(\mbox{d}x)=f(x)\mbox{d}x\)</span> for the density function <span class="math inline">\(f(\cdot)\)</span>. For a non-trivial <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a> <span class="math inline">\(\langle \cdot,f\rangle\)</span>, i.e., <span class="math inline">\(f\neq0\)</span>, the <strong>null space</strong> <span class="math display">\[\mbox{Null}(f)=\{x\in\mathcal{H}:\langle x,f\rangle=0\}\]</span> is a subspace of <span class="math inline">\(\mathcal{H}\)</span>. In fact, the space <span class="math inline">\(\mbox{Null}(f)\)</span> is so large that it is the “maximal” subspace of <span class="math inline">\(\mathcal{H}\)</span>, i.e., one dimensional lower than <span class="math inline">\(\mathcal{H}\)</span>.</span></p>
<div class="solution">
<p class="solution-begin">
Remarks about the null space of a linear functional <span id="sol-start-103" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-103', 'sol-start-103')"></span>
</p>
<div id="sol-body-103" class="solution-body" style="display: none;">
<p>Consider the null space of a linear functional <span class="math inline">\(g:\mathcal{V}\rightarrow\mathbb{F}\)</span> <span class="math display">\[\mbox{Null}(g):=\left\{ \langle f,\, g\rangle=0,\; f\in\mathcal{V}\right\}.\]</span></p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\mbox{Null}(g)\)</span> is one dimensional lower than the original space <span class="math inline">\(\mathcal{V}\)</span>.</li>
</ol>
<p>Proof: To see that, consider a point <span class="math inline">\(f_{0}\)</span> in <span class="math inline">\(\mathcal{V}\)</span> such that <span class="math inline">\(\langle f_{0},\, g\rangle\neq0\)</span>. Then for any point <span class="math inline">\(f\in\mathcal{V}\)</span>,
we have <span class="math display">\[f_{p}=f-\frac{\langle f,\, g\rangle}{\langle f_{0},\, g\rangle}f_{0}\]</span> where <span class="math inline">\(\langle f_{p},\, g\rangle=0\)</span> because <span class="math display">\[\langle f_{p},g\rangle=\langle f,g\rangle-\frac{\langle f,\, g\rangle}{\langle f_{0},\, g\rangle}\langle f_{0},g\rangle=0.\]</span>
That means each point <span class="math inline">\(f\in\mathcal{V}\)</span> can be expressed as the <strong>direct sum</strong> of a point in <span class="math inline">\(\mbox{Null}(g)\)</span> and a point in the one-dimensional subpsace spanned by <span class="math inline">\(f_{0}\)</span>. Since <span class="math inline">\(\mbox{span}(f_{0})\cap\mbox{ker}(g)=\emptyset\)</span>, we have <span class="math display">\[\mbox{span}(f_{0})\oplus\mbox{ker}(g)=\mathcal{V}.\]</span>
The result follows.</p>
<ol start="2" style="list-style-type: decimal">
<li><p>The inverse of the above statement is also true. That is, if a subspace whose dimension of the <strong>complement</strong> is only one, then we can use a linear functional <span class="math inline">\(g\)</span> to characterize this subspace so that the <strong>orthogonal complement</strong> subspace is equivalent to <span class="math inline">\(\mbox{Null}(g)\)</span>.</p></li>
<li><p>In chapter @ref(#sub:Proj1), we have seen that a <a href="ch-optApp.html#sub:Proj1">hyperlane</a> or (a regression) is one dimensional lower than the original space. In fact, a <a href="ch-optApp.html#sub:Proj1">hyperplane</a> can be viewed as a translated subspace of the <strong>null space</strong> of the linear functional, i.e. <span class="math inline">\(\mbox{Null}(g)\)</span>. Consider the hyperplane <span class="math inline">\(\mathcal{S}(g,c)=\{f\in\mathcal{V}:\,\langle f,g\rangle=c\}\)</span> and let <span class="math inline">\(f_{0}\)</span> be an arbitrary point in <span class="math inline">\(\mathcal{S}\)</span>
. We construct another hyperplane
<span class="math display">\[\mathcal{M}(g,c)=\{f\in\mathcal{V}:\, f+f_{0}\in\mathcal{S}(g,c)\}.\]</span>
This hyperplane is the <strong>null space</strong> of the linear functional <span class="math inline">\(g\)</span>, because <span class="math inline">\(\langle f+f_{0},g\rangle=c\)</span> in <span class="math inline">\(\mathcal{M}(g,c)\)</span> and <span class="math inline">\(\langle f_{0},g\rangle=c\)</span> in <span class="math inline">\(\mathcal{S}(g,c)\)</span>. So for any <span class="math inline">\(f\in \mathcal{M}(g,c)\)</span>, we have <span class="math display">\[\langle f_{0},g\rangle=0.\]</span> That means <span class="math inline">\(\mathcal{M}(g,c)=\mbox{Null}(g)\)</span>.</p></li>
</ol>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p><span class="newthought">Future and past space </span></p>
<p>Let’s look at another example. Consider a vector space linearly generated by <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathcal{H}\)</span>-valued time series</a> <span class="math inline">\(X_{t}(\omega)\)</span> such that
<span class="math display">\[\mbox{span}\left\{ X_{t}(\omega)\right\} =\left\{ \sum_{t}\phi(t)X_{t}(\omega),\,\phi\in\mathcal{H} \right\}.\]</span>
By <em>closing</em> this space, i.e., adding all <a href="ch-randomization.html#sub:RHilbert">mean square limits</a> of all sequences of <span class="math inline">\(\sum_{t}\phi(t)X_{t}(\omega)\)</span>, we will obtain a <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a> generated by the series <span class="math inline">\(X_{t}(\omega)\)</span>, denoted by <span class="math display">\[\mathcal{H}_{X(\omega)}=\overline{\mbox{span}}\left\{ \left.X_{t}(\omega)\right|t\in(-\infty,\infty)\right\},\]</span>
where <span class="math inline">\(\overline{\mbox{span}}\)</span> says that the <a href="">span</a> is <strong>closed</strong>.<label for="tufte-sn-447" class="margin-toggle sidenote-number">447</label><input type="checkbox" id="tufte-sn-447" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">447</span> The extension of the span to the continuous time requires more technicalities and is beyond our current concerns.</span></p>
<p>The space <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span> can be decomposed into two subspaces of interests. They are representing the past and the future space generated by <span class="math inline">\(X_t(\omega)\)</span> respectively:<label for="tufte-sn-448" class="margin-toggle sidenote-number">448</label><input type="checkbox" id="tufte-sn-448" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">448</span> The convention is to include the present in the future only and not in the past.</span>
<span class="math display">\[\mathcal{H}_{X(\omega)}^{t-}=\overline{\mbox{span}}\left\{ \left.X_{s}(\omega)\right|s&lt;t\right\} ,\;\mathcal{H}_{X(\omega)}^{t+}=\overline{\mbox{span}}\left\{ \left.X_{s}(\omega)\right|s\geq t\right\}.\]</span>
Combing the past and the future gives us the whole space, namely <span class="math inline">\(\mathcal{H}_{X(\omega)}=\mathcal{H}_{X(\omega)}^{t-}+\mathcal{H}_{X(\omega)}^{t+}\)</span>. In particular, if the process <span class="math inline">\(X_t(\omega)\)</span> is <a href="ch-UnMulti.html#sub:Markov">Markovian</a>, then the future and the past are orthogonal given the present, which means given <span class="math inline">\(X_{t}(\omega)\)</span>,<label for="tufte-sn-449" class="margin-toggle sidenote-number">449</label><input type="checkbox" id="tufte-sn-449" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">449</span> The orthogonality can be justified by the conditional expectation. We will discuss it in sec[?].</span> <span class="math display">\[\mathcal{H}_{X(\omega)}=\mathcal{H}_{X(\omega)}^{t-}\oplus \mathcal{H}_{X(\omega)}^{t+}.\]</span></p>
<p><span class="newthought">Decomposition by the projection </span></p>
<p>The third example is about decomposing the space by the <a href="ch-representation.html#sub:conjugacy">projection operator</a>. Let <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}:\mathcal{H}\rightarrow \mathcal{Y}\)</span> be the <a href="ch-representation.html#sub:conjugacy">projection operator</a> defined on the Hilbert space <span class="math inline">\(\mathcal{H}\)</span>, such that <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}\mathbf{P}_{\mathcal{Y}}=\mathbf{P}_{\mathcal{Y}}\)</span>. By the property of the projection, it is straightforward to see that <span class="math inline">\(\mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span> and <span class="math inline">\(\mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span> are <strong>disjoint subspaces</strong> of <span class="math inline">\(\mathcal{H}\)</span> such that
<span class="math display">\[\mathcal{H} = \mbox{Range}(\mathbf{P}_{\mathcal{Y}}) + \mbox{Null}(\mathbf{P}_{\mathcal{Y}})=\mathcal{Y} + \mbox{Null}(\mathbf{P}_{\mathcal{Y}}).\]</span> In addition, if the projection is <a href="ch-vecMat.html#sub:vec">orthogonal</a>, then
<span class="math display">\[\mathcal{H} = \mbox{Range}(\mathbf{P})_{\mathcal{Y}} \oplus \mbox{Null}(\mathbf{P}_{\mathcal{Y}})=\mathcal{Y} \oplus \mathcal{Y}^{\bot},\]</span> and <span class="math inline">\(\|f\|^2 = \|f_1\|^2 +\|f_2\|^2\)</span> for <span class="math inline">\(f\in \mathcal{H}\)</span>, <span class="math inline">\(f_1\in \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>, <span class="math inline">\(f_2 \in \mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span>.</p>
<p>In other words, one <a href="ch-representation.html#sub:conjugacy">projection</a> can uniquely decompose the Hilbert space into two <strong>disjoint subspaces</strong>. The <strong>range space</strong> of the projection operator is simply the target subspace (<span class="math inline">\(\mathcal{Y}\)</span>) of the projection. Given a target space, there could be many projections. If we want the <strong>null space</strong> of the projection to be <a href="ch-vecMat.html#sub:vec">orthogonal</a> to the <strong>range space</strong>, then we need the <a href="ch-representation.html#sub:conjugacy">orthogonal projection</a>. Because the <strong>decomposition</strong> is unique, we expect that for any target subspace, there is one and only one <a href="ch-representation.html#sub:conjugacy">orthogonal projection</a>.<label for="tufte-sn-450" class="margin-toggle sidenote-number">450</label><input type="checkbox" id="tufte-sn-450" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">450</span> In fact, if <span class="math inline">\(\mathcal{A}\)</span> is any <strong>closed</strong> subspace of a Hilbert space <span class="math inline">\(\mathcal{H}\)</span>. Then <span class="math inline">\(\mathcal{H}=\mathcal{A}\oplus\mathcal{A}^{\bot}\)</span>. The proof is given below.</span></p>
<div class="solution">
<p class="solution-begin">
Proof <span id="sol-start-104" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-104', 'sol-start-104')"></span>
</p>
<div id="sol-body-104" class="solution-body" style="display: none;">
<p>Let <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}:\mathcal{H}\rightarrow \mathcal{Y}\)</span> be the <a href="ch-representation.html#sub:conjugacy">projection operator</a> defined on the Hilbert space <span class="math inline">\(\mathcal{H}\)</span>, such that <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}\mathbf{P}_{\mathcal{Y}}=\mathbf{P}_{\mathcal{Y}}\)</span>.</p>
<p>To show that <span class="math inline">\(\mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span> and <span class="math inline">\(\mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>, are <strong>disjoint subspaces</strong>, let <span class="math inline">\(f\in \mbox{Null}(\mathbf{P}_{\mathcal{Y}}) \cap \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>. Because <span class="math inline">\(f\in \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span> then there must be some <span class="math inline">\(g \in \mathcal{H}\)</span> such that <span class="math inline">\(\mathbf{P}_{\mathcal{Y}} (g) =f\)</span>. By the projection property, we know that <span class="math display">\[\mathbf{P}_{\mathcal{Y}}\mathbf{P}_{\mathcal{Y}} (g) = \mathbf{P}_{\mathcal{Y}} f =f.\]</span> Also, because <span class="math inline">\(f\in \mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span>, <span class="math inline">\(\mathbf{P}_{\mathcal{Y}} f=0\)</span>. That tells <span class="math inline">\(f=\mathbf{P}_{\mathcal{Y}} f=0\)</span>. Since <span class="math inline">\(f\)</span> is any object in <span class="math inline">\(\mbox{Null}(\mathbf{P}_{\mathcal{Y}}) \cap \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>, we have <span class="math inline">\(\mbox{Null}(\mathbf{P}_{\mathcal{Y}}) \cap \mbox{Range}(\mathbf{P}_{\mathcal{Y}})=\{0\}\)</span>.</p>
<p>Now consider the orthogonal projection.
Firstly, note that <span class="math inline">\(\mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span> and <span class="math inline">\(\mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span> are <strong>null spaces</strong> of <span class="math inline">\(\mathbf{I}-\mathbf{P}_{\mathcal{Y}}\)</span> and <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}\)</span> where <span class="math inline">\(\mathbf{I}\)</span> is an identical operator/matrix. That says
<span class="math display">\[\langle \mathbf{P}_{\mathcal{Y}}, \mathbf{I}-\mathbf{P}_{\mathcal{Y}}\rangle =0.\]</span> Thus<br><span class="math display">\[\mbox{Range}(\mathbf{P}_{\mathcal{Y}}) = \mbox{Null}(\mathbf{I}-\mathbf{P}_{\mathcal{Y}}) = \mbox{Null}(\mathbf{P}_{\mathcal{Y}})^{\bot}.\]</span>
Secondly, note that any <span class="math inline">\(f\in\mathcal{H}\)</span> can be uniquely decompose as <span class="math display">\[f=f_1 + f_2\]</span> for <span class="math inline">\(f_1\in \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>, <span class="math inline">\(f_2 \in \mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span>. Since <span class="math inline">\(\mathbf{P}_{\mathcal{Y}}\)</span> is orthogonal, we have <span class="math inline">\(\langle f_1, f_2\rangle =0\)</span> for any <span class="math inline">\(f_1\in \mbox{Range}(\mathbf{P}_{\mathcal{Y}})\)</span>, <span class="math inline">\(f_2 \in \mbox{Null}(\mathbf{P}_{\mathcal{Y}})\)</span>. By Pythagorean theorem, we know that <span class="math inline">\(\|f\|^2 = \|f_1\|^2 +\|f_2\|^2\)</span>. The result follows.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Given the identical and independent property of the white noises, one should expect that the spaces of white noises share some invariant properties.</p>
<p>Recall that in the deterministic setting, stationarity refers to some equilibrium status, in which the dynamics remain invariant. The dynamics of the randomized models should be described by the probability laws. For <a href="ch-UnMulti.html#sub:Markov">Markov processes</a>, we have seen the invariance probability law is about the unchange of <a href="ch-UnMulti.html#sub:Markov">transition probabilities</a>. While for the <a href="ch-randomization.html#sub:RHilbert">second-order processes</a>, since the probability law is reduced to the first- and the second-order information, we can have define a new kind of invariance in a wider sense, called the <strong>weak (or wide sense) stationarity</strong>.</p>
<ul>
<li>
<strong>Weak (or wide sense) stationarity</strong> : A <a href="ch-randomization.html#sub:RHilbert">second-order process</a> <span class="math inline">\(X(t,\omega)\)</span> is <em>weakly stationary</em> if its <a href="ch-randomization.html#sub:RHilbert">covariance function</a> satisfies <span class="math display">\[\mbox{Cov}_X (s+t,s)= \mbox{Cov}_X (t,0),\]</span> for any <span class="math inline">\(s,t\in\mathbb{R}\)</span>, and its <a href="ch-randomization.html#sub:RHilbert">mean function</a> <span class="math inline">\(\mu_X(t) = \mu_X\)</span> where <span class="math inline">\(\mu_X\)</span> is a constant independent of time.</li>
</ul>
<p>The above defintion of the <a href="ch-randomization.html#sub:RHilbert">covariance function</a> says that only the difference between function’s two arguments matters, namely <span class="math display">\[\mbox{Cov}_X (t_1,t_2)= \mbox{Cov}_X (t_1 - t_2).\]</span></p>
<p>Another way to understand the definition is to see that, for a <strong>weakly stationary</strong> process, if one shifts the process by <span class="math inline">\(s\)</span>-unit of time, the first- and the second-order structures of the process remain invariant.
Recall that shifting the process backward is conducted by the <a href="ch-eigen.html#sub:matNorms">lagged operator</a> <span class="math inline">\(\mathbf{L}\)</span>. For a <strong>weak stationary</strong> time series <span class="math inline">\(X(\omega)_{t}\)</span>, we have the following equalities in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a><label for="tufte-sn-451" class="margin-toggle sidenote-number">451</label><input type="checkbox" id="tufte-sn-451" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">451</span> Note that the covariance function can be written in terms of an inner product in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a> <span class="math display">\[\begin{align*}\mathbb{E}[(X_{t}(\omega)-\mu_{X})(X_{t-1}(\omega)-\mu_{X})]\\=\langle X_{t}(\omega)-\mu_{X},X_{t-1}(\omega)-\mu_{X}\rangle_{\mathbb{P}}.\end{align*}\]</span>
For the <a href="ch-eigen.html#sub:matNorms">lagged operator</a>, <span class="math display">\[\mathbf{L}(X_{t}(\omega) - \mu_{X})=X_{t-1}(\omega)-\mu_X\]</span> because <span class="math inline">\(\mu_X\)</span> is a constant independent of time.</span>:
<span class="math display">\[\begin{align*}\mbox{Cov}_X (t,t-1) &amp;= \langle X_{t}(\omega) - \mu_{X},X_{t-1}(\omega)-\mu_{X}\rangle_{\mathbb{P}} \\
&amp;=\langle X_{t}(\omega) - \mu_{X},\mathbf{L}(X_{t}(\omega) - \mu_{X})\rangle_{\mathbb{P}}\\
&amp;=\langle\mathbf{L}^{\mbox{H}}(X_{t}(\omega)-\mu_X),X_{t}(\omega)-\mu_X\rangle_{\mathbb{P}}\\
&amp;=\langle X_{t+1}(\omega)-\mu_X,X_{t}(\omega)-\mu_{X}\rangle_{\mathbb{P}}\\
&amp;= \mbox{Cov}_X (t+1,t) \end{align*}\]</span>
where <span class="math inline">\(\mathbf{L}^{\mbox{H}}\)</span> is the <a href="ch-representation.html#sub:conjugacy">adjoint operator</a> of <span class="math inline">\(\mathbf{L}\)</span>, meaning a forward shift.</p>
<p>In other words, the <strong>weak stationarity</strong> is about endowing the lagged operator <span class="math inline">\(\mathbf{L}\)</span> the <a href="ch-representation.html#sub:dualBasis">unitary</a> property, as the operators perserve the value of the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathbb{P}\)</span>-inner product</a> (called <em>isometry</em>)<label for="tufte-sn-452" class="margin-toggle sidenote-number">452</label><input type="checkbox" id="tufte-sn-452" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">452</span> By the equality
<span class="math display">\[\mbox{Cov}_X (t,s)= \mathbb{E}[X_t(\omega)X_s(\omega)] -\mu_{X}^2,\]</span> we know that only the term <span class="math display">\[\mathbb{E}[X_t(\omega)X_s(\omega)]=\langle X_{t}(\omega),X_{s}(\omega)\rangle_{\mathbb{P}}\]</span> influences the covariance function. So we just need to consider <span class="math display">\[\langle X_{t}(\omega),X_{t-1}(\omega)\rangle_{\mathbb{P}}=\langle X_{t+1}(\omega),X_{t}(\omega)\rangle_{\mathbb{P}}.\]</span></span>
<span class="math display">\[\begin{align*}\langle\mathbf{L}X_{t+1}(\omega),\mathbf{L}X_{t}(\omega)\rangle_{\mathbb{P}}&amp;=\langle X_{t}(\omega),X_{t-1}(\omega)\rangle_{\mathbb{P}}=\\\langle X_{t+1}(\omega),\mathbf{L}^{\mbox{H}}\mathbf{L}X_{t}(\omega)\rangle_{\mathbb{P}}&amp;=\langle X_{t+1}(\omega),X_{t}(\omega)\rangle_{\mathbb{P}},\end{align*}\]</span>
and <span class="math inline">\(\mathbf{L}^{\mbox{H}}\mathbf{L}=\mathbf{I}\)</span> or say <span class="math inline">\(\mathbf{L}^{\mbox{H}}=\mathbf{L}^{-1}\)</span>.</p>
<p>For an <a href="ch-MatComp.html#sub:vecSpaces">operator</a> <span class="math inline">\(\mathbf{T}\)</span>, we say that the operator induces an <em>invariant subspace</em> <span class="math inline">\(\mathcal{V}\)</span>, if <span class="math inline">\(\mathbf{T}(\mathcal{V})\subset\mathcal{V}\)</span>. Therefore, the <strong>weak stationarity</strong> is to empower the <a href="ch-eigen.html#sub:matNorms">lagged operator</a> <span class="math inline">\(\mathbf{L}\)</span> and its <a href="ch-representation.html#sub:conjugacy">adjoint</a> <span class="math inline">\(\mathbf{L}^{\mbox{H}}\)</span> to induce the <strong>invariant subspaces</strong> of some <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a> generated by the process <span class="math inline">\(X(t,\omega)\)</span>:<label for="tufte-sn-453" class="margin-toggle sidenote-number">453</label><input type="checkbox" id="tufte-sn-453" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">453</span> The Hilbert space <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span> generated by a <strong>weak stationary</strong> stochastic process <span class="math inline">\(X(t,\omega)\)</span> is to be generated by the <a href="ch-representation.html#sub:dualBasis">unitary</a> lagged operator <span class="math inline">\(\mathbf{L}\)</span> in the following sense: <span class="math display">\[\mathcal{H}_{X(\omega)}=\overline{\mbox{span}}\{\mathbf{L}^{t}X(t,\omega)| t\in\mathbb{Z}\}.\]</span></span>
<span class="math display">\[\mathbf{L}\mathcal{H}_{X(\omega)}^{t-}=\mathcal{H}_{X(\omega)}^{t-},\;\mathbf{L}^{\mbox{H}}\mathcal{H}_{X(\omega)}^{t+}=\mathcal{H}_{X(\omega)}^{t+},\]</span>
where <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t-}\)</span> and <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t+}\)</span> represent the past and the future respectively, and <span class="math inline">\(\mathcal{H}_{X(\omega)}= \mathcal{H}_{X(\omega)}^{t-}+\mathcal{H}_{X(\omega)}^{t+}\)</span>.</p>
<p>With the concepts of <strong>direct sum</strong> (<strong>decomposition</strong>) and <strong>weak stationarity</strong>, we can analyze the <a href="ch-UnMulti.html#sub:rw">random walk</a> model <span class="math inline">\(X_{t}(\omega)=\sum_{i=1}^{t}\varepsilon_{i}(\omega)\)</span> from a different perspective. Let <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t}\)</span> and <span class="math inline">\(\mathcal{H}_{\varepsilon(\omega)}^{t}\)</span> be the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-spaces</a> generated by the <span class="math inline">\(X_t(\omega)\)</span> and <span class="math inline">\(\varepsilon_t(\omega)\)</span> at time <span class="math inline">\(t\)</span>. Because Gaussian white noises <span class="math inline">\(\{\varepsilon_{i}(\omega)\}_{i=1}^{t}\)</span>
are from <em>disjoint orthogonal subspaces</em> <span class="math inline">\(\mathcal{H}^{t}_{\varepsilon(\omega)},\dots,\mathcal{H}^{1}_{\varepsilon(\omega)}\)</span>, the <strong>direct sum</strong> <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t}\)</span> can be <strong>orthogonally decomposed</strong> as
<span class="math display">\[\begin{align*}\mathcal{H}_{X(\omega)}^{t}&amp;=\mathcal{H}^{t}_{\varepsilon(\omega)}\oplus\mathcal{H}^{t-1}_{\varepsilon(\omega)}\cdots\oplus\mathcal{H}^{1}_{\varepsilon(\omega)} = \oplus_{s=1}^{t}\mathcal{H}^{s}_{\varepsilon(\omega)} \\&amp;=
\mathcal{H}^{t}_{\varepsilon(\omega)}\oplus\mathbf{L}\mathcal{H}_{X(\omega)}^{t}.\end{align*}\]</span></p>
<p>The subspace <span class="math inline">\(\mathcal{H}^{t}_{\varepsilon(\omega)}\)</span>, as the <strong>orthogonal complement</strong> of lagged <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t}\)</span>, provides the gap information between <span class="math inline">\(\mathcal{H}_{X(\omega)}^{t}\)</span> and its one step backward shifted <span class="math inline">\(\mathbf{L}\mathcal{H}_{X(\omega)}^{t}\)</span>. Such a subspace is called the <em>wandering subspace</em> if the shift/lagged operator induces <strong>invariant subspaces</strong>.</p>
<p>In short, the space the <strong>weakly stationary</strong>
random walks is decomposed of the <strong>orthogonal direct sums</strong> of <strong>wandering subspaces</strong> generated by the white noises.<label for="tufte-sn-454" class="margin-toggle sidenote-number">454</label><input type="checkbox" id="tufte-sn-454" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">454</span> The <a href="ch-UnMulti.html#sub:rw">random walk</a> is <strong>weakly stationary</strong>, as the covariance function <span class="math inline">\(\mbox{Cov}_X(t,s)=\sigma^2(t-s)\)</span> only depend on the difference of its arguments.</span>
Thus, we can view the space of random walks as the <strong>direct sum</strong> of many subspaces generated by the white noises. The sequence <span class="math inline">\(\{\varepsilon_{i}\}_{i=1}^{t}\)</span> is called the <em>white noise basis</em> of the <a href="ch-UnMulti.html#sub:rw">random walk</a> series.</p>
<p>This kind of the <strong>decomposition</strong> turns out to be valid for all <strong>weakly stationary processes</strong> with zero means in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a>, even if the space consists of infinitely many <strong>wandering subspaces</strong>.<label for="tufte-sn-455" class="margin-toggle sidenote-number">455</label><input type="checkbox" id="tufte-sn-455" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">455</span> The result is called the <em>Wold’s decomposition</em>. However, additional restrictions have to be imposed on the remote past and future subspaces of the processes. The remote past and future subspaces have to be trivial, namely <span class="math display">\[\lim_{t\rightarrow\pm\infty}\mathcal{H}_{X(\omega)}^{t}=0,\]</span> so that they provide zero information to all the other subspaces of the process. The <strong>Wold decomposition</strong> basically says that a <a href="ch-representation.html#sub:dualBasis">unitary operator</a> can induce a sequence of <strong>subspaces</strong> whose <strong>orthogonal direct sum</strong> is invariant under the operator.</span></p>
<p>By the definition of the <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> in chapter <a href="ch-MatComp.html#sub:vecSpaces">11.4</a>, we know that if some objects live in a subspace, any linear combination of these objects also belongs to this subspace. The collection of <strong>wandering subspaces</strong> induced by the <strong>white noise basis</strong> can be infinite dimensional. This motivates us to represent some infinite dimensional elements through a linear combination of infinite dimensional white noises basis (a <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a> of white noise process) in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-spaces</a>.<label for="tufte-sn-456" class="margin-toggle sidenote-number">456</label><input type="checkbox" id="tufte-sn-456" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">456</span> The previous discussion says that the <strong>null space</strong> of a <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a> can be the “maximal” subspace. One may expect that the representation is good enough to approximate almost all <strong>second-order</strong> processes. This is roughly the case, except that the <strong>mean function</strong> (the “missing” one dimension) has to be removed from the approximation. So we only consider zero mean processes over here.</span> Let <span class="math inline">\(\varepsilon(t,\omega)\)</span> be a <a href="ch-randomization.html#sub:RHilbert">Gaussian white noise process</a>. The linear functional <span class="math inline">\(f(\omega)\in\mathcal{H}_{f(\omega)}\)</span> of the <a href="ch-randomization.html#sub:RHilbert">white noise process</a> has the form
<span class="math display">\[f(\omega)=\sum_{s=-\infty}^{\infty}c(-s)\varepsilon(s,\omega),\;\mbox{where }\sum_{s=-\infty}^{\infty}|\phi(-s)|^{2}&lt;\infty\]</span>
where the deterministic function <span class="math inline">\(c(-s)\)</span> is in the <a href="sub-continuity.html#sub:continuousFunc"><span class="math inline">\(\ell_2\)</span>-space</a>, and plays a role as the <span class="math inline">\(s\)</span>-th <a href="ch-representation.html#sub:conjugacy">Fourier coefficient</a> of the functional <span class="math inline">\(f\)</span> with respect to the <strong>white noise basis</strong> <span class="math inline">\(\varepsilon(s,\omega)\)</span> such that <span class="math display">\[c(-t)=\left\langle f(\omega),\varepsilon(t,\omega)\right\rangle _{\mathbb{P}}=\mathbb{E}\left[f(\omega)\varepsilon(t,\omega)\right].\]</span>
As you can see, the function <span class="math inline">\(c(\cdot)\)</span> is analogous to the coefficient in the <a href="ch-representation.html#sub:conjugacy">abstract Fourier series</a> in chapter <a href="ch-representation.html#sub:conjugacy">14.3</a>. Therefore, we expect that these coefficient functions are unique and form a square summable sequence.<label for="tufte-sn-457" class="margin-toggle sidenote-number">457</label><input type="checkbox" id="tufte-sn-457" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">457</span> It is possible to derive the continuous version of this representation. But due to the difficulty of defining “proper” mean square integral of white noises, it is better to construct the functionals in terms of <a href="ch-randomization.html#sub:RHilbert">mean square differential</a> of a Brownian motion <span class="math inline">\(B(t,\omega)\)</span>, i.e., <span class="math display">\[f(\omega)=\int_{-\infty}^{\infty}c(-s)\mbox{d}B(t,\omega)\]</span>
where <span class="math inline">\(\mbox{d}B(t,\omega)=\varepsilon(t,\omega)\mbox{d}t\)</span> according to the previous definition of the white nose.</span></p>
<p>Recall that given <span class="math inline">\(\omega\)</span>, a stochastic process <span class="math inline">\(X(t,\omega)\)</span> is a function (path) over time <span class="math inline">\(t\)</span>.<label for="tufte-sn-458" class="margin-toggle sidenote-number">458</label><input type="checkbox" id="tufte-sn-458" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">458</span> Given <span class="math inline">\(\omega\)</span>, <span class="math inline">\(X(\cdot, \omega):\mathcal{V}\rightarrow\mathbb{F}\)</span> is a <a href="ch-MatComp.html#sub:vecSpaces">functional</a>.</span> It would be natural think about using the previous decomposition for representing a stochastic processes. Because all white noises have zero means, <span class="math inline">\(\mathbb{E}[\varepsilon(t,\omega)]=0\)</span>, the inner product of <span class="math inline">\(X(t,\omega)\)</span> and <span class="math inline">\(\varepsilon(t,\omega)\)</span> actually gives the <a href="ch-randomization.html#sub:RHilbert">covariance function</a>.<label for="tufte-sn-459" class="margin-toggle sidenote-number">459</label><input type="checkbox" id="tufte-sn-459" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">459</span> Note that
<span class="math display">\[\begin{align*}\mbox{Cov}_{X,\varepsilon}(t,s)=&amp;\\\mathbb{E}\left[X(t,\omega)\varepsilon(s,\omega)\right]-&amp;\mathbb{E}[X(t,\omega)]\mathbb{E}[\varepsilon(s,\omega)]\\=&amp;\left\langle X(t,\omega),\varepsilon(s,\omega)\right\rangle _{\mathbb{P}}.\end{align*}\]</span></span>
So we have the following representation <span class="math display">\[X(t,\omega)=\sum_{s=-\infty}^{\infty}\mbox{Cov}_{X,\varepsilon}(t,s)\varepsilon(s,\omega).\]</span>
In particular, if the <a href="ch-randomization.html#sub:RHilbert">second order process</a> <span class="math inline">\(X(t,\omega)\)</span> is <strong>weakly stationary</strong>,
the <a href="ch-representation.html#sub:dualBasis">duality</a> between coefficient function <span class="math inline">\(c(s)=\mbox{Cov}_{X,\omega}(t,s)\)</span> and the <strong>white noise basis</strong> becomes clear. Because when we shift the all the white noises by <span class="math inline">\(\tau\)</span>-period backward, e.g., <span class="math inline">\(\varepsilon(s-\tau,\omega)=\mathbf{L}^{\tau}\varepsilon(s,\omega)\)</span>, the corresponding coefficient needs to be shifted <span class="math inline">\(\tau\)</span>-period forward:
<span class="math display">\[\begin{align*}
\mbox{Cov}_{X,\omega}(t,s-\tau)&amp;=\left\langle X(t,\omega),\varepsilon(s-\tau)\right\rangle _{\mathbb{P}}=\left\langle X(t,\omega),\mathbf{L}^{\tau}\varepsilon(s)\right\rangle _{\mathbb{P}}\\
&amp;=\left\langle \mathbf{L}^{-\tau}X(t,\omega),\varepsilon(s)\right\rangle _{\mathbb{P}}=\mbox{Cov}_{X,\omega}(t+\tau,s)\\
&amp;=\mathbf{L}^{-\tau}c(s).
\end{align*}\]</span>
Then the representation is simplified to<label for="tufte-sn-460" class="margin-toggle sidenote-number">460</label><input type="checkbox" id="tufte-sn-460" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">460</span> This is the exact formula for the <strong>Wold decomposition</strong> of the <strong>weakly stationary</strong> process <span class="math inline">\(X(t,\omega)\)</span>.</span><br><span class="math display">\[X(t,\omega)=\sum_{s=-\infty}^{\infty}\mbox{Cov}_{X,\varepsilon}(t-s)\varepsilon(s,\omega)\]</span> where the <a href="ch-randomization.html#sub:RHilbert">covariance function</a> only has one argument.</p>
</div>
<div id="sub:Kalman" class="section level2">
<h2>
<span class="header-section-number">16.3</span> Miscellaneous: Sample Average, Reduction Principle, and Kalman’s Filter</h2>
<p>For the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathcal{H}\)</span>-valued random variables</a> <span class="math inline">\(X(\omega)\)</span> and <span class="math inline">\(Y(\omega)\)</span> in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a>, the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathbb{P}\)</span>-inner product</a> <span class="math inline">\(\mathbb{E}[X(\omega)Y(\omega)]\)</span> implies the <em><span class="math inline">\(\mathbb{P}\)</span>-norm</em> <span class="math inline">\(\|X(\omega)\|_{\mathbb{P}}=\sqrt{\mathbb{E}[X(\omega)]^{2}}\)</span>, and the <em>mean square distance</em><label for="tufte-sn-461" class="margin-toggle sidenote-number">461</label><input type="checkbox" id="tufte-sn-461" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">461</span> In particular, if both <span class="math inline">\(X(\omega)\)</span> and <span class="math inline">\(Y(\omega)\)</span> are mean zero random variables, the distance is simply the covariance between <span class="math inline">\(X(\omega)\)</span> and <span class="math inline">\(Y(\omega)\)</span>.</span>
<span class="math display">\[\begin{align*}\mbox{d}_{\mathbb{P}}(X(\omega),Y(\omega))&amp;=\langle X(\omega)-Y(\omega), X(\omega)-Y(\omega) \rangle_{\mathbb{P}}\\
&amp;=\mathbb{E}[X(\omega)-Y(\omega)]^{2}=\|X(\omega)-Y(\omega)\|^{2}_{\mathbb{P}}.\end{align*}\]</span>
With these definitions, we can derive the <a href="ch-optApp.html#sub:appSys">metric projection operator</a> in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathcal{H})\)</span>-space</a>, which is nothing else but the <a href="ch-CalUn.html#sub:ex">expectation operator</a>.</p>
<p>Let’s illustrate the result by setting <span class="math inline">\(\mathcal{H}=\mathbb{R}\)</span>.
According to the definition of the <a href="ch-optApp.html#sub:appSys">metric projection</a>, the projection will map <span class="math inline">\(X(\omega)\in\mathbb{R}\)</span> to a single value in <span class="math inline">\(\mathbb{R}\)</span> that is the deterministic <a href="ch-optApp.html#sub:appSys">optimal approximation</a> for the random variable <span class="math inline">\(X(\omega)\)</span>. Thus, the <a href="ch-optApp.html#sub:appSys">hat operator</a> <span class="math inline">\(\hat{x}\)</span> will minimize the <strong>mean square distance</strong> such that<label for="tufte-sn-462" class="margin-toggle sidenote-number">462</label><input type="checkbox" id="tufte-sn-462" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">462</span> The <a href="ch-optApp.html#sub:appSys">hat operator</a> <span class="math inline">\(\hat{(\cdot)}:\mathcal{H}_{X(\omega)}\rightarrow \mathbb{R}\)</span> maps from the <a href="ch-randomization.html#sub:RHilbert">randomized Hilbert space</a> generated by <span class="math inline">\(X(\omega)\)</span> to <span class="math inline">\(\mathbb{R}\)</span>. So <span class="math inline">\(\hat{x}\)</span> is a <a href="ch-MatComp.html#sub:vecSpaces">linear functional</a> in <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span>.</span>
<span class="math display">\[\begin{align*}\min_{c\in\mathbb{R}}\mbox{d}_{\mathbb{P}}(X,c)&amp;=\min_{c\in\mathbb{R}}\mathbb{E}[X(\omega)-c]^{2}=\mathbb{E}[X(\omega)-\hat{x}]^{2}.\end{align*}\]</span>
Let <span class="math inline">\(\mu\)</span> be the mean value of the random variable <span class="math inline">\(X(\omega)\)</span>. Note that
<span class="math display">\[\begin{align*}&amp;\mathbb{E}[(X(\omega)-c)]^{2}=\mathbb{E}[(X(\omega)-\mu+\mu-c)]^{2}\\
=&amp;\mathbb{E}\left[(X(\omega)-\mu)^{2}+(\mu-c)^{2}+2(X(\omega)-\mu)(\mu-c)\right]\\
=&amp;  \underset{=\mbox{Var}(X(\omega))}{\underbrace{\mathbb{E}[(X(\omega)-\mu)^{2}]}}+(\mu-c)^{2}+2(\mu-c)\underset{=0}{\underbrace{\mathbb{E}[(X(\omega)-\mu)]}}.
\end{align*}\]</span>
The ﬁrst term is the variance, so its value is the same regardless of the choice of <span class="math inline">\(c\)</span>. The minimization only depends on the second term that is clearly minimized by <span class="math inline">\(\hat{x}=\mu=\mathbb{E}[X(\omega)]\)</span>.<label for="tufte-sn-463" class="margin-toggle sidenote-number">463</label><input type="checkbox" id="tufte-sn-463" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">463</span> In other words, we can also think the <a href="ch-MatComp.html#sub:vecSpaces">linear operator</a> <span class="math inline">\(\mathbb{E}[\cdot]\)</span> of a random variable as the <a href="ch-optApp.html#sub:appSys">metric projector</a> in <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F},\mathbb{P};\mathbb{R})\)</span></a>:
<span class="math display">\[\mathbf{P}_{\mathcal{\mathbb{R}}}(X(\omega))=\mathbb{E}[X(\omega)]=\int_{\mathcal{\mathbb{R}}}x\mathbb{P}(\mbox{d}x).\]</span></span></p>
<p><span class="newthought">Sample average </span></p>
<p>Expectation is about the probability integration. But the probability is often hard to attain in practice. In this case, we may turn to some coarse techniques, such as the sample average in statistics.</p>
<p>Let’s consider a sample vector <span class="math inline">\(\mathbf{x}\in\mathbb{R}^{n}\)</span> of <span class="math inline">\(n\)</span> observations. A coarse statistics can be a simple value in <span class="math inline">\(\mathbb{R}\)</span> like a constant vector in <span class="math inline">\(\mathbb{R}^{n}\)</span>. So the <a href="ch-optApp.html#sub:appSys">projection</a> is to project <span class="math inline">\(\mathbf{x}\)</span> onto a <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> spanned by a constant vector, say <span class="math inline">\(\mathbf{1}=[1,\dots,1]^{\top}\in\mathbb{R}^{n}\)</span>. The form of this <a href="ch-representation.html#sub:conjugacy">projection matrix</a> is given by<label for="tufte-sn-464" class="margin-toggle sidenote-number">464</label><input type="checkbox" id="tufte-sn-464" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">464</span> The matrix is a <a href="ch-optApp.html#sub:appSys">projection</a> because <span class="math display">\[\begin{align*}&amp;\mathbf{P}_{\mbox{span}(\mathbf{1})}\mathbf{P}_{\mbox{span}(\mathbf{1})}\\&amp;=\mathbf{1}(\mathbf{1}^{\top}\mathbf{1})^{-1}\mathbf{1}^{\top}\mathbf{1}(\mathbf{1}^{\top}\mathbf{1})^{-1}\mathbf{1}^{\top}\\&amp;= \mathbf{1}(\mathbf{1}^{\top}\mathbf{1})^{-1}\mathbf{1}^{\top}=\mathbf{P}_{\mbox{span}(\mathbf{1})}.\end{align*}\]</span> Also, the projection is <a href="ch-optApp.html#sub:Optimization">orthogonal</a> because <span class="math inline">\(\mathbf{P}_{\mbox{span}(\mathbf{1})}^{\top}=\mathbf{P}_{\mbox{span}(\mathbf{1})}\)</span>.</span>
<span class="math display">\[\begin{align*}\mathbf{P}_{\mbox{span}(\mathbf{1})}=\mathbf{1}(\mathbf{1}^{\top}\mathbf{1})^{-1}\mathbf{1}^{\top}=\left[\begin{array}{ccc}
\frac{1}{n} &amp; \cdots &amp; \frac{1}{n}\\
\vdots &amp; \ddots &amp; \vdots\\
\frac{1}{n} &amp; \cdots &amp; \frac{1}{n}
\end{array}\right].\end{align*}\]</span>
For any sample vector <span class="math inline">\(\mathbf{x}\in\mathbb{R}^{n}\)</span>, the difference between the vector and its projected vector is a residual vector <span class="math display">\[\left(\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})}\right)\mathbf{x}=\mathbf{x}-\hat{\mathbf{x}}=\left[\begin{array}{c}
x_{1}-\hat{x}\\
\vdots\\
x_{n}-\hat{x}
\end{array}\right],\]</span>
where the <a href="ch-optApp.html#sub:appSys">hat operator</a> <span class="math inline">\(\hat{(\cdot)}:\mathbb{R}^n\rightarrow\mbox{span}(\mathbf{1})\)</span>, <span class="math inline">\(\hat{\mathbf{x}}=\hat{x}\mathbf{1}\)</span>, is a constant vector of the statistical <a href="ch-CalUn.html#sub:ex">mean</a> <span class="math inline">\(\hat{x}=\sum_{i=1}^{n}x_i/n\)</span>, namely the statistical counterpart of <a href="ch-CalUn.html#sub:ex">expectation</a>. The <a href="ch-vecMat.html#sub:vec">norm</a> of this residual vector measures the distance between the sample vector and its projection<label for="tufte-sn-465" class="margin-toggle sidenote-number">465</label><input type="checkbox" id="tufte-sn-465" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">465</span> The residual of this projection is stored in the <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> <span class="math inline">\((\mbox{span}(\mathbf{1}))^{\bot}\)</span> where <span class="math inline">\((\mbox{span}(\mathbf{1}))\oplus(\mbox{span}(\mathbf{1}))^{\bot}=\mathbb{R}^{n}\)</span>. The explicit form of <span class="math inline">\(\mathbf{P}_{\mbox{span}(\mathbf{1})^{\bot}}=\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})}\)</span> is
<span class="math display">\[\begin{align*}&amp;\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})}\\&amp;=\left[\begin{array}{ccc}
1-\frac{1}{n} &amp; \cdots &amp; -\frac{1}{n}\\
\vdots &amp; \ddots &amp; \vdots\\
-\frac{1}{n} &amp; \cdots &amp; 1-\frac{1}{n}
\end{array}\right].\end{align*}\]</span>
It is another projection onto the <a href="ch-randomization.html#sub:SumDecomp">orthogonal complement</a> subspace of <span class="math inline">\(\mbox{span}(\mathbf{1})\)</span> such that
<span class="math display">\[\begin{align*}&amp;(\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})})^{2}=\mathbf{I}^{2}+(\mathbf{P}_{\mbox{span}(\mathbf{1})})^{2}\\&amp;-2\mathbf{I}\times\mathbf{P}_{\mbox{span}(\mathbf{1})}=\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})},\end{align*}\]</span>
and <span class="math inline">\((\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})})\mathbf{P}_{\mbox{span}(\mathbf{1})}\\=\mathbf{P}_{\mbox{span}(\mathbf{1})}-\mathbf{P}_{\mbox{span}(\mathbf{1})}^{2}=0.\)</span></span> <span class="math display">\[\begin{align*}\|\mathbf{x}-\hat{\mathbf{x}}\|^{2}&amp;=\left\|\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})}\mathbf{x}\right\|^2\\
&amp;=\mathbf{x}^{\top}(\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})})(\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})})\mathbf{x}\\
&amp;=\mathbf{x}^{\top}(\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})})\mathbf{x}.\end{align*}\]</span>
By the properties of the <a href="ch-optApp.html#sub:appSys">orthogonal metric projection</a> (see section <a href="ch-optApp.html#sub:Optimization">15.2</a>), we know that <span class="math inline">\(\mathbf{P}_{\mbox{span}(\mathbf{1})}\)</span> minimizes the distance while <span class="math inline">\(\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})}\)</span> orthogonalizes the sample vector to the vector in the <a href="ch-randomization.html#sub:SumDecomp">complement subspace</a> (the residual) of the projection:<label for="tufte-sn-466" class="margin-toggle sidenote-number">466</label><input type="checkbox" id="tufte-sn-466" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">466</span> Here is a general result regarding the orthogonality. Let <span class="math inline">\(\mathcal{Y}\)</span> be a <a href="ch-randomization.html#sub:SumDecomp">closed subspace</a> of the <a href="ch-representation.html#sub:conjugacy">Hilbert space</a> <span class="math inline">\(\mathcal{H}\)</span>. Given <span class="math inline">\(f\in\mathcal{H}\)</span>, the solution <span class="math inline">\(\hat{f}\)</span> of the problem <span class="math display">\[\min_{g\in\mathcal{Y}}\|f-g\|\]</span> is unique; and <span class="math inline">\(\hat{f}\)</span> is the orthogonal projection of <span class="math inline">\(f\)</span> onto <span class="math inline">\(\mathcal{Y}\)</span>, namely <span class="math display">\[\langle f-\hat{f},g\rangle=0,\]</span>
for any <span class="math inline">\(g\in\mathcal{Y}\)</span>.</span>
<span class="math display">\[\begin{align*}&amp;\min_{\mathbf{c}\in\mbox{span}(\mathbf{1})}\|\mathbf{x}-\mathbf{c}\|^{2}\\&amp;=\begin{cases}
\mbox{minimum distance}: &amp; \|\mathbf{x}-\mathbf{P}_{\mbox{span}(\mathbf{1})}\mathbf{x}\|^{2},\\
\mbox{orthogonality}: &amp; \left\langle(\mathbf{I}-\mathbf{P}_{\mbox{span}(\mathbf{1})})\mathbf{x}, \mathbf{c}\right\rangle=0.
\end{cases}\end{align*}\]</span></p>
<div class="solution">
<p class="solution-begin">
Numerical illustration <span id="sol-start-105" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-105', 'sol-start-105')"></span>
</p>
<div id="sol-body-105" class="solution-body" style="display: none;">
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb125-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb125-2" data-line-number="2"></a>
<a class="sourceLine" id="cb125-3" data-line-number="3">n =<span class="st"> </span><span class="dv">5</span>; x=<span class="kw">rnorm</span>(n,<span class="dv">3</span>); one=<span class="kw">rep</span>(<span class="dv">1</span>,n); </a>
<a class="sourceLine" id="cb125-4" data-line-number="4"><span class="co"># projection matrix P</span></a>
<a class="sourceLine" id="cb125-5" data-line-number="5">proj =one <span class="op">%*%</span><span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(one) <span class="op">%*%</span><span class="st"> </span>one) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(one);</a>
<a class="sourceLine" id="cb125-6" data-line-number="6"><span class="co"># I - P</span></a>
<a class="sourceLine" id="cb125-7" data-line-number="7">comp.proj =<span class="st"> </span><span class="kw">diag</span>(n)<span class="op">-</span>proj; </a>
<a class="sourceLine" id="cb125-8" data-line-number="8"></a>
<a class="sourceLine" id="cb125-9" data-line-number="9"><span class="kw">solve</span>(<span class="kw">t</span>(one) <span class="op">%*%</span><span class="st"> </span>one) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(one)<span class="op">%*%</span>x</a></code></pre></div>
<pre><code>##          [,1]
## [1,] 3.407266</code></pre>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb127-1" data-line-number="1"><span class="kw">mean</span>(x)</a></code></pre></div>
<pre><code>## [1] 3.407266</code></pre>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb129-1" data-line-number="1"><span class="co"># minimum vector norm</span></a>
<a class="sourceLine" id="cb129-2" data-line-number="2"><span class="kw">t</span>(comp.proj<span class="op">%*%</span>x) <span class="op">%*%</span><span class="st"> </span>(comp.proj<span class="op">%*%</span>x)</a></code></pre></div>
<pre><code>##           [,1]
## [1,] 0.5482674</code></pre>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb131-1" data-line-number="1">normFn =<span class="st"> </span><span class="cf">function</span>(c) {   <span class="co">## norm function</span></a>
<a class="sourceLine" id="cb131-2" data-line-number="2">      (x <span class="op">-</span><span class="st"> </span>c <span class="op">%*%</span>one)<span class="op">%*%</span><span class="kw">t</span>(x <span class="op">-</span><span class="st"> </span>c<span class="op">%*%</span>one)</a>
<a class="sourceLine" id="cb131-3" data-line-number="3">}</a>
<a class="sourceLine" id="cb131-4" data-line-number="4"><span class="kw">optim</span>(<span class="kw">c</span>(<span class="dv">1</span>), normFn)<span class="op">$</span>value</a></code></pre></div>
<pre><code>## [1] 0.5482675</code></pre>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb133-1" data-line-number="1"><span class="co"># orthogonality with any constant vector</span></a>
<a class="sourceLine" id="cb133-2" data-line-number="2"><span class="kw">t</span>(comp.proj<span class="op">%*%</span>x) <span class="op">%*%</span><span class="st"> </span>(proj<span class="op">%*%</span><span class="kw">rep</span>(<span class="dv">10000</span>,n))</a></code></pre></div>
<pre><code>##               [,1]
## [1,] -9.094947e-13</code></pre>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p><span class="newthought">Reduction principle </span></p>
<p>The expectation (or the sample average) provides a unification of the minimization of <a href="ch-optApp.html#sub:appSys">metric distances</a>, the projection and the <a href="ch-optApp.html#sub:Proj1">hyperplane</a> in the Hilbert space. This machinary can be extended the projection of <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(\mathcal{H}\)</span>-valued random variables</a> onto some <a href="ch-MatComp.html#sub:vecSpaces">subspace</a>. Let <span class="math inline">\(Y(\omega)\)</span> belong to a randomized Hilbert space <span class="math inline">\(L_{2}(\Omega,\mathcal{Y},\mathbb{P};\mathcal{H})\)</span>. Consider the projection of <span class="math inline">\(Y(\omega)\)</span> onto <span class="math inline">\(L_{2}(\Omega,\mathcal{X},\mathbb{P};\mathcal{H})\)</span>, a <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> of <span class="math inline">\(L_{2}(\Omega,\mathcal{Y},\mathbb{P};\mathcal{H})\)</span>, with the <a href="sub-incomplete.html#sub:beyond2"><span class="math inline">\(\sigma\)</span>-algebra</a> set <span class="math inline">\(\mathcal{X}\subset\mathcal{Y}\)</span>. The projection minimizes the <strong>mean square distance</strong>: <span class="math display">\[\begin{align*}\min_{g\in L_{2}(\Omega,\mathcal{X},\mathbb{P};\mathcal{H})}\|Y(\omega)-g\|_{\mathbb{P}}^{2}&amp;=\min_{g\in {L_{2}(\Omega,\mathcal{X},\mathbb{P};\mathcal{H})}}\mathbb{E}\left[Y(\omega)-g\right]^{2}\\&amp;=\mathbb{E}\left[Y(\omega)-\hat{g}\right]^{2}\end{align*}\]</span>
where <span class="math inline">\(\hat{g}=\mathbb{E}[Y(\omega)|\mathcal{X}]=\mathbb{E}[Y(\omega)(\mathbf{1}_{\mathcal{X}}(Y(\omega))]\)</span> is the expectation of <span class="math inline">\(Y(\omega)\)</span> conditional on <span class="math inline">\(\mathcal{X}\)</span>. Moreover, if the set <span class="math inline">\(\mathcal{X}\)</span> is the <a href="sub-incomplete.html#sub:beyond2"><span class="math inline">\(\sigma\)</span>-algebra</a> generated by another <a href="ch-CalUn.html#sub:rv">random variable</a> <span class="math inline">\(X(\omega)\)</span>, then the expectation becomes <span class="math inline">\(\hat{g}=\mathbb{E}[Y(\omega)|X(\omega)]\)</span>.</p>
<p>The exact representation of a conditional expectation may involve some tedious computation, e.g., the computation of a <a href="ch-UnMulti.html#sub:MultiVar">conditional normal distribution</a> in chapter <a href="ch-UnMulti.html#sub:MultiVar">13.1</a>. To escape from the computational catastroph, one often assume the conditional expectation has some <a href="ch-vecMat.html#sub:linearity">linear regression</a> setting: <span class="math display">\[\mathbb{E}[Y(\omega)|X(\omega)]=\hat{\beta}X(\omega)\]</span> for some projected linear coefficient <span class="math inline">\(\hat{\beta}\in\mathbb{R}\)</span>.<label for="tufte-sn-467" class="margin-toggle sidenote-number">467</label><input type="checkbox" id="tufte-sn-467" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">467</span> This is a special case of the general <a href="ch-CalUn.html#sub:conProb">regression</a> setting, e.g., <span class="math inline">\(\mathbb{E}[Y(\omega)|X(\omega)]=\hat{g}(X(\omega))\)</span> for some continuous function <span class="math inline">\(\hat{g}(\cdot)\)</span>. In the regression setting, the projection is about <span class="math display">\[\begin{align*}&amp;\min_{g\in {L_{2}(\Omega,\mathcal{X},\mathbb{P};\mathcal{H})}}\mathbb{E}\left[Y(\omega)-g\right]^{2}
\\&amp;=\min_{g\in\mathcal{C}}\mathbb{E}\left[Y(\omega)-g(X(\omega))\right]^{2}.\end{align*}\]</span></span>
Then the projection associates with the following minimization:<label for="tufte-sn-468" class="margin-toggle sidenote-number">468</label><input type="checkbox" id="tufte-sn-468" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">468</span> The minimization problem can be rigorously written as <span class="math display">\[\begin{align*}&amp;\min_{g\in\mathcal{L}(\mathcal{H}_{X(\omega)},\mathcal{H}_{Y(\omega)})}\mathbb{E}\left[Y(\omega)-g\right]^{2}\\&amp;=\mathbb{E}\left[Y(\omega)-\mathbb{E}\left[\left.Y(\omega)\right|X(\omega)\right]\right]^{2}\\&amp;=\mathbb{E}\left[Y(\omega)-\hat{\beta}X(\omega)\right]^{2}\end{align*}\]</span>
where <span class="math inline">\(\mathcal{L}(\mathcal{H}_{X(\omega)},\mathcal{H}_{Y(\omega)})\)</span> stands for the space of all <a href="ch-MatComp.html#sub:vecSpaces">linear operators</a> mapping from <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span> to <span class="math inline">\(\mathcal{H}_{Y(\omega)}\)</span>. Recall that <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span> is the space generated by the <a href="ch-vecMat.html#sub:vec">linear combination</a> of <span class="math inline">\(X(\omega)\)</span>.</span><span class="math display">\[\begin{align*}\min_{g\in {L_{2}(\Omega,\mathcal{X},\mathbb{P};\mathcal{H})}}\mathbb{E}\left[Y(\omega)-g\right]^{2}&amp;=\min_{\beta\in\mathbb{R}}\mathbb{E}\left[Y(\omega)-\beta X(\omega)\right]^{2}\\
&amp;=\mathbb{E}\left[Y(\omega)-\hat{\beta}X(\omega)\right]^{2},\end{align*}\]</span>
where <span class="math inline">\(\hat{\beta}X(\omega)=\mathbb{E}[Y(\omega)|X(\omega)]=\mathbf{P}_{\mathcal{H}_{X(\omega)}}Y(\omega)\)</span>
for the linear <a href="ch-optApp.html#sub:appSys">projection operator</a>: <span class="math display">\[\mathbb{E}[\cdot|X(\omega)]=\mathbf{P}_{\mathcal{H}_{X(\omega)}}(\cdot)=\frac{\left\langle \cdot,X(\omega)\right\rangle _{\mathbb{P}}}{\left\langle X(\omega),X(\omega)\right\rangle _{\mathbb{P}}}X(\omega)\]</span> that coincides with <a href="">projection formula</a> in a <a href="">abstract Fourier series</a>.<label for="tufte-sn-469" class="margin-toggle sidenote-number">469</label><input type="checkbox" id="tufte-sn-469" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">469</span> The orthogonality of the projection gives <span class="math display">\[\left\langle Y(\omega)-\hat{\beta}X(\omega),Z(\omega)\right\rangle _{\mathbb{P}}=0\]</span>
for <span class="math inline">\(Z(\omega)\in\mathcal{H}_{X(\omega)}\)</span>.</span></p>
<p>As you can see, the <a href="ch-vecMat.html#sub:linearity">linear regression</a> imposes two conditions on the projection 1. the specific functional form of <span class="math inline">\(\mathbb{E}[Y(\omega)|X(\omega)]\)</span> and 2. the linear projected coefficient <span class="math inline">\(\hat{\beta}\)</span>. The first condition attaches the <a href="ch-vecMat.html#sub:linearity">linearity</a> to the projection. The second condition makes the projection map to the space <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span> that is generated by the <a href="ch-vecMat.html#sub:vec">linear combination</a> of <span class="math inline">\(X(\omega)\)</span>.<label for="tufte-sn-470" class="margin-toggle sidenote-number">470</label><input type="checkbox" id="tufte-sn-470" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">470</span> The orthogonality always holds when <span class="math inline">\(\mathbb{E}[Y(\omega)|X(\omega)]\)</span> satisfies these two conditions, i.e. being a linear projection onto <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span>. However, if one has no idea about the functional form of <span class="math inline">\(\mathbb{E}[Y(\omega)|X(\omega)]\)</span> or if <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span> is not in the subspace <span class="math inline">\(L_{2}(\Omega,\mathcal{X},\mathbb{P};\mathcal{H})\)</span>, then the <a href="ch-randomization.html#sub:SumDecomp">orthogonality</a> may be violated. For example, if <span class="math inline">\(\mathbb{E}[Y|X]=g(X)=\hat{\beta}_{1}X+\hat{\beta}_{2}X^{2}\)</span>, but one insists (wrongly) using the linear setting <span class="math inline">\(\hat{\beta}_{1}X\)</span>, then obviously one will face <span class="math display">\[\left\langle Y(\omega)-\hat{\beta}_1X(\omega),Z(\omega)\right\rangle _{\mathbb{P}}\neq0.\]</span></span></p>
<p>These two conditions (on <a href="ch-vecMat.html#sub:linearity">linearity</a>) turn out to be essential to design two commuting projections to satisfy with the <strong>reduction principle</strong>. The <em>reduction principle</em> says that for two projection operators, if the product of two commuting projections is also a projection, then one can “reduce” a “causal chain” of two causalities into one.</p>
<p>Let’s illustrate the <strong>reduction principle</strong> by the following linear system:
<span class="math display">\[\begin{cases}
X(\omega)= &amp; \hat\beta_{1}Z(\omega)+e_{1}(\omega)\\
Y(\omega)= &amp; \hat\beta_{2}X(\omega)+e_{2}(\omega)
\end{cases}\Leftrightarrow\begin{cases}
\mathbb{E}\left[\left.X(\omega)\right|Z(\omega)\right]= &amp; \hat\beta_{1}Z(\omega)\\
\mathbb{E}\left[\left.Y(\omega)\right|X(\omega)\right]= &amp; \hat\beta_{2}X(\omega)
\end{cases},\]</span>
where <span class="math inline">\(e_{i}(\omega)\)</span> for <span class="math inline">\(i=1,2\)</span> are some zero mean noises.<label for="tufte-sn-471" class="margin-toggle sidenote-number">471</label><input type="checkbox" id="tufte-sn-471" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">471</span> The subspaces generated by these noises should be orthogonal to <span class="math inline">\(\mathcal{H}_{X(\omega)}\)</span> and <span class="math inline">\(\mathcal{H}_{Z(\omega)}\)</span>.</span> You may find a simple <a href="ch-UnMulti.html#sub:Markov">hierarchical</a> structure of the causality. In the system, <span class="math inline">\(Z(\omega)\)</span> gives the causal effect of <span class="math inline">\(X(\omega)\)</span>, and <span class="math inline">\(X(\omega)\)</span> transfers the effect to <span class="math inline">\(Y(\omega)\)</span>. By this causal chain, we can iteratively project <span class="math inline">\(Y(\omega)\)</span> onto the <a href="ch-MatComp.html#sub:vecSpaces">subspaces</a> generated by <span class="math inline">\(X(\omega)\)</span> and <span class="math inline">\(Z(\omega)\)</span> through <span class="math inline">\(\mathbb{E}[\cdot|X(\omega)]\)</span> and <span class="math inline">\(\mathbb{E}[\cdot|Z(\omega)]\)</span>:
<span class="math display">\[\mathbb{E}\left[\left.\mathbb{E}\left[\left.Y(\omega)\right|X(\omega)\right]\right|Z(\omega)\right]=\mathbb{E}\left[\left.\hat{\beta}_{2}X(\omega)\right|Z(\omega)\right]=\hat{\beta_{2}}\hat{\beta_{1}}Z(\omega).\]</span>
Notice that this expectation concides with <span class="math inline">\(\mathbb{E}\left[\left.Y(\omega)\right|Z(\omega)\right]\)</span> from a “reduced” regression <span class="math display">\[\begin{align*}Y(\omega)&amp;=\hat{\beta}_{2}\left(\hat{\beta}_{1}Z(\omega)+e_1(\omega)\right)+e_2(\omega)\\
&amp;=\hat{\beta}_{2}\hat{\beta}_{1}Z(\omega)+e_{12}(\omega)\end{align*}\]</span>
where <span class="math inline">\(e_{12}(\omega)=\hat\beta_{2}e_{1}(\omega)+e_{2}(\omega)\)</span>.<label for="tufte-sn-472" class="margin-toggle sidenote-number">472</label><input type="checkbox" id="tufte-sn-472" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">472</span> The result <span class="math display">\[\begin{align*}&amp;\mathbb{E}\left[\left.\mathbb{E}\left[\left.Y(\omega)\right|X(\omega)\right]\right|Z(\omega)\right]\\&amp;=\mathbb{E}\left[\left.Y(\omega)\right|Z(\omega)\right]\end{align*}\]</span> is also known as the <em>law of iterated expectations</em> when the <span class="math inline">\(\sigma\)</span>-algebra generated by <span class="math inline">\(X(\omega)\)</span> is contained in the <span class="math inline">\(\sigma\)</span>-algebra generated by <span class="math inline">\(Z(\omega)\)</span>.</span></p>
<div class="solution">
<p class="solution-begin">
Numerical illustration <span id="sol-start-106" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-106', 'sol-start-106')"></span>
</p>
<div id="sol-body-106" class="solution-body" style="display: none;">
<p>Using the formula <span class="math display">\[\mathbb{E}\left[\left.\cdot\right|Z(\omega)\right]=\frac{\left\langle \cdot,Z(\omega)\right\rangle _{\mathbb{P}}}{\left\langle Z(\omega),Z(\omega)\right\rangle _{\mathbb{P}}}Z(\omega),\]</span>
we have <span class="math inline">\(\mathbf{P}_{\mathcal{H}_{Z(\omega)}}Y(\omega)=\mathbb{E}[Y(\omega)|Z(\omega)]=\frac{\left\langle Y(\omega),Z(\omega)\right\rangle _{\mathbb{P}}}{\left\langle Z(\omega),Z(\omega)\right\rangle _{\mathbb{P}}}Z(\omega)\)</span>, and<label for="tufte-sn-473" class="margin-toggle sidenote-number">473</label><input type="checkbox" id="tufte-sn-473" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">473</span> The result follows by combining <span class="math display">\[\begin{align*}&amp;\mathbb{E}\left[\left.\mathbb{E}\left[\left.Y(\omega)\right|X(\omega)\right]\right|Z(\omega)\right]\\&amp;=\mathbb{E}\left[\left.\hat{\beta}_{2}X(\omega)\right|Z(\omega)\right]\end{align*}\]</span>
and <span class="math display">\[\begin{align*}&amp;\mathbb{E}\left[\left.\hat{\beta}_{2}X(\omega)\right|Z(\omega)\right]\\&amp;=\mathbb{E}\left[\left.\mathbf{P}_{\mathcal{H}_{X(\omega)}}Y(\omega)\right|Z(\omega)\right].\end{align*}\]</span></span><br><span class="math display">\[\begin{align*}&amp;\mathbf{P}_{\mathcal{H}_{Z(\omega)}}\mathbf{P}_{\mathcal{H}_{X(\omega)}}Y(\omega)\\
&amp;=\mathbb{E}\left[\left.\mathbb{E}\left[\left.Y(\omega)\right|X(\omega)\right]\right|Z(\omega)\right]=\hat{\beta}_{2}\hat{\beta}_{1}Z(\omega)
\\&amp;=\frac{\left\langle \mathbf{P}_{\mathcal{H}_{X(\omega)}}Y(\omega),Z(\omega)\right\rangle _{\mathbb{P}}}{\left\langle Z(\omega),Z(\omega)\right\rangle _{\mathbb{P}}}Z(\omega).\end{align*}\]</span>
and <span class="math display">\[\begin{align*}&amp;\mathbf{P}_{\mathcal{H}_{X(\omega)}}\mathbf{P}_{\mathcal{H}_{Z(\omega)}}Y(\omega)\\
&amp;=\mathbb{E}\left[\left.\mathbb{E}\left[\left.Y(\omega)\right|Z(\omega)\right]\right|X(\omega)\right]
\\&amp;=\frac{\left\langle \mathbf{P}_{\mathcal{H}_{Z(\omega)}}Y(\omega),X(\omega)\right\rangle _{\mathbb{P}}}{\left\langle X(\omega),X(\omega)\right\rangle _{\mathbb{P}}}X(\omega).\end{align*}\]</span>
The following example shows the equivalence of <span class="math inline">\(\mathbf{P}_{\mathcal{H}_{Z(\omega)}}\)</span>, <span class="math inline">\(\mathbf{P}_{\mathcal{H}_{Z(\omega)}}\mathbf{P}_{\mathcal{H}_{X(\omega)}}\)</span>, and <span class="math inline">\(\mathbf{P}_{\mathcal{H}_{X(\omega)}}\mathbf{P}_{\mathcal{H}_{Z(\omega)}}\)</span>.</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb135-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb135-2" data-line-number="2">n =<span class="st"> </span><span class="dv">300</span>; b1=<span class="fl">0.5</span>; b2=<span class="dv">2</span>; z =<span class="st"> </span><span class="kw">runif</span>(n,<span class="op">-</span><span class="dv">10</span>,<span class="dv">10</span>); </a>
<a class="sourceLine" id="cb135-3" data-line-number="3"><span class="co"># create orthogonal noises</span></a>
<a class="sourceLine" id="cb135-4" data-line-number="4"><span class="kw">library</span>(MASS); orth =<span class="st"> </span><span class="kw">sample</span>(n<span class="op">*</span><span class="dv">2</span>, n); orth.sample =<span class="st"> </span><span class="kw">Null</span>(orth);</a>
<a class="sourceLine" id="cb135-5" data-line-number="5"></a>
<a class="sourceLine" id="cb135-6" data-line-number="6">error1 =<span class="st"> </span>orth.sample[,<span class="kw">sample</span>(n,<span class="dv">1</span>)];  </a>
<a class="sourceLine" id="cb135-7" data-line-number="7">error2 =<span class="st"> </span>orth.sample[,<span class="kw">sample</span>(n,<span class="dv">1</span>)]; </a>
<a class="sourceLine" id="cb135-8" data-line-number="8"></a>
<a class="sourceLine" id="cb135-9" data-line-number="9"><span class="co"># commuting property appears in a slower rate: </span></a>
<a class="sourceLine" id="cb135-10" data-line-number="10"><span class="co"># error1 = rnorm(n,0); error2 =rnorm(n,0)</span></a>
<a class="sourceLine" id="cb135-11" data-line-number="11"></a>
<a class="sourceLine" id="cb135-12" data-line-number="12">x =<span class="st"> </span>b1<span class="op">*</span>z <span class="op">+</span><span class="st"> </span>error1;</a>
<a class="sourceLine" id="cb135-13" data-line-number="13">y =<span class="st"> </span>b2<span class="op">*</span>x <span class="op">+</span><span class="st"> </span>error2;</a>
<a class="sourceLine" id="cb135-14" data-line-number="14"></a>
<a class="sourceLine" id="cb135-15" data-line-number="15">proj.x =x <span class="op">%*%</span><span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(x) <span class="op">%*%</span><span class="st"> </span>x) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(x);</a>
<a class="sourceLine" id="cb135-16" data-line-number="16"></a>
<a class="sourceLine" id="cb135-17" data-line-number="17"><span class="co"># b12 = 2*0.5 = 1</span></a>
<a class="sourceLine" id="cb135-18" data-line-number="18">b12h.Pz =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(z) <span class="op">%*%</span><span class="st"> </span>z) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(z)<span class="op">%*%</span><span class="st"> </span>y; </a>
<a class="sourceLine" id="cb135-19" data-line-number="19">b12h.PzPx =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(z) <span class="op">%*%</span><span class="st"> </span>z) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(z)<span class="op">%*%</span><span class="st"> </span>proj.x <span class="op">%*%</span><span class="st"> </span>y; </a>
<a class="sourceLine" id="cb135-20" data-line-number="20">b12h.PxPz=<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(x) <span class="op">%*%</span><span class="st"> </span>x) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(x)<span class="op">%*%</span><span class="st"> </span>(<span class="kw">c</span>(b12h.Pz)<span class="op">*</span>x) ;</a>
<a class="sourceLine" id="cb135-21" data-line-number="21"></a>
<a class="sourceLine" id="cb135-22" data-line-number="22"><span class="kw">print</span>(<span class="kw">c</span>(b12h.Pz,b12h.PzPx,b12h.PxPz))</a></code></pre></div>
<pre><code>## [1] 1.001882 1.001882 1.001882</code></pre>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>The following <em>commuting projection</em> property tells us when such a reduction merges:<label for="tufte-sn-474" class="margin-toggle sidenote-number">474</label><input type="checkbox" id="tufte-sn-474" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">474</span> Note that <span class="math display">\[\begin{align*}\mathbf{P}_{\mathcal{H}_{Z(\omega)}}X(\omega)=\mathbb{E}\left[\left.X(\omega)\right|Z(\omega)\right]=&amp;\hat{\beta}_{1}Z(\omega),\\\mathbf{P}_{\mathcal{H}_{X(\omega)}}Y(\omega)=\mathbb{E}\left[\left.Y(\omega)\right|X(\omega)\right]=&amp;\hat{\beta}_{2}X(\omega)\end{align*}\]</span>
The iterated expectations can be written as <span class="math display">\[\begin{align*}&amp;\mathbb{E}\left[\left.\mathbb{E}\left[\left.Y(\omega)\right|X(\omega)\right]\right|Z(\omega)\right]\\&amp;=\mathbb{E}\left[\left.\mathbf{P}_{\mathcal{H}_{X(\omega)}}Y(\omega)\right|Z(\omega)\right]\\&amp;=\mathbf{P}_{\mathcal{H}_{Z(\omega)}}\mathbf{P}_{\mathcal{H}_{X(\omega)}}Y(\omega).\end{align*}\]</span></span>
<span class="math display">\[\mathbf{P}_{\mathcal{H}_{Z(\omega)}}\mathbf{P}_{\mathcal{H}_{X(\omega)}}=\mathbf{P}_{\mathcal{H}_{X(\omega)}}\mathbf{P}_{\mathcal{H}_{Z(\omega)}}=\mathbf{P}_{\mathcal{H}_{Z(\omega)}}\]</span> when the <a href="ch-MatComp.html#sub:vecSpaces">subspace</a> generated by <span class="math inline">\(X(\omega)\)</span> belongs to the one generated by <span class="math inline">\(Z(\omega)\)</span>, namely <span class="math inline">\(\mathcal{H}_{X(\omega)}\subset\mathcal{H}_{Z(\omega)}\)</span>. This condition is indeed the reason for having the <strong>reduction principle</strong> in the previous example.</p>
<ul>
<li>Assume that two different <a href="ch-randomization.html#sub:SumDecomp">orthogonal direct sums</a> are valid in a <a href="ch-representation.html#sub:conjugacy">Hilbert space</a> <span class="math inline">\(\mathcal{V}\)</span> <span class="math display">\[\mathcal{V}=\mathcal{V}_{1}\oplus\mathcal{W}_{1}=\mathcal{V}_{2}\oplus\mathcal{W}_{2}.\]</span>
Let <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}:\mathcal{V}\rightarrow\mathcal{V}_{1}\)</span> and <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{2}}:\mathcal{V}\rightarrow\mathcal{V}_{2}\)</span> be <a href="ch-randomization.html#sub:SumDecomp">orthogonal projection operators</a>. The <strong>reduction principle</strong> says:<label for="tufte-sn-475" class="margin-toggle sidenote-number">475</label><input type="checkbox" id="tufte-sn-475" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">475</span> Moreover. <span class="math inline">\(\mathcal{V}_{1}\subset\mathcal{W}_{2}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\subset\mathcal{W}_{1}\)</span> if and only if
<span class="math display">\[\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}=0.\]</span> The proof of these two statements is given below.</span> <span class="math inline">\(\mathcal{V}_{1}\subset\mathcal{V}_{2}\)</span> and <span class="math inline">\(\mathcal{W}_{2}\subset\mathcal{W}_{1}\)</span> if and only if <span class="math display">\[\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}=\mathbf{P}_{\mathcal{V}_{1}}.\]</span>
</li>
</ul>
<div class="solution">
<p class="solution-begin">
Proof <span id="sol-start-107" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-107', 'sol-start-107')"></span>
</p>
<div id="sol-body-107" class="solution-body" style="display: none;">
<ol style="list-style-type: lower-roman">
<li>Assume <span class="math display">\[\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}=\mathbf{P}_{\mathcal{V}_{1}}.\]</span>
</li>
</ol>
<p>For any <span class="math inline">\(f\in\mathcal{V}\)</span>, <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}f\in\mathcal{V}_{1}\)</span> implies <span class="math display">\[\mathbf{P}_{\mathcal{V}_{1}}f=\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}f\in\mathcal{V}_{2}.\]</span> This implication tells <span class="math inline">\(\mathcal{V}_{1}\subset\mathcal{V}_{2}\)</span>. Moreover,
<span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=\mathbf{P}_{\mathcal{V}_{1}}\)</span> implies
<span class="math display">\[(\mathbf{I}-\mathbf{P}_{\mathcal{V}_{1}})(\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}})=\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}}-\mathbf{P}_{\mathcal{V}_{1}}+\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}}.\]</span>
So for any <span class="math inline">\((\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}})g\in\mathcal{W}_{2}\)</span>, we know that <span class="math display">\[(\mathbf{I}-\mathbf{P}_{\mathcal{V}_{1}})(\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}})g\in\mathcal{W}_{1},\]</span> which implies <span class="math inline">\(\mathcal{W}_{2}\subset\mathcal{W}_{1}\)</span>.</p>
<p>Assume <span class="math inline">\(\mathcal{V}_{1}\subset\mathcal{V}_{2}\)</span> and <span class="math inline">\(\mathcal{W}_{2}\subset\mathcal{W}_{1}\)</span>.</p>
<p>If <span class="math inline">\(\mathcal{V}_{1}\subset\mathcal{V}_{2}\)</span>, for any <span class="math inline">\(f\in\mathcal{V}\)</span>, the projection gives <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}f\in\mathcal{V}_{1}\subset\mathcal{V}_{2}\)</span>. The fixed point property of the projection operator tells that <span class="math display">\[\mathbf{P}_{\mathcal{V}_{2}}(\mathbf{P}_{\mathcal{V}_{1}}f)=\mathbf{P}_{\mathcal{V}_{1}}f.\]</span>
Thus we can see <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}=\mathbf{P}_{\mathcal{V}_{1}}\)</span>. Moreover, because <span class="math inline">\(\mathcal{W}_{2}\subset\mathcal{W}_{1}\)</span>, for any <span class="math inline">\(g\in\mathcal{V}\)</span>, <span class="math display">\[(\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}})g\in\mathcal{W}_{2}\subset\mathcal{W}_{1}.\]</span> Again we have <span class="math display">\[(\mathbf{I}-\mathbf{P}_{\mathcal{V}_{1}})\left((\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}})g\right)=(\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}})g,\]</span>
which implies <span class="math display">\[\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}}-\mathbf{P}_{\mathcal{V}_{1}}+\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}}\]</span>
or say <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=\mathbf{P}_{\mathcal{V}_{1}}\)</span>.</p>
<ol start="2" style="list-style-type: lower-roman">
<li>Assume <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}=0\)</span>.</li>
</ol>
<p>For any <span class="math inline">\(f\in\mathcal{V}_{1}\)</span>, the fixed point property tells that <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}f=f\)</span>. Hence <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}f=\mathbf{P}_{\mathcal{V}_{2}}f=0\)</span>
implies <span class="math inline">\(f\in\mathcal{W}_{2}\)</span>, and so <span class="math inline">\(\mathcal{V}_{1}\subset\mathcal{W}_{2}\)</span>. On the other hand, for any <span class="math inline">\(g\in\mathcal{V}_{2}\)</span>, it follows that <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{2}}g\in\mathcal{V}_{2}\)</span>. The condition <span class="math display">\[\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}g=\mathbf{P}_{\mathcal{V}_{1}}g=0\]</span>
tells that <span class="math inline">\(g\in\mathcal{W}_{2}\)</span>. Thus we have <span class="math inline">\(\mathcal{V}_{2}\subset\mathcal{W}_{2}\)</span>.</p>
<p>Assume <span class="math inline">\(\mathcal{V}_{1}\subset\mathcal{W}_{2}\)</span> and <span class="math inline">\(\mathcal{V}_{2}\subset\mathcal{W}_{1}\)</span>.</p>
<p>For any <span class="math inline">\(f\in\mathcal{V}\)</span>, <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}f\in\mathcal{V}_{1}\)</span>
and the condition
<span class="math display">\[(\mathbf{I}-\mathbf{P}_{\mathcal{V}_{2}})\mathbf{P}_{\mathcal{V}_{1}}f=\mathbf{P}_{\mathcal{V}_{1}}f-\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}f=\mathbf{P}_{\mathcal{V}_{1}}f\]</span>
holds for any <span class="math inline">\(f\)</span> as <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}f\in\mathcal{W}_{1}\)</span>. Thus we conclude <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}}=0\)</span>. A similar argument works for <span class="math inline">\(\mathbf{P}_{\mathcal{V}_{1}}\mathbf{P}_{\mathcal{V}_{2}}=0\)</span>.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Executing a sequence projections behaves like mutiplying matrices, so the order matters. However, the <strong>reduction principle</strong> reveals a possibility that one can permute the order.<label for="tufte-sn-476" class="margin-toggle sidenote-number">476</label><input type="checkbox" id="tufte-sn-476" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">476</span> In the general setting, one may be interested in a sequence of non commuting pair of projections but converges to a new projection. This sequence would also achieve the <strong>reduction principle</strong> in a wider sense. For example, <span class="citation">Neumann (<a href="bibliography.html#ref-vonNeumann1950">1950</a>)</span> (Chapter 14) considers any two closed subspace <span class="math inline">\(\mathcal{V}_{1}\)</span>, <span class="math inline">\(\mathcal{V}_{2}\)</span> in the Hilbert space <span class="math inline">\(\mathcal{H}\)</span>. The new projection operator <span class="math display">\[\lim_{n\rightarrow\infty}(\mathbf{P}_{\mathcal{V}_{2}}\mathbf{P}_{\mathcal{V}_{1}})^{n}(f)=\mathbf{P}_{\mathcal{V}_{1}\cap\mathcal{V}_{2}}(f)\]</span>
emerges for any <span class="math inline">\(f\in\mathcal{H}\)</span>.</span> In several quantum mechanics related disciplines, the experiments are identified by the orthogonal projections on a Hilbert space. For example, a binary decision of yes and no may be thought of as a projection onto two orthogonal subspaces, the <a href="ch-randomization.html#sub:SumDecomp">range space</a> and the <a href="ch-randomization.html#sub:SumDecomp">null space</a>. Moreover, unlike the classical mechanics, the outcome of two yes-no experiments may depend on the order in which the experiments are measured. Therefore, the <strong>reduction principle</strong> can play an important role in simplifying the identification.</p>
<div class="solution">
<p class="solution-begin">
Two-stage estimate and pseudo-norm <span id="sol-start-108" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-108', 'sol-start-108')"></span>
</p>
<div id="sol-body-108" class="solution-body" style="display: none;">
<p>Consider the data vector <span class="math inline">\(\mathbf{x},\mathbf{y},\mathbf{z}\in\mathbb{R}^n\)</span>. One may assume that they are generated by the following model <span class="math display">\[\begin{cases}
\mathbf{x}= &amp; \beta_{1,0}\mathbf{z}+\mathbf{e}_{1},\\
\mathbf{y}= &amp; \beta_{2,0}\mathbf{x}+\mathbf{e}_{2}.
\end{cases}\]</span></p>
<p>The estimates of <span class="math inline">\(\beta_{1,0}\)</span>, <span class="math inline">\(\beta_{2,0}\)</span> are also projections. Unlike the <strong>reduction principle</strong>, in this case, one has to estimate the parameters seperately.</p>
<p>It turns out that the following two way of estimating <span class="math inline">\(\beta_{2,0}\)</span> are equivalent.</p>
<ol style="list-style-type: decimal">
<li><p>One can first estimate <span class="math inline">\(\beta_{1,0}\)</span>, then reconstruct <span class="math inline">\(\mathbf{x}\)</span> by <span class="math inline">\(\hat{\beta}_{1}\mathbf{z}\)</span> so that one estimates <span class="math inline">\(\beta_{2,0}\)</span> through the regression
<span class="math display">\[\mathbf{y}=\beta_{2,0}(\hat{\beta}_{1}\mathbf{z})+\mathbf{e}_{2}.\]</span></p></li>
<li><p>For a projection matrix <span class="math inline">\(\mathbf{P}_{\mathbf{y}}\)</span>, we’ve derived the connection between the distance and the sandwich form <span class="math display">\[\|(\mathbf{I}-\mathbf{P}_{\mathbf{y}})\mathbf{y}\|^{2}=\left\langle \mathbf{P}_{\mathbf{y}^{\bot}}\mathbf{y},\mathbf{P}_{\mathbf{y}^{\bot}}\mathbf{y}\right\rangle =\mathbf{y}^{\top}\mathbf{P}_{\mathbf{y}^{\bot}}\mathbf{y}.\]</span> Recall that for any psotive definited matrix <span class="math inline">\(\mathbf{M}\in\mathbb{R}^{n\times n}\)</span>, we have <span class="math inline">\(\mathbf{y}^{\top}\mathbf{M}\mathbf{y}\geq0\)</span> for any <span class="math inline">\(\mathbf{y}\in\mathbb{R}^{n}\)</span>. The sandwich form <span class="math inline">\(\mathbf{y}^{\top}\mathbf{M}\mathbf{y}\)</span> serves like a norm.<label for="tufte-sn-477" class="margin-toggle sidenote-number">477</label><input type="checkbox" id="tufte-sn-477" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">477</span> It is a <em>pseudo norm</em>, because <span class="math inline">\(\mathbf{y}^{\top}\mathbf{M}\mathbf{y}\)</span> can be zero for <span class="math inline">\(\mathbf{y}\neq 0\)</span> when <span class="math inline">\(\mathbf{M}\)</span> is singular.</span> Because any projection matrix <span class="math inline">\(\mathbf{P}_{\mathbf{y}}\)</span> is positive definited, we can formalize the estimate of <span class="math inline">\(\beta_{2,0}\)</span> under the <strong>pseduo-norm</strong> of <span class="math inline">\(\mathbf{P}_{\mathbf{z}}\)</span>: <span class="math display">\[\min_{\beta_{2}} \left\langle \mathbf{P}_{\mathbf{z}}(\mathbf{y}-\beta_{2}\mathbf{x}),\mathbf{P}_{\mathbf{z}}(\mathbf{y}-\beta_{2}\mathbf{x})\right\rangle =(\mathbf{y}-\beta_{2}\mathbf{x})^{\top}\mathbf{P}_{\mathbf{z}}(\mathbf{y}-\beta_{2}\mathbf{x})^{\top}.\]</span> By the orthogonality condition, <span class="math display">\[\left[\mathbf{P}_{\mathbf{z}}(\mathbf{y}-\hat{\beta}_{2}\mathbf{x})\right]^{\top}\mathbf{P}_{\mathbf{z}}\mathbf{x}=\left[\mathbf{P}_{\mathbf{z}}\mathbf{x}\right]^{\top}\left[\mathbf{P}_{\mathbf{z}}(\mathbf{y}-\hat{\beta}_{2}\mathbf{x})\right]=0\]</span>
we have <span class="math inline">\((\mathbf{P}_{\mathbf{z}}\mathbf{x})^{\top}\mathbf{y}-\hat{\beta}_{2}\mathbf{x}^{\top}\mathbf{P}_{\mathbf{z}}\mathbf{x}=0\)</span> which implies <span class="math inline">\(\hat{\beta}_{2}=(\mathbf{x}^{\top}\mathbf{P}_{\mathbf{z}}\mathbf{x})^{-1}\mathbf{x}^{\top}\mathbf{P}_{\mathbf{z}}\mathbf{y}\)</span>.</p></li>
</ol>
<p>The numerical example will show us these two estimates return exactly the same results. Because the second expression is just a mathematical tautology of the first one.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb137-1" data-line-number="1"><span class="co"># Method 1</span></a>
<a class="sourceLine" id="cb137-2" data-line-number="2"></a>
<a class="sourceLine" id="cb137-3" data-line-number="3"><span class="co"># regression x = b1 * z + e_1</span></a>
<a class="sourceLine" id="cb137-4" data-line-number="4">b1h =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(z) <span class="op">%*%</span><span class="st"> </span>z) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(z)<span class="op">%*%</span><span class="st"> </span>x; </a>
<a class="sourceLine" id="cb137-5" data-line-number="5"><span class="co"># construct x_hat</span></a>
<a class="sourceLine" id="cb137-6" data-line-number="6">xhat=<span class="st"> </span>z<span class="op">%*%</span>b1h;</a>
<a class="sourceLine" id="cb137-7" data-line-number="7"><span class="co"># regression y = b2 * x_hat + e_2</span></a>
<a class="sourceLine" id="cb137-8" data-line-number="8">b2h<span class="fl">.1</span> =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(xhat) <span class="op">%*%</span><span class="st"> </span>xhat) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(xhat)<span class="op">%*%</span><span class="st"> </span>y;</a>
<a class="sourceLine" id="cb137-9" data-line-number="9"></a>
<a class="sourceLine" id="cb137-10" data-line-number="10"><span class="co"># Method 2</span></a>
<a class="sourceLine" id="cb137-11" data-line-number="11"><span class="co"># projection matrix</span></a>
<a class="sourceLine" id="cb137-12" data-line-number="12">proj.z =z <span class="op">%*%</span><span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(z) <span class="op">%*%</span><span class="st"> </span>z) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(z);</a>
<a class="sourceLine" id="cb137-13" data-line-number="13"><span class="co"># || y-b2x || under the pseudo-norm induced by P_z</span></a>
<a class="sourceLine" id="cb137-14" data-line-number="14">b2h<span class="fl">.2</span>  =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(x) <span class="op">%*%</span><span class="st"> </span>proj.z <span class="op">%*%</span><span class="st"> </span>x) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(x) <span class="op">%*%</span><span class="st"> </span>proj.z <span class="op">%*%</span><span class="st"> </span>y</a>
<a class="sourceLine" id="cb137-15" data-line-number="15"></a>
<a class="sourceLine" id="cb137-16" data-line-number="16"><span class="kw">print</span>(<span class="kw">c</span>(b2h<span class="fl">.1</span>,b2h<span class="fl">.2</span>))</a></code></pre></div>
<pre><code>## [1] 2.000449 2.000449</code></pre>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p><span class="newthought">Kalman filter </span></p>
<p><a href="ch-randomization.html#sub:SumDecomp">Wold decomposition</a> gives a causal representation of the <a href="ch-randomization.html#sub:SumDecomp">weakly stationary processes</a>. It says that the causality of all these processes may be explained by a <a href="ch-randomization.html#sub:SumDecomp">white noise basis</a>. On the other hand, apart from the space <a href="ch-MatComp.html#sub:vecSpaces">spanned</a> by <a href="ch-randomization.html#sub:RHilbert">white noises</a>, one may have some underlying processes in mind that are believed (by oneself) to acquire the driving forces for some observations of interest.</p>
<p>The current concern is to derive a scheme from which the model of interest and the observations resonate. To achieve an ideal resonation, one needs to filter out the unnecessary information. A natural attempt is to project the dynamical law of the model onto that of the observable <span class="math inline">\(\mathbf{y}_t\)</span> by minimizing their <strong>mean square distance</strong>. This scheme is called <strong>Kalman’s filter</strong>.</p>
<p>The implementation of this scheme relates to the use of the <a href="ch-vecMat.html#sub:linearity">affine function</a> for the conditional expectation. Let’s start with a toy example. Suppose that <span class="math inline">\(X(\omega)\)</span> represent the underlying model and <span class="math inline">\(Y(\omega)\)</span> be the observable process. We need to project <span class="math inline">\(X(\omega)\)</span> onto the space generated by <span class="math inline">\(Y(\omega)\)</span>. And we expect that the projection will be <span class="math inline">\(\mathbb{E}[X(\omega)|Y(\omega)]\)</span>. Now assume the functional form of <span class="math inline">\(\mathbb{E}[X(\omega)|Y(\omega)]\)</span> is <a href="ch-vecMat.html#sub:linearity">affine</a>, the projection becomes <span class="math display">\[\mathbb{E}\left[X(\omega)-\mathbb{E}[X(\omega)|Y(\omega)]\right]^{2}=\min_{a,b}\mathbb{E}\left[X(\omega)-a-bY(\omega)\right]^{2},\]</span>
with <span class="math inline">\(\hat{a}=\mathbb{E}[X(\omega)]-\hat{b}\mathbb{E}[Y(\omega)]\)</span> and <span class="math inline">\(\hat{b}=\mbox{Cov}(Y(\omega),X(\omega))/\mbox{Var}(Y(\omega))\)</span> when both <span class="math inline">\(X(\omega)\)</span> and <span class="math inline">\(Y(\omega)\)</span> are Gaussian (or normal).<label for="tufte-sn-478" class="margin-toggle sidenote-number">478</label><input type="checkbox" id="tufte-sn-478" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">478</span> To see this result, we use the result of the <a href="ch-UnMulti.html#sub:MultiVar">conditional normal expectation</a> given in chapter <a href="ch-UnMulti.html#sub:MultiVar">13.1</a>:
<span class="math display">\[\begin{align*}&amp;\mathbb{E}[X(\omega)|Y(\omega)]=\mathbb{E}[X(\omega)]\\&amp;+\frac{\mbox{Cov}(Y(\omega),X(\omega))}{\mbox{Var}(Y(\omega))}\left(Y(\omega)-\mathbb{E}[Y(\omega)]\right)=\\
&amp;= \mathbb{E}[X(\omega)]-\frac{\mbox{Cov}(Y(\omega),X(\omega))}{\mbox{Var}(Y(\omega))}\mathbb{E}[Y(\omega)]\\
&amp;+ \frac{\mbox{Cov}(Y(\omega),X(\omega))}{\mbox{Var}(Y(\omega))}Y(\omega).
\end{align*}\]</span></span></p>
<p>Now we can extend this idea to more sophisticated setting. Consider the model of interests as a time series <a href="ch-UnMulti.html#sub:Markov">state vector</a> <span class="math inline">\(\mathbf{X}_{t}(\omega)\in\mathbb{R}^{m}\)</span>. Let the observations be the data vector <span class="math inline">\(\mathbf{y}_{t}\in\mathbb{R}^{n}\)</span>. The specification of the system is<label for="tufte-sn-479" class="margin-toggle sidenote-number">479</label><input type="checkbox" id="tufte-sn-479" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">479</span> Note that <span class="math inline">\(\mathbf{y}_t\)</span> is the realization of some random vector <span class="math inline">\(\mathbf{Y}_t(\omega)\)</span>. That is, <span class="math inline">\(\mathbf{X}_t(\omega)\)</span> does not generate the observations. However, <span class="math inline">\(\mathbf{X}_t(\omega)\)</span> is the model of the interest. And it could be the case that <span class="math display">\[\mathbf{X}_t(\omega)=\mathbf{Y}_{t}(\omega).\]</span> However, the second equation of the model simply says that <span class="math inline">\(\mathbf{X}_{t}(\omega)\)</span> may have a linear relationship with the observations <span class="math inline">\(\mathbf{y}_t\)</span>. Thus <span class="math inline">\(\mathbf{X}_t(\omega)\)</span> does not need to equal to (or not even cause the change of) <span class="math inline">\(\mathbf{Y}_t(\omega)\)</span>.</span>
<span class="math display">\[\begin{align*}
\mathbf{X}_{t+1}(\omega)&amp;=  \mathbf{A}\mathbf{X}_{t}(\omega)+\mathbf{C}\mathbf{W}_{1,t}(\omega)\\
\mathbf{y}_{t}&amp;=    \mathbf{B}\mathbf{X}_{t}(\omega)+\mathbf{W}_{2,t}(\omega)\\
\left(\begin{array}{c}
\mathbf{W}_{1,t}(\omega)\\
\mathbf{W}_{2,t}(\omega)
\end{array}\right)  &amp;\sim\mathcal{N}\left(0,\left[\begin{array}{cc}
\mathbf{Q} &amp; 0\\
0 &amp; \mathbf{R}
\end{array}\right]\right)
\end{align*}\]</span>
The <em>filtering procedure</em> is about (re-)constructing a <a href="ch-randomization.html#sub:RHilbert">probabilistic dynamical law</a> of the underlying state <span class="math inline">\(\{\mathbf{X}_{s}(\omega)\}_{s\leq t}\)</span> given all observations <span class="math inline">\(\{\mathbf{y}_{s}\}_{s\leq t-1}\)</span>.
We can see that at time <span class="math inline">\(t\)</span>, <span class="math inline">\(\mathbf{X}_{t}(\omega)\)</span> is a Gaussian random vector in the <a href="ch-randomization.html#sub:RHilbert"><span class="math inline">\(L_{2}(\Omega,\mathcal{F}_{t},\mathbb{P};\mathbb{R})\)</span></a>. In addition, the system is <a href="ch-UnMulti.html#sub:Markov">Markovian</a>. Thus, the dynamics of <span class="math inline">\(\{\mathbf{X}_{s}(\omega)\}_{s\leq t}\)</span> is driven by a <a href="ch-UnMulti.html#sub:Markov">transition probability</a> that only depends on the information from the previous period, namely <span class="math inline">\(\mathcal{F}_{t-1}\)</span>. We can express the <a href="ch-randomization.html#sub:RHilbert">probabilistic dynamical law</a> of <span class="math inline">\(\{\mathbf{X}_{s}(\omega)\}_{s\leq t}\)</span> in terms of the <a href="ch-randomization.html#sub:RHilbert">mean and covariance functions</a> of <span class="math inline">\(\mathbf{X}_{t}(\omega)\)</span> conditioning on the previous information <span class="math inline">\(\mathcal{F}_t\)</span>:
<span class="math display">\[\begin{align*}
&amp;\Pr\left(\mathbf{X}_{t}(\omega)|\mathcal{F}_{t-1}\right)   \sim\mathcal{N}\left(\hat{\mathbf{x}}_{t},\hat{\Sigma}_{t}\right)\\
 &amp;\hat{\mathbf{x}}_{t}=\mathbb{E}[\mathbf{X}_{t}(\omega)|\mathcal{F}_{t-1}],\\  &amp; \hat{\Sigma}_{t}=\mathbb{E}\left[\left.(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})^{\top}\right|\mathcal{F}_{t-1}\right].
\end{align*}\]</span>
To allow <span class="math inline">\(\mathbf{X}_{t}\)</span> resonate with <span class="math inline">\(\mathbf{y}_t\)</span>, the scheme has to make the previous <a href="ch-randomization.html#sub:RHilbert">probabilistic dynamical law</a> incorporate with the observations. For the <a href="ch-randomization.html#sub:RHilbert">mean function</a>, it is about conditioning the new information provided by <span class="math inline">\(\mathbf{y}_t\)</span>. So we just use the previous <a href="ch-UnMulti.html#sub:MultiVar">conditional normal mean</a> formula to incorporate <span class="math inline">\(\mathbf{y}_t\)</span>,
<span class="math display">\[\begin{align*}\mathbb{E}[\mathbf{X}_{t}(\omega)|\mathbf{y}_{t},\mathcal{F}_{t-1}]&amp;=\mathbb{E}[\mathbf{X}_{t}(\omega)|\mathcal{F}_{t-1}]+\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbb{E}\left[\left.\mathbf{y}_{t}\right|\mathcal{F}_{t-1}\right]\right)\\&amp;= \mathbb{E}[\mathbf{X}_{t}(\omega)|\mathcal{F}_{t-1}]+\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbb{E}\left[\left.\mathbf{B}\mathbf{X}_{t}(\omega)+\mathbf{W}_{2,t}(\omega)\right|\mathcal{F}_{t-1}\right]\right)\\
&amp;=  \hat{\mathbf{x}}_{t}+\underset{\mbox{error correction}}{\underbrace{\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)}},\end{align*}\]</span> where the ad hoc term <span class="math inline">\(\mathbf{K}_{t}\)</span> (like the coefficient <span class="math inline">\(b\)</span> in the affine function)
is called (optimal) <em>Kalman gain matrix</em>.</p>
<p>Then one can derive the <a href="ch-randomization.html#sub:RHilbert">covarian function</a> to have the full picture of this dynamical law:<label for="tufte-sn-480" class="margin-toggle sidenote-number">480</label><input type="checkbox" id="tufte-sn-480" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">480</span> Here is a quick and dirty “derivation”. In chapter <a href="ch-UnMulti.html#sub:MultiVar">13.1</a>, we know that the <a href="ch-UnMulti.html#sub:MultiVar">conditional normal variance</a> formula gives
<span class="math display">\[\begin{align*}&amp;\mbox{Var}[X(\omega)|Y(\omega)]=\mbox{Var}(X(\omega))\\&amp;-\frac{\mbox{Cov}(Y(\omega),X(\omega))\times\mbox{Cov}(X(\omega),Y(\omega))}{\mbox{Var}(Y(\omega))}.\end{align*}\]</span> Notice that <span class="math inline">\(\mathbf{K}_{t}\)</span>
plays the same role as <span class="math inline">\(\mbox{Cov}(Y(\omega),X(\omega))/\mbox{Var}(Y(\omega))\)</span> in the <a href="ch-UnMulti.html#sub:MultiVar">conditional normal mean</a> projection. So we can guess that <span class="math display">\[\begin{align*}&amp;\mbox{Var}[\mathbf{X}_{t}(\omega)|\mathbf{y}_{t},\mathcal{F}_{t-1}]=   \mbox{Var}(\mathbf{X}_{t}(\omega)|\mathcal{F}_{t-1})\\&amp;-\mathbf{K}_{t}\left[\mbox{Var}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)\right]\mathbf{K}_{t}^{\top}
\\&amp;=    \hat{\Sigma}_{t}-\mathbf{K}_{t}\left[\mbox{Var}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)\right]\mathbf{K}_{t}^{\top}.\end{align*}\]</span> Indeed, it is.</span>
<span class="math display">\[\begin{align*}\Pr\left(\mathbf{X}_{t}(\omega)|\mathbf{y}_{t},\mathcal{F}_{t-1}\right)\sim\mathcal{N}\left(\underset{\mbox{mean}}{\underbrace{\hat{\mathbf{x}}_{t}+\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)}},\\ \underset{\mbox{variance}}{\underbrace{\hat{\Sigma}_{t}-\mathbf{K}_{t}\left[\mbox{Var}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)\right]^{-1}\mathbf{K}_{t}^{\top}}}\right).\end{align*}\]</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:Kalman"></span>
<img src="fig/Part4/Kalman.gif" alt="Kalman filter" width="100%"><!--
<p class="caption marginnote">-->Figure 16.4: Kalman filter<!--</p>-->
<!--</div>--></span>
</p>
<p>Roughly speaking, at each time step <span class="math inline">\(t\)</span>, the scheme of <em>Kalman’s filter</em> consists of two steps:</p>
<ol style="list-style-type: decimal">
<li><p>Estimate the mean and covariance of <span class="math inline">\(\mathbf{X}_t(\omega)\)</span> through the probability law that incorporate with the observation <span class="math inline">\(\mathbf{y}_t\)</span>, namely <span class="math inline">\(\Pr\left(\mathbf{X}_{t}(\omega)|\mathbf{y}_t, \mathcal{F}_{t-1}\right)\)</span>.</p></li>
<li><p>Predict the distribution of <span class="math inline">\(\mathbf{y}_{t+1}\)</span> by <span class="math inline">\(\hat{\mathbf{x}}_{t+1}\)</span> where <span class="math inline">\(\hat{\mathbf{X}}_{t+1}\)</span> follows the dynamical law <span class="math inline">\(\Pr\left(\mathbf{X}_{t+1}(\omega)|\mathcal{F}_{t}\right)\)</span>.</p></li>
</ol>
<p>The cornerstone of this filtering procedure is given by the <strong>Kalman gain matrix</strong> <span class="math inline">\(\mathbf{K}_{t}\)</span>. The matrix is to form the projection so it has to be chosen to minimize the distance (as the projection) between the errors under the information <span class="math inline">\(\mathcal{F}_{t-1}\)</span>:
<span class="math display">\[\min_{\mathbf{K}_{t}\in\mathcal{F}_{t-1}}\left\Vert \underset{\mbox{error of estimate}}{\underbrace{\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t}}}-\mathbf{K}_{t}\underset{\mbox{error of prediction}}{\underbrace{\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)}}\right\Vert^2 _{\mathbb{P}}\]</span></p>
<p>The exact expression of <span class="math inline">\(\mathbf{K}_{t}\)</span> is given below.</p>
<div class="solution">
<p class="solution-begin">
Kalman gain matrix <span id="sol-start-109" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-109', 'sol-start-109')"></span>
</p>
<div id="sol-body-109" class="solution-body" style="display: none;">
<p>The mean and covariance matrix of dynamical law are
<span class="math display">\[\begin{align*}
\hat{\mathbf{x}}_{t}    &amp;=\mathbb{E}[\mathbf{X}_{t}(\omega)|\mathcal{F}_{t-1}]\\
\hat{\Sigma}_{t}    &amp;=\mathbb{E}\left[\left.(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})^{\top}\right|\mathcal{F}_{t-1}\right]\end{align*}
\]</span></p>
<p>Note that <span class="math display">\[\mathbb{E}\left[\left.\mathbf{B}\mathbf{X}_{t}(\omega)+\mathbf{W}_{2,t}(\omega)\right|\mathcal{F}_{t-1}\right]=\mathbf{B}\mathbb{E}\left[\left.\mathbf{X}_{t}(\omega)\right|\mathcal{F}_{t-1}\right]=\mathbf{B}\hat{\mathbf{x}}_{t}.\]</span>
Also
<span class="math display">\[\begin{align*}
\mbox{Variance of projection error}&amp;=   \mathbb{E}\left[\left.(\mathbf{y}-\mathbf{B}\hat{\mathbf{x}}_{t})(\mathbf{y}-\mathbf{B}\hat{\mathbf{x}}_{t})^{\top}\right|\mathcal{F}_{t-1}\right]\\
&amp;=  \mathbb{E}\left[\left.(\mathbf{B}\mathbf{X}_{t}(\omega)+\mathbf{W}_{2,t}(\omega)-\mathbf{B}\hat{\mathbf{x}}_{t})(\mathbf{B}\mathbf{X}_{t}(\omega)+\mathbf{W}_{2,t}(\omega)-\mathbf{B}\hat{\mathbf{x}}_{t})^{\top}\right|\mathcal{F}_{t-1}\right]\\
&amp;=  \mathbf{B}\underset{\hat{\Sigma}_{t}}{\underbrace{\mathbb{E}\left[\left.(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})^{\top}\right|\mathcal{F}_{t-1}\right]}}\mathbf{B}^{\top}+\underset{\mathbf{R}}{\underbrace{\mathbb{E}\left[\left.\mathbf{W}_{2,t}(\omega)\mathbf{W}_{2,t}(\omega)^{\top}\right|\mathcal{F}_{t-1}\right]}}\\
&amp;=  \mathbf{B}\hat{\Sigma}_{t}\mathbf{B}+\mathbf{R}.
\end{align*}\]</span></p>
<p>Now we can derive the Kalman gain matrix. Because the updating rule must follow the conditional expectation <span class="math display">\[\mathbb{E}[\mathbf{X}_{t}(\omega)|\mathbf{y}_{t}]=\mathbb{E}[\mathbf{X}_{t}(\omega)|\mathcal{F}_{t-1}]+\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right),\]</span>
or say <span class="math inline">\(\mathbb{E}[\mathbf{X}_{t}(\omega)|\mathbf{y}_{t}]=\hat{\mathbf{x}}_{t}+\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)\)</span>.
To obtain the “optimality”, <span class="math inline">\(\mathbf{K}_{t}\)</span> has to be chosen to minimize the error covariance of the projection under the information <span class="math inline">\(\mathcal{F}_{t-1}\)</span>. <span class="math display">\[\min_{\mathbf{K}_{t}}\left\Vert \mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t}-\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)\right\Vert _{\mathbb{P}}\]</span>
Recall that the minimization can be equivalently stated as the orthogonal condition. We have <span class="math display">\[\mbox{Cov}_{t-1}\left(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t}-\mathbf{K}_{t}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right),\:\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)=0,\]</span>
which implies <span class="math display">\[\underset{(*)}{\underbrace{\mbox{Cov}_{t-1}\left(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t},\:\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)}}-\mathbf{K}_{t}\underset{\mbox{Variance of projection error}}{\underbrace{\mbox{Cov}_{t-1}\left(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t},\:\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\right)}}=0.\]</span>
Note the <span class="math inline">\((*)\)</span> term can be simplified to<span class="math display">\[\begin{align*}&amp;\mbox{Cov}\left(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t},\:\mathbf{B}\mathbf{X}_{t}(\omega)+\mathbf{W}_{2,t}(\omega)-\mathbf{B}\hat{\mathbf{x}}_{t}\right)\\&amp;=\underset{\hat{\Sigma}_{t}}{\underbrace{\mathbb{E}\left[\left.(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})^{\top}\right|\mathcal{F}_{t-1}\right]}}\mathbf{B}^{\top}\\
&amp;+\mathbb{E}\left[\left.(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t})\mathbf{W}_{2,t}(\omega)^{\top}\right|\mathcal{F}_{t-1}\right]=\hat{\Sigma}_{t}\mathbf{B}^{\top}.\end{align*}\]</span>
Then substitute the simplified <span class="math inline">\((*)\)</span>
back into the orthogonal condition, we have <span class="math display">\[\hat{\Sigma}_{t}\mathbf{B}^{\top}-\mathbf{K}_{t}\left(\mathbf{B}\hat{\Sigma}_{t}\mathbf{B}+\mathbf{R}\right)=0,\]</span>
namely <span class="math inline">\(\mathbf{K}_{t}=\hat{\Sigma}_{t}\mathbf{B}^{\top}(\mathbf{B}\hat{\Sigma}_{t}\mathbf{B}+\mathbf{R})^{-1}\)</span>
which is the <strong>Kalman gain</strong>.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>The <strong>Kalman gain</strong> <span class="math inline">\(\mathbf{K}_t(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t})\)</span> basically is a correction term and it represents the amount by which to correct the propagated state estimate due to the measurement <span class="math inline">\(\mathbf{y}_{t}\)</span>. In other words, <strong>Kalman filter</strong> provides the best linear approximation of the observation vector <span class="math inline">\(\hat{\mathbf{y}}_{t}\)</span> when the projection <span class="math inline">\(\mathbf{K}_t\)</span> can minimize the distance between the error vectors <span class="math inline">\(\mathbf{X}_{t}(\omega)-\hat{\mathbf{x}}_{t}\)</span> and <span class="math inline">\(\mathbf{y}_{t}-\mathbf{B}\hat{\mathbf{x}}_{t}\)</span>.</p>
<div class="solution">
<p class="solution-begin">
code <span id="sol-start-110" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-110', 'sol-start-110')"></span>
</p>
<div id="sol-body-110" class="solution-body" style="display: none;">
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb139-1" data-line-number="1">Kalman.filter =<span class="st"> </span><span class="cf">function</span>(A, C, B, Q, R, y, xhat, Sigma){</a>
<a class="sourceLine" id="cb139-2" data-line-number="2">  <span class="co"># x[t+1] = Ax[t] + Cv[t]</span></a>
<a class="sourceLine" id="cb139-3" data-line-number="3">  <span class="co"># y[t] = Bx[t] + w[t]</span></a>
<a class="sourceLine" id="cb139-4" data-line-number="4">  <span class="co"># v[t] ~ N(0, Q)</span></a>
<a class="sourceLine" id="cb139-5" data-line-number="5">  <span class="co"># w[t] ~ N(0, R)</span></a>
<a class="sourceLine" id="cb139-6" data-line-number="6">  <span class="co"># y = observation value at [t]</span></a>
<a class="sourceLine" id="cb139-7" data-line-number="7">  <span class="co"># xhat = prior state estimates at [t-1]</span></a>
<a class="sourceLine" id="cb139-8" data-line-number="8">  <span class="co"># Sigma    = posteriror state variance at [t-1]</span></a>
<a class="sourceLine" id="cb139-9" data-line-number="9">  <span class="co"># prior and posterior</span></a>
<a class="sourceLine" id="cb139-10" data-line-number="10">  <span class="co"># xpri = xpri[t+1], xpost = xpost[t], </span></a>
<a class="sourceLine" id="cb139-11" data-line-number="11">  <span class="co"># Sigmapri = Sigmapri[t+1], Sigmapost = Sigmapost[t],</span></a>
<a class="sourceLine" id="cb139-12" data-line-number="12">  <span class="co"># K = Kalman gain at t</span></a>
<a class="sourceLine" id="cb139-13" data-line-number="13">  y =<span class="st"> </span><span class="kw">matrix</span>(y, <span class="dt">ncol=</span><span class="dv">1</span>); xhat =<span class="st"> </span><span class="kw">matrix</span>(xhat, <span class="dt">ncol=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb139-14" data-line-number="14">  <span class="co"># error/innovataion term</span></a>
<a class="sourceLine" id="cb139-15" data-line-number="15">  error =<span class="st"> </span>y <span class="op">-</span><span class="st"> </span>B <span class="op">%*%</span><span class="st"> </span>xhat</a>
<a class="sourceLine" id="cb139-16" data-line-number="16">  V.error =<span class="st"> </span>B <span class="op">%*%</span><span class="st"> </span>Sigma <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(B) <span class="op">+</span><span class="st"> </span>R</a>
<a class="sourceLine" id="cb139-17" data-line-number="17">  <span class="co"># Kalman gain</span></a>
<a class="sourceLine" id="cb139-18" data-line-number="18">  K =<span class="st"> </span>Sigma <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(B) <span class="op">%*%</span><span class="st"> </span><span class="kw">solve</span>(V.error)</a>
<a class="sourceLine" id="cb139-19" data-line-number="19">  <span class="co"># filtering step</span></a>
<a class="sourceLine" id="cb139-20" data-line-number="20">   <span class="cf">if</span>(<span class="op">!</span><span class="kw">any</span>(<span class="kw">is.na</span>(error))){</a>
<a class="sourceLine" id="cb139-21" data-line-number="21">    xpost =<span class="st"> </span>xhat <span class="op">+</span><span class="st"> </span>K <span class="op">%*%</span><span class="st"> </span>error</a>
<a class="sourceLine" id="cb139-22" data-line-number="22">    Sigmapost =<span class="st"> </span>Sigma <span class="op">-</span><span class="st"> </span>K <span class="op">%*%</span><span class="st"> </span>V.error <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(K) <span class="co"># variance</span></a>
<a class="sourceLine" id="cb139-23" data-line-number="23">  } <span class="cf">else</span>{</a>
<a class="sourceLine" id="cb139-24" data-line-number="24">    <span class="co"># NA</span></a>
<a class="sourceLine" id="cb139-25" data-line-number="25">    xpost =<span class="st"> </span>xhat</a>
<a class="sourceLine" id="cb139-26" data-line-number="26">    Sigmapost =<span class="st"> </span>Sigma</a>
<a class="sourceLine" id="cb139-27" data-line-number="27">  }</a>
<a class="sourceLine" id="cb139-28" data-line-number="28"> </a>
<a class="sourceLine" id="cb139-29" data-line-number="29">  <span class="co"># one-step-ahead forecast (prior in the next period)</span></a>
<a class="sourceLine" id="cb139-30" data-line-number="30">  xpred =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>xpost; Sigmapred =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>Sigmapost <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(A) <span class="op">+</span><span class="st"> </span><span class="kw">t</span>(C) <span class="op">%*%</span><span class="st"> </span>Q <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(C) </a>
<a class="sourceLine" id="cb139-31" data-line-number="31">  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">xpost=</span>xpost, <span class="dt">xpred=</span>xpred, <span class="dt">Sigmapost=</span>Sigmapost, <span class="dt">Sigmapred=</span>Sigmapred, <span class="dt">K=</span>K))</a>
<a class="sourceLine" id="cb139-32" data-line-number="32">}</a>
<a class="sourceLine" id="cb139-33" data-line-number="33"></a>
<a class="sourceLine" id="cb139-34" data-line-number="34"></a>
<a class="sourceLine" id="cb139-35" data-line-number="35"><span class="co">## Data and Model</span></a>
<a class="sourceLine" id="cb139-36" data-line-number="36">N      =<span class="st"> </span><span class="dv">30</span></a>
<a class="sourceLine" id="cb139-37" data-line-number="37"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb139-38" data-line-number="38">y =<span class="st"> </span><span class="kw">arima.sim</span>(<span class="dt">model=</span><span class="kw">list</span>(<span class="dt">ar=</span><span class="fl">0.5</span>, <span class="dt">ma=</span><span class="fl">0.2</span>),<span class="dt">n=</span>N, <span class="dt">innov=</span><span class="kw">rnorm</span>(N))</a>
<a class="sourceLine" id="cb139-39" data-line-number="39">y =<span class="st"> </span>y <span class="op">+</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="dv">1</span><span class="op">:</span>N</a>
<a class="sourceLine" id="cb139-40" data-line-number="40"></a>
<a class="sourceLine" id="cb139-41" data-line-number="41"><span class="co"># data y</span></a>
<a class="sourceLine" id="cb139-42" data-line-number="42"></a>
<a class="sourceLine" id="cb139-43" data-line-number="43">y =<span class="st"> </span><span class="kw">matrix</span>(y, <span class="dt">nrow=</span>N, <span class="dt">ncol=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb139-44" data-line-number="44"></a>
<a class="sourceLine" id="cb139-45" data-line-number="45"><span class="co"># system: x[t+1] = Ax[t] + Cv[t]</span></a>
<a class="sourceLine" id="cb139-46" data-line-number="46"><span class="co"># obs:    y[t]   = Bx[t] + e[t]</span></a>
<a class="sourceLine" id="cb139-47" data-line-number="47"><span class="co"># v[t] ~ NID(0,Q), e[t] ~ NID(0, R)</span></a>
<a class="sourceLine" id="cb139-48" data-line-number="48"></a>
<a class="sourceLine" id="cb139-49" data-line-number="49"></a>
<a class="sourceLine" id="cb139-50" data-line-number="50">A =<span class="st">  </span><span class="kw">matrix</span>(<span class="kw">c</span>(.<span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb139-51" data-line-number="51">               <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb139-52" data-line-number="52">               <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb139-53" data-line-number="53">               <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">nrow=</span><span class="dv">4</span>, <span class="dt">ncol=</span><span class="dv">4</span>, <span class="dt">byrow =</span> T)</a>
<a class="sourceLine" id="cb139-54" data-line-number="54">B =<span class="st">  </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="dt">nrow=</span><span class="dv">1</span>, <span class="dt">ncol=</span><span class="dv">4</span>, <span class="dt">byrow =</span> T)</a>
<a class="sourceLine" id="cb139-55" data-line-number="55">C =<span class="st">  </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb139-56" data-line-number="56">               <span class="fl">.5</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb139-57" data-line-number="57">               <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb139-58" data-line-number="58">               <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="dt">nrow=</span><span class="dv">4</span>, <span class="dt">ncol=</span><span class="dv">4</span>, <span class="dt">byrow =</span> T)</a>
<a class="sourceLine" id="cb139-59" data-line-number="59">Q =<span class="st"> </span><span class="kw">diag</span>(<span class="dv">1</span>, <span class="dv">4</span>); R =<span class="st"> </span><span class="kw">diag</span>(<span class="dv">0</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb139-60" data-line-number="60"></a>
<a class="sourceLine" id="cb139-61" data-line-number="61"><span class="co"># Recurisve filtering the data y</span></a>
<a class="sourceLine" id="cb139-62" data-line-number="62"></a>
<a class="sourceLine" id="cb139-63" data-line-number="63">recursive.kalman =<span class="st"> </span><span class="cf">function</span>(A, C, B, Q, R, y, xini, Sigmaini){</a>
<a class="sourceLine" id="cb139-64" data-line-number="64">  N =<span class="st"> </span><span class="kw">nrow</span>(y); err =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,N)</a>
<a class="sourceLine" id="cb139-65" data-line-number="65">  xpri =<span class="st"> </span>xpost  =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>N, <span class="dt">ncol=</span><span class="kw">nrow</span>(A))</a>
<a class="sourceLine" id="cb139-66" data-line-number="66">  Sigma =<span class="st"> </span>K =<span class="st"> </span><span class="kw">list</span>(<span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span><span class="kw">nrow</span>(A), <span class="dt">ncol=</span><span class="kw">nrow</span>(A)))</a>
<a class="sourceLine" id="cb139-67" data-line-number="67">  <span class="co"># Initial state</span></a>
<a class="sourceLine" id="cb139-68" data-line-number="68">  xpri[<span class="dv">1</span>, ] =<span class="st"> </span>xini</a>
<a class="sourceLine" id="cb139-69" data-line-number="69">  Sigma[[<span class="dv">1</span>]]    =<span class="st"> </span>Sigmaini</a>
<a class="sourceLine" id="cb139-70" data-line-number="70">  <span class="co"># Run the filter at every time t</span></a>
<a class="sourceLine" id="cb139-71" data-line-number="71">  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N){</a>
<a class="sourceLine" id="cb139-72" data-line-number="72">    temp     =<span class="st"> </span><span class="kw">Kalman.filter</span>(<span class="dt">A =</span> A, <span class="dt">C =</span> C, <span class="dt">B =</span> B, <span class="dt">Q =</span> Q, <span class="dt">R =</span> R,</a>
<a class="sourceLine" id="cb139-73" data-line-number="73">                         <span class="dt">y =</span> y[i, ], <span class="dt">xhat =</span> xpri[i, ], <span class="dt">Sigma =</span> Sigma[[i]])</a>
<a class="sourceLine" id="cb139-74" data-line-number="74">    xpost[i,] =<span class="st"> </span>temp<span class="op">$</span>xpost</a>
<a class="sourceLine" id="cb139-75" data-line-number="75">    <span class="cf">if</span>(i <span class="op">&lt;</span><span class="st"> </span>N)</a>
<a class="sourceLine" id="cb139-76" data-line-number="76">      xpri[i<span class="op">+</span><span class="dv">1</span>,] =<span class="st"> </span>temp<span class="op">$</span>xpred; Sigma[[i<span class="op">+</span><span class="dv">1</span>]] =<span class="st"> </span>temp<span class="op">$</span>Sigmapred; K[[i]] =<span class="st"> </span>temp<span class="op">$</span>K; </a>
<a class="sourceLine" id="cb139-77" data-line-number="77">  }</a>
<a class="sourceLine" id="cb139-78" data-line-number="78">  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">xpost=</span>xpost, <span class="dt">xpri=</span>xpri, <span class="dt">Sigma=</span>Sigma, <span class="dt">K=</span>K))</a>
<a class="sourceLine" id="cb139-79" data-line-number="79">}</a>
<a class="sourceLine" id="cb139-80" data-line-number="80"></a>
<a class="sourceLine" id="cb139-81" data-line-number="81"><span class="co">#</span></a>
<a class="sourceLine" id="cb139-82" data-line-number="82">result =<span class="st"> </span><span class="kw">recursive.kalman</span>(A, C, B, Q, R, y, <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), <span class="kw">diag</span>(<span class="fl">1.5</span>, <span class="dv">4</span>))</a></code></pre></div>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
</div>
</div></body></html>

<p style="text-align: center;">
<a href="ch-optApp.html"><button class="btn btn-default">Previous</button></a>
<a href="bibliography.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2021-03-20
</p>
</div>
</div>



</body>
</html>
