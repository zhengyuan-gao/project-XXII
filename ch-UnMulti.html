<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="13 Uncertainty in Multiple Dimensions | Project XXII" />
<meta property="og:type" content="book" />





<meta name="author" content="Zhengyuan Gao" />

<meta name="date" content="2020-07-30" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="13 Uncertainty in Multiple Dimensions | Project XXII">

<title>13 Uncertainty in Multiple Dimensions | Project XXII</title>

<link href="libs/tufte-css/tufte.css" rel="stylesheet" />
<link href="libs/tufte-css/envisioned.css" rel="stylesheet" />
<link href="libs/msmb-css/msmb.css" rel="stylesheet" />
<script>
function toggle_visibility(id1, id2) {
var e = document.getElementById(id1);
var f = document.getElementById(id2);

e.style.display = ((e.style.display!='none') ? 'none' : 'block');

if(f.classList.contains('fa-plus-square')) {
    f.classList.add('fa-minus-square')
    f.classList.remove('fa-plus-square')
} else {
    f.classList.add('fa-plus-square')
    f.classList.remove('fa-minus-square')
}

}
</script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding/datatables.js"></script>
<link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk/js/crosstalk.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }

code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>



</head>

<body>



<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul class="navbar">
<li class="msmb"><p class="title">Project XXII<p><p class="author">Zhengyuan Gao</p>
<li class="dropdown" style="float:right">
<a href="javascript:void(0)" class="dropbtn">&#x25BA; Chapters</a>
<div class="dropdown-content">
<a href="index.html">Preface</a>
<a href="part-i-inference-based-upon-logic-and-logical-computation.html">PART I: Inference Based upon Logic and Logical Computation</a>
<a href="sub-logic.html"><span class="toc-section-number">1</span> Logic</a>
<a href="sub-set-theory.html"><span class="toc-section-number">2</span> Set Theory</a>
<a href="sub-axioms.html"><span class="toc-section-number">3</span> Axioms</a>
<a href="sub-inferknow.html"><span class="toc-section-number">4</span> Inference and Knowledge</a>
<a href="sub-incomplete.html"><span class="toc-section-number">5</span> Incompleteness</a>
<a href="part-ii-infinitesimal-changes-and-their-consequences.html">PART II: Infinitesimal Changes and their Consequences</a>
<a href="sub-continuity.html"><span class="toc-section-number">6</span> Continuity</a>
<a href="sub-calculus.html"><span class="toc-section-number">7</span> Calculus</a>
<a href="ch-DE.html"><span class="toc-section-number">8</span> Differential Equations</a>
<a href="ch-CalUn.html"><span class="toc-section-number">9</span> Calculus under Uncertainty</a>
<a href="part-iii-emergence-of-abstract-interactions.html">PART III: Emergence of Abstract Interactions</a>
<a href="ch-vecMat.html"><span class="toc-section-number">10</span> Vector and Matrix</a>
<a href="ch-MatComp.html"><span class="toc-section-number">11</span> Matrix Computation</a>
<a href="ch-eigen.html"><span class="toc-section-number">12</span> Eigenvalues and Eigenvectors</a>
<a id="active-page" href="ch-UnMulti.html"><span class="toc-section-number">13</span> Uncertainty in Multiple Dimensions</a><ul class="toc-sections">
<li class="toc"><a href="#sub:MultiVar"> Multivariate Distributions</a></li>
<li class="toc"><a href="#sub:Markov"> Stochastic Process and Markov’s Principle</a></li>
</ul>
<a href="bibliography.html">Bibliography</a>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body><div id="ch:UnMulti" class="section level1">
<h1>
<span class="header-section-number">13</span> Uncertainty in Multiple Dimensions</h1>
<p>Our states are somehow dependent on and co-varies with our experiences, our epistemic and aesthetic norms, or more generally our changable roles in this world. On the other hand, the world has various paradigms, cultures, and belief systems that attribute different values to an individual’s state. An interference with the world could make one’s state an uncertain object.</p>
<p>In reality, when we consider the uncertainty of multiple <a href="sub-incomplete.html#sub:beyond2">random events</a> ahead that co-vary with each others, we tend to find out a law to characterize these events in a unified way. It turns out that vectors and matrices are indispensable to construct the characterizations for this joint venture.</p>
<div id="sub:MultiVar" class="section level2">
<h2>
<span class="header-section-number">13.1</span> Multivariate Distributions</h2>
<p>The previous examples in section <a href="ch-eigen.html#sub:matNorms">12.4</a> of data vectors didn’t involve any discussion about uncertainty. In reality, most datasets contain (more or less) some random features. When the uncertainty enters one’s concern, one may think that behind all the uncertain random events lie certain laws of probabilities. The event which one actually observes in a single instance could always be referred to a collection of events that might have happened. In other words, if one observes a single event, say <span class="math inline">\(x_t\)</span> at time <span class="math inline">\(t\)</span> from a random experiment, it is possible that at time <span class="math inline">\(t\)</span> the experiment can generate a set of possible outcomes. It is by chance that the value <span class="math inline">\(x_t\)</span> was generated, but if the experiment runs in a “parallel” world at time <span class="math inline">\(t\)</span>, the outcome <span class="math inline">\(x_t\)</span> could be different from the current one.<label for="tufte-sn-245" class="margin-toggle sidenote-number">245</label><input type="checkbox" id="tufte-sn-245" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">245</span> This argument relates to the <strong>many-worlds interpretation</strong> or <strong>many-minds interpretation</strong> in quantum mechanics where nondeterministic observations root in the system that “selects” a single value in the range of possible values.</span></p>
<p>By this argument, we can treat <span class="math inline">\(x_t\)</span> at every time <span class="math inline">\(t\)</span> as a realization of a random variable <span class="math inline">\(X_t(\omega)\)</span>. And since the time series data vector contains a series of realizations, say <span class="math inline">\([x_1,\dots,x_T]^\top\)</span>, we have to consider this whole vector to be a realization from a <em>random vector</em> <span class="math inline">\(\mathbf{X}(\omega) =[X_1(\omega),\dots,X_T(\omega)]^\top\)</span>.<label for="tufte-sn-246" class="margin-toggle sidenote-number">246</label><input type="checkbox" id="tufte-sn-246" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">246</span> The <span class="math inline">\(\omega\)</span>, as the use in chapter <a href="ch-CalUn.html#sub:conProb">9.4</a>, stands for a deeper invisible state that generates the outcome of the random variable.</span> Any attempt of understanding the underlying law of such data vectors becomes an attempt of understanding the underlying <a href="ch-CalUn.html#sub:conProb">joint probability</a> law of <span class="math inline">\(X_1(\omega),\dots,X_T(\omega)\)</span>, namely <span class="math inline">\(\mathbb{P}(X_1,\dots,X_T)\)</span>.</p>
<p>The <a href="ch-CalUn.html#sub:rv">probability distribution</a> refering to a joint probability of a <strong>random vector</strong> is called the <em>multivariate distribution</em>. We explore some properties of the <strong>multivariate distribution</strong> through one of the most important multivariate distributions, the <strong>multivariate normal (Gaussian) distribution</strong>. Recall that a <a href="ch-CalUn.html#sub:divRV">standard normal random variable</a> <span class="math inline">\(X\sim \mathcal{N}(0,1)\)</span> has the density function <span class="math inline">\(f(x)=\frac{1}{\sqrt{2\pi}}\exp(x^2)\)</span> for <span class="math inline">\(x\in\mathbb{R}\)</span>. Now let <span class="math inline">\(X_1(\omega),\dots X_T(\omega) \sim \mathcal{N}(0,1)\)</span> be <a href="ch-CalUn.html#sub:divRV">independent random variables</a>. The <strong>joint density</strong> of such a vector <span class="math inline">\(\mathbf{X}(\omega)=[X_1(\omega),\dots,X_T(\omega)]^\top\)</span> is
<span class="math display">\[
\begin{align*}
f(\mathbf{x})&amp;=\prod_{t=1}^{T}f(x_{t})=\frac{1}{(2\pi)^{T/2}}\exp\left\{ -\frac{1}{2}\sum_{t=1}^{T}x_{t}\right\} \\ &amp;=\frac{1}{(2\pi)^{T/2}}\exp\left\{ -\frac{1}{2}\mathbf{x}^{\top}\mathbf{x}\right\}.
\end{align*}
\]</span> which is the <em>standard multivariate normal density</em>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:mvn3D"></span>
<img src="fig/Part3/mvn3D.png" alt="Joint density of two independent normal random variables " width="100%"><!--
<p class="caption marginnote">-->Figure 13.1: Joint density of two independent normal random variables <!--</p>-->
<!--</div>--></span>
</p>
<p>The <a href="#sub:divRv">independence</a> property splits the joint density into product of individual densities <span class="math inline">\(f(\mathbf{x})=\prod_{t=1}^{T}f(x_{t})\)</span>. The assumption of independence may be violated in many situations. In the dynamical enviorments, an event <span class="math inline">\(X_{t}(\omega)\)</span> is followed by another event <span class="math inline">\(X_{t+1}(\omega)\)</span>, so it is natural to think that the <a href="sub-inferknow.html#sub:dyn">arrow of time</a> attaches some kind of the dependent chain between these two successive events. From another perspective, when one suspects that an interaction happened between <span class="math inline">\(Y(\omega)\)</span> and <span class="math inline">\(X(\omega)\)</span>, it is also natural to treat <span class="math inline">\(Y(\omega)\)</span> and <span class="math inline">\(X(\omega)\)</span> jointly as a vector, and to assume some degree of dependence between these two random elements.<label for="tufte-sn-247" class="margin-toggle sidenote-number">247</label><input type="checkbox" id="tufte-sn-247" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">247</span> Essentially, since we consider that the variables <span class="math inline">\(\{X_1(\omega),\dots,X_T(\omega)\}\)</span> or <span class="math inline">\(\{Y(\omega), X(\omega)\}\)</span> depend on the same invisible state <span class="math inline">\(\omega\)</span>, the dependence may come with the underlying features that are shared by all the variables generated by <span class="math inline">\(\omega\)</span>.</span> For example, the <a href="ch-CalUn.html#sub:conProb">conditional structure</a> <span class="math inline">\(Y|X\)</span> used in a <a href="ch-CalUn.html#sub:conProb">probabilistic causation</a> simply assumes the dependence exist.<label for="tufte-sn-248" class="margin-toggle sidenote-number">248</label><input type="checkbox" id="tufte-sn-248" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">248</span> If <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are independent, then the conditional probability <span class="math display">\[
\begin{align*}
\mathbb{P}(Y|X)= \frac{\mathbb{P}(X,Y)}{\mathbb{P}(X)}\\
=\frac{\mathbb{P}(X)\mathbb{P}(Y)}{\mathbb{P}(X)}=\mathbb{P}(Y)
\end{align*}
\]</span> will not reveal any convincing <a href="ch-CalUn.html#sub:conProb">causal relation</a> between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>.</span></p>
<p>The <a href="#sub:divRv">dependence</a> or <a href="#sub:divRv">independence</a> is a condition regarding the joint probability law. For any normal distributed random variable <span class="math inline">\(X\sim \mathcal{N}(\mu,\sigma^2)\)</span>, the whole distribution is characterized by the <a href="ch-CalUn.html#sub:ex">mean</a> <span class="math inline">\(\mu\)</span> and the <a href="ch-CalUn.html#sub:ex">variance</a> <span class="math inline">\(\sigma^2\)</span>, namely a first and a second order information criteron respectively. The <strong>covariance</strong> is a second order information criterion to depict the dependence between any two <a href="#sub:divRv">random variables</a>. Futhermore, the <strong>covariance matrix</strong> of any random vector <span class="math inline">\(\mathbf{X}(\omega)\)</span> gives a characterization of the <a href="#sub:divRv">dependence</a> amongst any two random variables <span class="math inline">\(X_i, X_j\)</span> of the vector <span class="math inline">\(\mathbf{X}(\omega)\)</span>.</p>
<ul>
<li>
<em>Covariance</em>, <em>correlation</em>, and <em>multivariate normal density</em> : Let <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> be random variables with means <span class="math inline">\(\mu_i\)</span> and <span class="math inline">\(\mu_j\)</span> and variance <span class="math inline">\(\sigma_i^2\)</span> and <span class="math inline">\(\sigma_j^2\)</span>. The <strong>covariance</strong> between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> is given by
<span class="math display">\[\mbox{Cov} (X_i , X_j) = \mbox{Cov} (X_j , X_i) =\mathbb{E}[(X_i-\mu_i)(X_j-\mu_j)],\]</span>
and the <strong>correlation</strong> between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> is defined by
<span class="math display">\[\rho_{ij} = \rho_{ji} = \frac{\mbox{Cov}(X_i,X_j)}{\sigma_i \sigma_j}.\]</span> The <strong>covariance matrix</strong> random vector <span class="math inline">\(\mathbf{X}(\omega)=[X_1, \dots, X_T]^\top\)</span> is given by
<span class="math display">\[
\begin{align*}
\mbox{Var}(\mathbf{X}(\omega))&amp;=\left[\begin{array}{cccc}
\mbox{Var}X_{1} &amp; \mbox{Cov}(X_{1},X_{2}) &amp; \cdots &amp; \mbox{Cov}(X_{1},X_{T})\\
\mbox{Cov}(X_{2},X_{1}) &amp; \mbox{Var}X_{2} &amp; \cdots &amp; \mbox{Cov}(X_{2},X_{T})\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots\\
\mbox{Cov}(X_{T},X_{1}) &amp; \mbox{Cov}(X_{T},X_{2}) &amp; \cdots &amp; \mbox{Var}X_{T}
\end{array}\right]
\\
&amp;=\left[\begin{array}{cccc}
\sigma_{1}^{2} &amp; \sigma_{12} &amp;  &amp; \sigma_{1T}\\
\sigma_{21} &amp; \sigma_{2}^{2} &amp;  &amp; \sigma_{2T}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
\sigma_{T1} &amp; \sigma_{T2} &amp; \cdots &amp; \sigma_{T}^{2}
\end{array}\right]=\left[\begin{array}{ccc}
\sigma_{1}^{2} &amp; \rho_{12}\sigma_{1}\sigma_{2} &amp; \dots\\
\rho_{12}\sigma_{1}\sigma_{2} &amp; \sigma_{2}^{2} &amp; \dots\\
\vdots &amp; \vdots &amp; \ddots
\end{array}\right]=\Sigma
\end{align*}
\]</span>
</li>
</ul>
<p>If any <span class="math inline">\(X_i\)</span> in <span class="math inline">\(\mathbf{X}(\omega)\)</span> is a normal random variable, then <span class="math inline">\(\mathbf{X}(\omega)\)</span> follows the <em>multivariate normal distribution</em> such that
<span class="math inline">\(\mathbf{X}(\omega)\sim\mathcal{N}\left(\mathbf{\mu},\:\Sigma\right)\)</span>, where <span class="math inline">\(\mathbf{\mu}=[\mu_1,\dots,\mu_T]^\top\)</span> is the <em>mean vector</em>, namely <span class="math inline">\(\mathbf{\mu}\)</span>, and <span class="math inline">\(\Sigma\)</span> is the <strong>covariance matrix</strong>. The density function of <span class="math inline">\(\mathbf{X}(\omega)\)</span> is given by <span class="math display">\[f(\mathbf{x})=\frac{1}{(2\pi)^{T/2}|\Sigma|^{1/2}}\exp\left\{ -\frac{1}{2}(\mathbf{x}-\mathbf{\mu})^{\top}\Sigma^{-1}(\mathbf{x}-\mathbf{\mu})\right\}.\]</span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:mvn"></span>
<img src="fig/Part3/mvn.gif" alt="Correlation (dependence pattern) changes of a bivariate normal density " width="100%"><!--
<p class="caption marginnote">-->Figure 13.2: Correlation (dependence pattern) changes of a bivariate normal density <!--</p>-->
<!--</div>--></span>
</p>
<p>It is easy to show that for any real vector <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span> and the <strong>random vector</strong> <span class="math inline">\(\mathbf{X}(\omega) \sim \mathcal{N}(\mathbf{\mu}, \Sigma)\)</span>, the expectation and <strong>(co)variance</strong> operation for a <a href="ch-vecMat.html#sub:linearity">linear/affine transformation</a> have the following results <span class="math inline">\(\mathbb{E}[\mathbf{a}^\top \mathbf{X}(\omega) + \mathbf{b}]=\mathbf{a}^\top \mathbf{\mu} + \mathbf{b}\)</span>, and <span class="math inline">\(\mbox{Var}(\mathbf{a}^\top \mathbf{X}(\omega) + \mathbf{b})=\mathbf{a}^\top \Sigma \mathbf{a}\)</span>. These results are analogous to those in the scalar cases. One can extend the results for any real matrix <span class="math inline">\(\mathbf{A}\)</span>: <span class="math inline">\(\mathbb{E}[\mathbf{A} \mathbf{X}(\omega)]=\mathbf{A} \mathbf{\mu}\)</span>, and <span class="math inline">\(\mbox{Var}(\mathbf{A} \mathbf{X}(\omega))=\mathbf{A} \Sigma\mathbf{A}^\top\)</span>.
These relations indicate that it is possible to construct any <strong>multivariate normal distribution</strong> by the <strong>standard</strong> one. The idea is to decompose the covariance matrix as a product, i.e.
<span class="math inline">\(\Sigma=\mathbf{A}\mathbf{A}^\top\)</span>. With this decomposition, we can represent any <span class="math inline">\(\mathbf{X}(\omega) \sim \mathcal{N}(\mathbf{\mu}, \Sigma)\)</span> by <span class="math display">\[ \mathbf{A}\mathbf{W}(\omega) + \mathbf{\mu},\,\, \mbox{ where } \mathbf{W}(\omega) \sim \mathcal{N}(0, \mathbf{I})\]</span>
where <span class="math inline">\(\mathbf{W}(\omega)\)</span> is the <strong>standard multivariate normal random vector</strong>. The decomposition <span class="math inline">\(\Sigma=\mathbf{A}\mathbf{A}^\top\)</span> utilizes several properties of the <strong>covariance matrix</strong>. We are going to examine these properties one by one.</p>
<p>One important property of the variance is that <span class="math inline">\(\mbox{Var}(X)&gt;0\)</span> for any random variable <span class="math inline">\(X\)</span>. The covariance of any two random variables, however, can be either positive or negative, because the dependence can comes from a positive or negative relation. So we cannot say that all entries of the covariance matrix <span class="math inline">\(\mbox{Var}(\mathbf{X}(\omega))\)</span> are positive. But <span class="math inline">\(\mathbf{a}^\top \Sigma \mathbf{a}\)</span> must be non-negative for any real non-zero vector <span class="math inline">\(\mathbf{a}\)</span>.<label for="tufte-sn-249" class="margin-toggle sidenote-number">249</label><input type="checkbox" id="tufte-sn-249" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">249</span> Otherwise, we may have <span class="math inline">\(a^2_i\mbox{Var}(X_i)&lt;0\)</span> at some entry <span class="math inline">\(X_i\)</span>. As <span class="math inline">\(a^2_i\)</span> is non-negative, we have <span class="math inline">\(\mbox{Var}(X_i)&lt;0\)</span> for a random variable <span class="math inline">\(X_i\)</span>. This resutl contradicts with the property of the variance operator.</span> Any matrix satisfying <span class="math inline">\(\mathbf{a}^\top \Sigma \mathbf{a}\geq 0\)</span> for non-zero real vector <span class="math inline">\(\mathbf{a}\)</span> is called <em>positive (semi-)definite matrix</em>.</p>
<p>The <a href="ch-eigen.html#sub:det">eigenvalues</a> of the <strong>positive (semi-)definite matrix</strong> are real postive.
Let <span class="math inline">\(\lambda\)</span> be an eigenvalue of <span class="math inline">\(\Sigma\)</span>, and let <span class="math inline">\(\Sigma \mathbf{v}=\lambda \mathbf{v}\)</span> for some non-zero <a href="ch-eigen.html#sub:det">eigenvector</a> <span class="math inline">\(\mathbf{v}\)</span>. It is obvious to see that <span class="math display">\[\mathbf{v}^{\top}\Sigma\mathbf{v}=\mathbf{v}^{\top}\lambda \mathbf{v}=\lambda\|\mathbf{v}\|^{2}.\]</span> As <span class="math inline">\(\mathbf{v}^{\top}\Sigma\mathbf{v}\geq 0\)</span> by the definition of <strong>positive (semi-)definite matrix</strong>, <span class="math inline">\(\lambda\|\mathbf{v}\|^{2}\geq 0\)</span>. Because <span class="math inline">\(\|\mathbf{v}\|^{2}\geq 0\)</span>, we can see that <span class="math inline">\(\lambda\)</span> is a real positive number.</p>
<p>In addition, we can see that the <strong>covariance matrix</strong> <span class="math inline">\(\Sigma\)</span> is <a href="#sub:">symmetric</a>. Recall that <a href="ch-eigen.html#sub:diag">eigenvalue-eigenvector decomposition</a> <span class="math inline">\(\Sigma=\mathbf{V}\Lambda \mathbf{V}^{-1}\)</span> where <span class="math inline">\(\Lambda\)</span> is the <a href="ch-eigen.html#sub:diag">diagonal eigenvalue matrix</a>. By the <a href="#sub:">transposed operations</a>, we have <span class="math display">\[\Sigma^\top=(\mathbf{V}\Lambda \mathbf{V}^{-1})^\top = (\mathbf{V}^{-1})^\top \Lambda (\mathbf{V})^\top\]</span> where <span class="math inline">\(\Lambda=\Lambda^\top\)</span> by the diagonal property. The symmetry of <span class="math inline">\(\Sigma\)</span> gives
<span class="math inline">\(\mathbf{V}\Lambda \mathbf{V}^{-1}=(\mathbf{V}^{-1})^\top \Lambda (\mathbf{V})^\top\)</span> or say
<span class="math display">\[\Lambda = \left(\mathbf{V}^{-1}(\mathbf{V}^{-1})^\top\right) \Lambda \left(\mathbf{V}^\top\mathbf{V}\right).\]</span> One can infer that <span class="math inline">\(\mathbf{V}^\top\mathbf{V}=\mathbf{I}\)</span> or <span class="math inline">\(\mathbf{V}^\top =\mathbf{V}^{-1}\)</span>. In other words, for a symmetric square matrix of real-valued entries, the eigenvector matrix <span class="math inline">\(\mathbf{V}\)</span> are <em>orthonormal</em>
<span class="math inline">\(\mathbf{V}^\top\mathbf{V}=\mathbf{I}\)</span>, namely all vectors in <span class="math inline">\(\mathbf{V}\)</span> are mutually orthogonal and all of unit length.<label for="tufte-sn-250" class="margin-toggle sidenote-number">250</label><input type="checkbox" id="tufte-sn-250" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">250</span> Any two vectors <span class="math inline">\(\mathbf{v}_i, \mathbf{v}_j\)</span> in the matrix <span class="math inline">\(\mathbf{V}\)</span> are <a href="ch-vecMat.html#sub:vec">orthogonal</a> <span class="math inline">\(\mathbf{v}_i^\top \mathbf{v}_j=\left\langle \mathbf{v}_{i},\mathbf{v}_{j}\right\rangle =0\)</span>. The norm of the any vector in <span class="math inline">\(\mathbf{V}\)</span> is one: <span class="math inline">\(\|\mathbf{v}_i\|=\sqrt{\left\langle \mathbf{v}_{i},\mathbf{v}_{i}\right\rangle}=1\)</span>.</span></p>
<p>Now we can rewrite <span class="math inline">\(\Sigma=\mathbf{V}\Lambda \mathbf{V}^{-1}\)</span> as <span class="math inline">\(\Sigma=\mathbf{V}\Lambda \mathbf{V}^\top\)</span>. Since <span class="math inline">\(\Lambda\)</span> is a diagonal matrix with real positive entries <span class="math inline">\(\{\lambda_i\}\)</span>, we can represent <span class="math inline">\(\Lambda\)</span> by <span class="math inline">\(\Lambda=\mathbf{S}\mathbf{S}\)</span> where <span class="math inline">\(\mathbf{S}\)</span> is also a diagonal matrix with real positive entries <span class="math inline">\(\{\sqrt{\lambda_i}\}\)</span>. Then the covariance matrix becomes
<span class="math display">\[\Sigma=\mathbf{V}\mathbf{S}\mathbf{S}^\top \mathbf{V}^\top=(\mathbf{V}\mathbf{S})(\mathbf{V}\mathbf{S})^\top.\]</span> Let’s denote <span class="math inline">\(\mathbf{V}\mathbf{S}\)</span> by <span class="math inline">\(\mathbf{A}\)</span>. We have the desired result <span class="math inline">\(\Sigma=\mathbf{A}\mathbf{A}^\top\)</span>. The result is called the <em>Cholesky decomposition</em>. It says that any symmetric <strong>positive semi-definite</strong> matrix <span class="math inline">\(\Sigma\)</span> can be decomposed as a product of one matrix <span class="math inline">\(\mathbf{A}\)</span> and its transpose <span class="math inline">\(\mathbf{A}^\top\)</span>.<label for="tufte-sn-251" class="margin-toggle sidenote-number">251</label><input type="checkbox" id="tufte-sn-251" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">251</span> Such a matrix <span class="math inline">\(\mathbf{A}\)</span> is analogous to the matrix version square root of <span class="math inline">\(\Sigma\)</span>. Unlike the real numbers, whose expression of a square root is unique. The “square root” of a matrix does not have a unique representation. For example, if one selects an <strong>orthonormal</strong> matrix <span class="math inline">\(\mathbf{U}\)</span> such that <span class="math inline">\(\mathbf{U}\mathbf{U}^\top = \mathbf{I}\)</span>, then <span class="math display">\[\Sigma = \mathbf{A}\mathbf{A}^\top=\mathbf{A}\mathbf{I}\mathbf{A}^\top=(\mathbf{A}\mathbf{U})(\mathbf{A}\mathbf{U})^\top\]</span> is also a valid representation for <span class="math inline">\(\Sigma\)</span>.</span></p>
<p>The multivariate normal distribution of <span class="math inline">\(\mathbf{X}(\omega)\)</span> gives a full description about the dependent structure amongst all normal random variables in the random vector <span class="math inline">\(\mathbf{X}(\omega)\)</span>. With the joint probability law, we can derive other useful dependent structures. For example, the <a href="ch-CalUn.html#sub:conProb">conditional probability</a> can induce the <a href="ch-CalUn.html#sub:conProb">probabilistic causal relation</a> as the conditional probability law implicitly treats the conditions as the (probabilitistic) cause.</p>
<p>The calculation of multivariate conditional probability is non-trivial. Take a <span class="math inline">\(2N\)</span>-dimensional <strong>multivariate normal random vector</strong> as an example. By splitting the vector into two subvectors, we have the following expression for the joint density
<span class="math display">\[
\begin{bmatrix}
 \mathbf{Y}(\omega) \\
 \mathbf{X}(\omega)
\end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix}
 \mathbf{\mu_y} \\
 \mathbf{\mu_x}
\end{bmatrix}
,  \begin{bmatrix}
 \Sigma_{11} &amp; \Sigma_{12} \\
 \Sigma_{21} &amp; \Sigma_{22}
\end{bmatrix}
 \right)
\]</span>
where <span class="math inline">\(\Sigma_{11}\)</span>, <span class="math inline">\(\Sigma_{22}\)</span> are the <strong>covariance matrices</strong> of the random vectors <span class="math inline">\(\mathbf{Y}(\omega)\)</span> and <span class="math inline">\(\mathbf{X}(\omega)\)</span> respectively, and <span class="math inline">\(\Sigma_{12} = \Sigma_{21}\)</span> is <span class="math inline">\(\mbox{Cov}(\mathbf{Y}(\omega), \mathbf{X}(\omega))\)</span>.</p>
<p>If we want to calculate the conditional density <span class="math inline">\((\mathbf{Y}|\mathbf{X})(\omega)\)</span>, then in principle we need to compute <span class="math inline">\(f(\mathbf{y}|\mathbf{x})=f(\mathbf{y}, \mathbf{x})/f(\mathbf{x})\)</span>.<label for="tufte-sn-252" class="margin-toggle sidenote-number">252</label><input type="checkbox" id="tufte-sn-252" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">252</span> The full expression of the joint density function <span class="math inline">\(f(\mathbf{y}, \mathbf{x})\)</span> contains the term
<span class="math display">\[  \left[\begin{array}{c}
\mathbf{y}-\mathbf{\mu_{y}}\\
\mathbf{x}-\mathbf{\mu_{x}}
\end{array}\right]^\top\left[\begin{array}{cc}
\Sigma_{11} &amp; \Sigma_{12}\\
\Sigma_{21} &amp; \Sigma_{22}
\end{array}\right]^{-1}\left[\begin{array}{c}
\mathbf{y}-\mathbf{\mu_{y}}\\
\mathbf{x}-\mathbf{\mu_{x}}
\end{array}\right]
\]</span>
which requires to evaluate the inversion of the block matrices.</span> Rather than fully involve in the derivation of the conditional density, we give the direct
results of <strong>conditional vector mean</strong> and <strong>conditional covariance matrix</strong>, and then consider why such results make sense.
<span class="math display">\[
\begin{align*}
\mathbb{E}[\mathbf{Y}(\omega) | \mathbf{X}(\omega)=\mathbf{x}]&amp;=
\mathbf{\mu_y} + \Sigma_{12} \Sigma_{22}^{-1}
\left(
 \mathbf{x} - \mathbf{\mu_x}
\right),\\
\mbox{Var}[\mathbf{Y}(\omega) | \mathbf{X}(\omega) =\mathbf{x}]
&amp;=
\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}.
\end{align*}
\]</span>
The results show that the conditioning vector <span class="math inline">\(\mathbf{x}\)</span> will adjust the first and the second moment information criteria of dependent vector <span class="math inline">\(\mathbf{Y}(\omega)\)</span>.</p>
<p>This conditional covariance matrix <span class="math inline">\(\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1}\Sigma_{21}\)</span> is the <a href="ch-MatComp.html#sub:LU">Schur complement</a> of <span class="math inline">\(\Sigma_{22}\)</span>. Similar to the role in the <a href="ch-MatComp.html#sub:LU">block LU factorization</a>, such a term is to eliminate the covariance blocks corresponding to the variables being conditioned upon. The main instrument <span class="math inline">\(\Sigma_{12} \Sigma_{22}^{-1}\)</span> is to eliminate the dependence caused by <span class="math inline">\(\mathbf{X}(\omega)\)</span>.</p>
<div class="solution">
<p class="solution-begin">
Proof of the elimination <span id="sol-start-174" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-174', 'sol-start-174')"></span>
</p>
<div id="sol-body-174" class="solution-body" style="display: none;">
<p>First, we shows that the covariance <span class="math inline">\(\mbox{Var}[\mathbf{Y}(\omega) | \mathbf{X}(\omega)]=\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}\)</span> is the Schur complement. That is to show <span class="math inline">\(\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1}\Sigma_{21}= \mbox{Var}[\mathbf{Y}(\omega)+\Sigma_{12} \Sigma_{22}^{-1}\mathbf{X}(\omega)]\)</span>. To see this,
<span class="math display">\[
\begin{align*}
&amp;\mbox{Var}[\mathbf{Y}(\omega)+\Sigma_{12} \Sigma_{22}^{-1}\mathbf{X}(\omega)]\\
=&amp;\mbox{Var}(\mathbf{Y}(\omega))+\Sigma_{12} \Sigma_{22}^{-1}\mbox{Var}(\mathbf{X}(\omega))(\Sigma_{12} \Sigma_{22}^{-1})^{\top}\\
&amp;+\Sigma_{12} \Sigma_{22}^{-1}\mbox{Cov}(\mathbf{Y}(\omega),\,\mathbf{X}(\omega))+\mbox{Cov}(\mathbf{X}(\omega),\,\mathbf{Y}(\omega))(\Sigma_{12} \Sigma_{22}^{-1})^{\top}\\
=&amp; \Sigma_{11}+\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{22}\Sigma_{22}^{-1}\Sigma_{21}-2\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} \\
=&amp; \Sigma_{11}+\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}-2\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} \\
=&amp; \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1}\Sigma_{21}
\end{align*}
\]</span>
On the other hand, we can see that
<span class="math display">\[
\begin{align*}\mbox{Cov}(\mathbf{Y}(\omega)+\Sigma_{12}\Sigma_{22}^{-1}\mathbf{X}(\omega),\:\mathbf{X}(\omega)) &amp; =\mbox{Cov}(\mathbf{Y}(\omega),\:\mathbf{X}(\omega))+\mbox{Cov}(\Sigma_{12}\Sigma_{22}^{-1}\mathbf{X}(\omega),\:\mathbf{X}(\omega))\\
 &amp; =\Sigma_{12}+\Sigma_{12}\Sigma_{22}^{-1}\mbox{Cov}(\mathbf{X}(\omega),\:\mathbf{X}(\omega))\\
 &amp; =\Sigma_{12}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{22}\\
 &amp; =0.
\end{align*}
\]</span>
So the term <span class="math inline">\(\mathbf{Y}(\omega)+\Sigma_{12}\Sigma_{22}^{-1}\mathbf{X}(\omega)\)</span> gets rid of the dependence of conditional vector <span class="math inline">\(\mathbf{X}(\omega)\)</span> by eliminating the effects of <span class="math inline">\(\Sigma_{22}\)</span> and <span class="math inline">\(\Sigma_{12}=\Sigma_{21}\)</span>.</p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>The conditional results of mean vector and covariance matrix serve the foundation of estimating a dynamical system adaptively. We will see in [?]. For now, a quick application is to use the conditional results to recover the joint density.<label for="tufte-sn-253" class="margin-toggle sidenote-number">253</label><input type="checkbox" id="tufte-sn-253" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">253</span> Any two dependent random variables <span class="math inline">\(X_2\)</span> and <span class="math inline">\(X_1\)</span> have the expression
<span class="math display">\[\mathbb{P}(X_2, X_1)=\mathbb{P}(X_2\,|\,X_1)\mathbb{P}(X_1)\]</span>
which tells how to compute the joint by the conditionals.</span></p>
<div class="solution">
<p class="solution-begin">
Simulate the bivariate normal random vector <span id="sol-start-175" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-175', 'sol-start-175')"></span>
</p>
<div id="sol-body-175" class="solution-body" style="display: none;">
<p>If <span class="math inline">\(X_2\sim \mathcal{N}(\mu_2,\sigma_2^2)\)</span> and <span class="math inline">\(X_1 \sim \mathcal{N}(\mu_1,\sigma_1^2)\)</span> are normal random variables, then <span class="math inline">\(\mathbb{P}(X_2, X_1)=\mathbb{P}(X_2\,|\,X_1)\mathbb{P}(X_1)\)</span> becomes
<span class="math display">\[\mathcal{N}(\mu_\mbox{joint},\Sigma_\mbox{joint})=\mathcal{N}\left(\mu_2 + \frac{\rho\sigma_1}{\sigma_{2}}(x_1 - \mu_1),\, \sigma^2_{1}(1 - \rho^2)\right)\mathcal{N}(\mu_1,\sigma_1^2).\]</span>
Note that <span class="math inline">\(\Sigma_{11}=\sigma_1^2\)</span>, <span class="math inline">\(\Sigma_{12}=\Sigma_{21}=\rho\sigma_1\sigma_2\)</span>, <span class="math inline">\(\Sigma_{22}=\sigma_2^2\)</span>. We simplify the previous conditional mean and covariance expressions to the scalar case: the conditional mean
<span class="math display">\[\mu_2 + \rho\sigma_1\sigma_2 \sigma_{2}^{-2}(x_1 - \mu_1)=\mu_2 + \frac{\rho\sigma_1}{\sigma_{2}}(x_1 - \mu_1),\]</span>
and the conditional variance
<span class="math display">\[\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1}\Sigma_{21}= \sigma^2_{1} - (\rho\sigma_{1}\sigma_{2})^2 \sigma_{2}^{-2} =\sigma^2_{1}(1 - \rho^2).\]</span></p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb38-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb38-2" data-line-number="2">rbvn=<span class="cf">function</span> (n, m1, s1, m2, s2, rho){</a>
<a class="sourceLine" id="cb38-3" data-line-number="3">     X1 =<span class="st"> </span><span class="kw">rnorm</span>(n, mu1, s1)</a>
<a class="sourceLine" id="cb38-4" data-line-number="4">     X2 =<span class="st"> </span><span class="kw">rnorm</span>(n, </a>
<a class="sourceLine" id="cb38-5" data-line-number="5">                mu2 <span class="op">+</span><span class="st"> </span>(s1<span class="op">/</span>s2) <span class="op">*</span><span class="st"> </span>rho <span class="op">*</span>(X1 <span class="op">-</span><span class="st"> </span>mu1), </a>
<a class="sourceLine" id="cb38-6" data-line-number="6">                <span class="kw">sqrt</span>((<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>rho<span class="op">^</span><span class="dv">2</span>)<span class="op">*</span>s1<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb38-7" data-line-number="7">     <span class="kw">cbind</span>(X1, X2)}</a>
<a class="sourceLine" id="cb38-8" data-line-number="8">mu1 =<span class="st"> </span><span class="dv">1</span>; sigma1 =<span class="st"> </span><span class="dv">2</span>; mu2 =<span class="st"> </span><span class="dv">4</span>; sigma2 =<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb38-9" data-line-number="9">rho =<span class="st"> </span><span class="fl">-0.2</span>; n =<span class="st"> </span><span class="dv">1000</span>;</a>
<a class="sourceLine" id="cb38-10" data-line-number="10">bvn =<span class="st"> </span><span class="kw">rbvn</span>(n,mu1,sigma1,mu2,sigma2,rho)</a>
<a class="sourceLine" id="cb38-11" data-line-number="11"></a>
<a class="sourceLine" id="cb38-12" data-line-number="12"></a>
<a class="sourceLine" id="cb38-13" data-line-number="13"><span class="co"># Plot</span></a>
<a class="sourceLine" id="cb38-14" data-line-number="14"></a>
<a class="sourceLine" id="cb38-15" data-line-number="15">bvn =<span class="st"> </span><span class="kw">data.frame</span>(bvn)</a>
<a class="sourceLine" id="cb38-16" data-line-number="16"></a>
<a class="sourceLine" id="cb38-17" data-line-number="17"><span class="kw">library</span>(ggplot2); <span class="kw">library</span>(gridExtra)</a>
<a class="sourceLine" id="cb38-18" data-line-number="18">htop =<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data=</span>bvn, <span class="kw">aes</span>(<span class="dt">x=</span>X1)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb38-19" data-line-number="19"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y=</span>..density..), <span class="dt">fill=</span><span class="st">"skyblue"</span>, <span class="dt">bins=</span><span class="dv">200</span>, <span class="dt">alpha=</span><span class="fl">0.8</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb38-20" data-line-number="20"><span class="st">  </span><span class="kw">stat_density</span>(<span class="dt">colour =</span> <span class="st">"blue"</span>, <span class="dt">geom=</span><span class="st">"line"</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_minimal</span>() <span class="op">+</span><span class="st"> </span><span class="kw">theme</span>(<span class="dt">axis.title.x =</span> <span class="kw">element_blank</span>())</a>
<a class="sourceLine" id="cb38-21" data-line-number="21"></a>
<a class="sourceLine" id="cb38-22" data-line-number="22">blank =<span class="st"> </span><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">colour=</span><span class="st">"white"</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb38-23" data-line-number="23"><span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.ticks=</span><span class="kw">element_blank</span>(), <span class="dt">panel.background=</span><span class="kw">element_blank</span>(), <span class="dt">panel.grid=</span><span class="kw">element_blank</span>(),</a>
<a class="sourceLine" id="cb38-24" data-line-number="24">        <span class="dt">axis.text.x=</span><span class="kw">element_blank</span>(), <span class="dt">axis.text.y=</span><span class="kw">element_blank</span>(), <span class="dt">axis.title.x=</span><span class="kw">element_blank</span>(), <span class="dt">axis.title.y=</span><span class="kw">element_blank</span>())</a>
<a class="sourceLine" id="cb38-25" data-line-number="25"></a>
<a class="sourceLine" id="cb38-26" data-line-number="26">scatter =<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data=</span>bvn, <span class="kw">aes</span>(<span class="dt">x=</span>X1, <span class="dt">y=</span>X2)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb38-27" data-line-number="27"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span><span class="st">"grey"</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb38-28" data-line-number="28"><span class="st">    </span><span class="kw">stat_ellipse</span>(<span class="dt">type =</span> <span class="st">"norm"</span>, <span class="dt">linetype =</span> <span class="dv">2</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb38-29" data-line-number="29"><span class="st">    </span><span class="kw">annotate</span>(<span class="dt">geom=</span><span class="st">"text"</span>, <span class="dt">x=</span><span class="dv">6</span>, <span class="dt">y=</span><span class="dv">8</span>, <span class="dt">label=</span><span class="st">"rho=-0.2"</span>,</a>
<a class="sourceLine" id="cb38-30" data-line-number="30">              <span class="dt">color=</span><span class="st">"red"</span>) <span class="op">+</span><span class="st">  </span><span class="kw">theme_minimal</span>()</a>
<a class="sourceLine" id="cb38-31" data-line-number="31"></a>
<a class="sourceLine" id="cb38-32" data-line-number="32">hright =<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data=</span>bvn, <span class="kw">aes</span>(<span class="dt">x=</span>X2)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb38-33" data-line-number="33"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y=</span>..density..), <span class="dt">fill=</span><span class="st">"skyblue"</span>, <span class="dt">bins=</span><span class="dv">200</span>, <span class="dt">alpha=</span><span class="fl">0.8</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb38-34" data-line-number="34"><span class="st">  </span><span class="kw">stat_density</span>(<span class="dt">colour =</span> <span class="st">"blue"</span>, <span class="dt">geom=</span><span class="st">"line"</span>) <span class="op">+</span><span class="st">  </span><span class="kw">theme_minimal</span>() <span class="op">+</span><span class="st"> </span><span class="kw">theme</span>(<span class="dt">axis.title.y =</span> <span class="kw">element_blank</span>())</a>
<a class="sourceLine" id="cb38-35" data-line-number="35"></a>
<a class="sourceLine" id="cb38-36" data-line-number="36"><span class="kw">grid.arrange</span>(htop, blank, scatter, hright, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">nrow=</span><span class="dv">2</span>, <span class="dt">widths=</span><span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">1</span>), <span class="dt">heights=</span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">4</span>))</a>
<a class="sourceLine" id="cb38-37" data-line-number="37"></a>
<a class="sourceLine" id="cb38-38" data-line-number="38"></a>
<a class="sourceLine" id="cb38-39" data-line-number="39"><span class="co">### 3D plot</span></a>
<a class="sourceLine" id="cb38-40" data-line-number="40">x =<span class="st"> </span><span class="kw">seq</span>( <span class="dv">-3</span>, <span class="dv">3</span>, <span class="dt">length=</span><span class="dv">25</span> ); y =<span class="st"> </span><span class="kw">seq</span>( <span class="dv">-3</span>, <span class="dv">3</span>, <span class="dt">length=</span><span class="dv">25</span> )</a>
<a class="sourceLine" id="cb38-41" data-line-number="41">z =<span class="st"> </span><span class="kw">outer</span>( x, y, <span class="cf">function</span>(x,y) <span class="kw">dnorm</span>(x,<span class="dv">0</span>,<span class="fl">0.75</span>)<span class="op">*</span><span class="kw">dnorm</span>(y,<span class="dv">0</span>,<span class="fl">0.5</span>) )</a>
<a class="sourceLine" id="cb38-42" data-line-number="42">zl =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">4</span><span class="op">*</span><span class="kw">max</span>(z))</a>
<a class="sourceLine" id="cb38-43" data-line-number="43"><span class="co">## persp plot</span></a>
<a class="sourceLine" id="cb38-44" data-line-number="44">trmat =<span class="st"> </span><span class="kw">persp</span>(x,y,z, <span class="dt">theta=</span><span class="dv">120</span>, <span class="dt">zlim=</span>zl, <span class="dt">box=</span><span class="ot">FALSE</span>, <span class="dt">shade=</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb38-45" data-line-number="45"><span class="co">## marginal for x</span></a>
<a class="sourceLine" id="cb38-46" data-line-number="46"><span class="kw">lines</span>( <span class="kw">trans3d</span>( <span class="kw">seq</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="dt">length=</span><span class="dv">100</span>), <span class="dv">-3</span>, <span class="kw">dnorm</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="dt">length=</span><span class="dv">100</span>),<span class="dv">0</span>,.<span class="dv">75</span>), </a>
<a class="sourceLine" id="cb38-47" data-line-number="47">    trmat), <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">'blue'</span> )</a>
<a class="sourceLine" id="cb38-48" data-line-number="48"><span class="co">## marginal for y</span></a>
<a class="sourceLine" id="cb38-49" data-line-number="49"><span class="kw">lines</span>( <span class="kw">trans3d</span>( <span class="dv">-3</span>, <span class="kw">seq</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="dt">length=</span><span class="dv">100</span>), <span class="kw">dnorm</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="dt">length=</span><span class="dv">100</span>),<span class="dv">0</span>,<span class="fl">0.5</span>), </a>
<a class="sourceLine" id="cb38-50" data-line-number="50">    trmat), <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">'blue'</span> )</a></code></pre></div>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
</div>
<div id="sub:Markov" class="section level2">
<h2>
<span class="header-section-number">13.2</span> Stochastic Process and Markov’s Principle</h2>
<p>The sequence of random variables <span class="math inline">\(X_{1}(\omega),\dots X_{t}(\omega)\)</span> refers to a <strong>stochastic process</strong> if one wants to emphasize that the index of the sequence indicates the successive steps.</p>
<ul>
<li>(<em>Filtered space</em>, <em>stochastic process</em>, <em>state space</em>) <em>Filtered space</em> is <span class="math inline">\((\Omega,\mathcal{F},\{\mathcal{F}_{t}\},P)\)</span> where <span class="math inline">\((\Omega,\mathcal{F},P)\)</span> is a <a href="sub-incomplete.html#sub:beyond2">probability space</a>, <span class="math inline">\(\{\mathcal{F}_{t}:t\geq0\}\)</span> is called the <em>filteration</em>, that is, an increasing family of <span class="math inline">\(\sigma\)</span>-algebras of <span class="math inline">\(\mathcal{F}\)</span> such that
<span class="math display">\[\mathcal{F}_{s}\subseteq\mathcal{F}_{t}\cdots\subseteq\mathcal{F}\]</span>
for all <span class="math inline">\(s&lt;t\)</span>. A <strong>stochastic process</strong>, <span class="math inline">\(\{X(t,\omega),\, t\in\mathbb{R}^{+}\}\)</span> for the continuous time or <span class="math inline">\(\{X_{t}(\omega),\, t\in\mathbb{N}\}\)</span> for the discrete time, is a collection of random variables defined on a <strong>filtered space</strong> <span class="math inline">\((\Omega,\mathcal{F},\{\mathcal{F}_{t}\},P)\)</span><label for="tufte-sn-254" class="margin-toggle sidenote-number">254</label><input type="checkbox" id="tufte-sn-254" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">254</span> More precisely, <span class="math inline">\(X(t,\omega)\)</span> or <span class="math inline">\(X_{t}(\omega)\)</span> is <span class="math inline">\(\mathcal{F}_{t}\)</span>-measurable with the <strong>filteration</strong>
<span class="math display">\[\mathcal{F}_{t}=\underset{\mbox{continuous time}}{\underbrace{\sigma\left(\{X_{\tau}\}_{0&lt;\tau\leq t}\right)}}\;\mbox{or }\\\mathcal{F}_{t}=\underset{\mbox{discrete time}}{\underbrace{\sigma(X_{0},X_{1},\dots,X_{t})}}.\]</span>
We also say <span class="math inline">\(\{X(t,\omega),\, t\in\mathbb{R}^{+}\}\)</span> or <span class="math inline">\(\{X_{t}(\omega),\, t\in\mathbb{N}\}\)</span> is <em>adapted</em> to the <strong>filtered space</strong> <span class="math inline">\((\Omega,\mathcal{F},\{\mathcal{F}_{t}\},P)\)</span>.</span>
From the calculus perspective (differential/difference equations), given the underlying <span class="math inline">\(\omega\)</span>, each variable <span class="math inline">\(X(t, \cdot)\)</span> or <span class="math inline">\(X_{t}(\cdot)\)</span> of the process is a <a href="ch-DE.html#sub:ode">state variable</a> over time <span class="math inline">\(t\)</span>. At any time <span class="math inline">\(t\)</span>, all possible <a href="ch-DE.html#sub:ode">states</a> of <span class="math inline">\(X(t, \omega)\)</span> belong to the <strong>state space</strong> <span class="math inline">\((\mathcal{X}, \sigma(\mathcal{X}))\)</span> where <span class="math display">\[\mathcal{X}:=\left\{x\,|\,X(t,\omega)=x,\:\omega\in\Omega, t\in \mathbb{R}^{+}\right\}\]</span> is the set of all possible <a href="ch-DE.html#sub:ode">states</a> (realizations) for <span class="math inline">\(X(t,\omega)\)</span>.<label for="tufte-sn-255" class="margin-toggle sidenote-number">255</label><input type="checkbox" id="tufte-sn-255" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">255</span> For the discrete time <span class="math inline">\(X_{t}(\cdot)\)</span>, the <strong>state space</strong> <span class="math inline">\((\mathcal{X}, \sigma(\mathcal{X}))\)</span> has a similar form with <span class="math inline">\(\mathcal{X}:=\left\{x\,|\,X_t(\omega)=x,\:\omega\in\Omega, t\in \mathbb{N}\right\}\)</span>.</span>
</li>
</ul>
<div class="solution">
<p class="solution-begin">
Discussion about the filter <span id="sol-start-176" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-176', 'sol-start-176')"></span>
</p>
<div id="sol-body-176" class="solution-body" style="display: none;">
<p>We can rigorously define the <strong>filter</strong> by set theory. Let <span class="math inline">\(\mathcal{F}\)</span> belong to the <a href="sub-axioms.html#sub:axSet">power set</a> of a <a href="sub-set-theory.html#sub:order">partially ordered set</a> <span class="math inline">\(\mathcal{G}\)</span>, namely <span class="math inline">\(\mathcal{F}\subset2^{\mathcal{G}}\)</span>, then <span class="math inline">\(\mathcal{F}\)</span> is a <em>filter</em> on <span class="math inline">\(\mathcal{G}\)</span> if <span class="math inline">\(\mathcal{F}\)</span> satisfies</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\textrm{Ø}\notin\mathcal{F}\)</span>, <span class="math inline">\(\mathcal{G}\in\mathcal{F}\)</span>;</p></li>
<li><p>(intersection-closed) if <span class="math inline">\(\mathcal{F}_{m},\mathcal{F}_{f}\in\mathcal{F}\)</span>, then <span class="math inline">\(\mathcal{F}_{m}\cap\mathcal{F}_{f}\in\mathcal{F}\)</span>;</p></li>
<li><p>(upward-closed) if <span class="math inline">\(\mathcal{F}_{m}\subset\mathcal{F}_{j}\subset\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{F}_{m}\in\mathcal{F}\)</span>, then <span class="math inline">\(\mathcal{F}_{j}\in\mathcal{F}\)</span>.</p></li>
</ol>
<p>In other words, a <strong>filter</strong> selects a <a href="sub-set-theory.html#sub:order">partially ordered</a> subset that satisfy some given criterion (intersection-closed and upward-closed).</p>
<p>In stochastic process, the <strong>filtration</strong> is a <strong>filter</strong> of <span class="math inline">\(\sigma\)</span>-<a href="sub-incomplete.html#sub:beyond2">algebra</a> that gradually selects the historical information to make the information comptible. That is, the information at time <span class="math inline">\(t\)</span> needs to be comptible with the existing one, namely <span class="math inline">\(\mathcal{F}_{s}\subset\mathcal{F}_{t}\)</span> for any <span class="math inline">\(s&lt;t\)</span>. The comptible historical information excludes the possibility of predicting <span class="math inline">\(\mathcal{F}_{t'}\)</span> for any <span class="math inline">\(t'&gt;t\)</span>. Thus, a process that is adapted to a <strong>filtration</strong>, is also called non-anticipating, i.e. one that cannot see into the future beyond the existing historical framework.<label for="tufte-sn-256" class="margin-toggle sidenote-number">256</label><input type="checkbox" id="tufte-sn-256" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">256</span> In sociology, the <strong>filter</strong> also refers to a dating or mating selector that can construct a comptible (similar and complementary) group of candidates for the marriages. Using the language of set theory, we consider all acceptable matching patterns of two persons as the power set <span class="math inline">\(\mathcal{G}\)</span>. Let the social filter <span class="math inline">\(\mathcal{F}\)</span>, the male’s taste <span class="math inline">\(\mathcal{F}_m\)</span>, the female’s taste <span class="math inline">\(\mathcal{F}_f\)</span>, and their tastes with the complement <span class="math inline">\(\mathcal{F}_c\)</span>. The (ideal) filter <span class="math inline">\(\mathcal{F}\)</span> selects a set given the following criteria:
1. Being single is not a choice (<span class="math inline">\(\textrm{Ø}\notin\mathcal{F}\)</span>). And the filter doesn’t exlude any acceptable matching (<span class="math inline">\(\mathcal{G}\in\mathcal{F}\)</span>);
2. The tastes of both sides are similar from the filter’s perspective (if <span class="math inline">\(\mathcal{F}_{m},\mathcal{F}_{f}\in\mathcal{F}\)</span>, then <span class="math inline">\(\mathcal{F}_{m}\cap\mathcal{F}_{f}\in\mathcal{F}\)</span>);
3. The acceptable complementary taste is preserved by the filter (if <span class="math inline">\(\mathcal{F}_{m}, \mathcal{F}_f\subset\mathcal{F}_{c}\subset\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{F}_{m}, \mathcal{F}_f\in\mathcal{F}\)</span>, then <span class="math inline">\(\mathcal{F}_{c}\in\mathcal{F}\)</span>).</span></p>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>Roughly speaking, a <strong>stochastic process</strong> is a collection of random variables indexed by time <span class="math inline">\(t\)</span>.
A <strong>discrete time stochastic processes</strong> <span class="math inline">\(\{X_{1},\dots,X_{T}\}\)</span> is simply a <a href="ch-UnMulti.html#sub:MultiVar">random vector</a> whose random entries emerge following the <a href="sub-inferknow.html#sub:dyn">arrow of time</a>, namely a time series with random entities. However, the flowing time illuminates that the index number <span class="math inline">\(t\)</span> can grow, and that the time series may not remain at a modest size. In this case, the joint probability law <span class="math inline">\(\mathbb{P}(X_{1},\dots,X_{T})\)</span> may need to be characterized by a rather high dimensional <a href="ch-UnMulti.html#sub:MultiVar">multivariate distribution</a> if <span class="math inline">\(t\)</span> grows to a large enough number.<label for="tufte-sn-257" class="margin-toggle sidenote-number">257</label><input type="checkbox" id="tufte-sn-257" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">257</span> For example, a <span class="math inline">\(10\)</span>-dimensional <a href="ch-eigen.html#sub:matNorms">multivariate normal</a> joint density needs to specify <span class="math inline">\(55\)</span> entries of its <a href="ch-eigen.html#sub:matNorms">covariance matrix</a> (half of the off-diagonal entries are identical). That is to say, the computational complexity will grow exponentially as the dimension grows. Nowadays, the length of a simple data vector can easily go beyond <span class="math inline">\(10^{3}\)</span>.</span> The specification of the <a href="ch-CalUn.html#sub:conProb">joint probability law</a> in a high dimensional space may be so complex (in both theoretical and applicable aspects) that we need an alternative method to interpret the law.</p>
<p>One method of fighting with the growing complexity is to keep the law in a tractable representation by using <a href="ch-CalUn.html#sub:conProb">conditioning</a>.<label for="tufte-sn-258" class="margin-toggle sidenote-number">258</label><input type="checkbox" id="tufte-sn-258" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">258</span>  We can see that the <strong>filtration</strong> <span class="math inline">\(\{\mathcal{F}_{t}:t\leq T\}\)</span> is generated by <span class="math inline">\(\{ X_t(\omega) \}_{0&lt;t\leq T}\)</span>. Thus the dependent structure of the series <span class="math inline">\(\{ X_t(\omega) \}_{0&lt;t\leq T}\)</span> also comes from the conditioning of the historical information generated by <span class="math inline">\(\{ X_t(\omega) \}_{0&lt;t&lt; T}\)</span>.</span> Recall that in the simulation of the <a href="ch-UnMulti.html#sub:MultiVar">bivariate normal</a> random vector, we use two one-dimensional random variables by splitting the joint probability law into the conditionals, i.e. <span class="math inline">\(\mathbb{P}(X_{2},X_{1})=\mathbb{P}(X_{2}\,|\, X_{1})\mathbb{P}(X_{1})\)</span>. For a <strong>discrete time stochastic process</strong>, we can recursively apply the splittings to the joint probability<label for="tufte-sn-259" class="margin-toggle sidenote-number">259</label><input type="checkbox" id="tufte-sn-259" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">259</span> For a <strong>continuous time stochastic process</strong>, the splitting trick generally does not work unless the probability law acquires some additional structure that behaves like an exponent, called semi-group. We will see it in sec[?].</span>
<span class="math display">\[\mathbb{P}(X_{1},\dots,X_{T})= \\ \mathbb{P}(X_{T}|X_{T-1},\dots X_{1})\mathbb{P}(X_{T-1}|X_{T-2},\dots X_{1})\cdots\mathbb{P}(X_{2}|X_{1})\mathbb{P}(X_{1}).\]</span>
The splitting works but it does not completely resolve the complexity issue, as the conditional distributions <span class="math inline">\(\mathbb{P}(X_{T}|X_{T-1},\dots X_{1})\)</span>, <span class="math inline">\(\mathbb{P}(X_{T-1}|X_{T-2},\dots X_{1})\)</span>, etc, generally involve an intensive computation (like the one we saw in the multivariate normal case).</p>
<p>The splitting conditionals utilitize the full history of the process up to the current step to determine the probability for the next step, which greatly complicates analysis. However, our sense of the passage of time leads us to arrange events in the following manner: the past of our memory just fades out when we proceed; our perceptions highly depend on our present feelings but not so much on the historical ones. Extending this idea to the conditionals, we can replace those long-term conditions by the short-term ones including one and only one <a href="ch-CalUn.html#sub:conProb">conditioning variable</a>:
<span class="math display">\[\mathbb{P}(X_{1},\dots,X_{T})=\mathbb{P}(X_{T}|X_{T-1})\mathbb{P}(X_{T-1}|X_{T-2})\cdots\mathbb{P}(X_{2}|X_{1})\mathbb{P}(X_{1}),\]</span>
where each conditional probability <span class="math inline">\(\mathbb{P}(X_{t+1}|X_{t})\)</span> is about “one-step” transition, namely
regarding the future evolution at time <span class="math inline">\(t\)</span> depends only on the current state <span class="math inline">\(X_{t}\)</span>.
The system is said to possess the <em>Markov’s principle</em>: a principle of alleviating the complicated long-term dependence by using the simple short-term substitutes. A <strong>stochastic process</strong> whose probability law satisfies the <strong>Markov’s principle</strong> is also called a <em>Markov process</em>.<label for="tufte-sn-260" class="margin-toggle sidenote-number">260</label><input type="checkbox" id="tufte-sn-260" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">260</span> 
<strong>Markov’s principle</strong> is not restrict to stochastic models. Most dynamical problems of our interests satisfy this principle. For instance, the deterministic iterative model <span class="math inline">\(\mathbf{x}_{t+1}=\mathbf{A}\mathbf{x}_{t}\)</span>, and the nonlinear iterative model <span class="math inline">\(\mathbf{x}_{t+1}=f(\mathbf{x}_{t})\)</span> for some continous real-valued function <span class="math inline">\(f(\cdot)\)</span> are <strong>Markov’s models</strong> due to the fact that the dynamics depends on the current state <span class="math inline">\(\mathbf{x}_{t}\)</span>, and that all the previous state <span class="math inline">\(\mathbf{x}_{t-1},\dots,\mathbf{x}_{1}\)</span> are completely irrelevant in the current dynamical law. The <a href="ch-DE.html#sub:ode">automous ODE</a> <span class="math inline">\(\frac{\mbox{d}\mathbf{x}(t)}{\mbox{d}t}=f(\mathbf{x}(t))\)</span> also satisfies the principle for the differential <span class="math inline">\(\mbox{d}\mathbf{x}(t)/\mbox{d}t\)</span>. But the (solution) process <span class="math inline">\(\mathbf{x}(t)=\int_{0}^{t}f(\mathbf{x}(s))\mbox{d}s\)</span> does not.</span></p>
<p>The <strong>Markov’s principle</strong> views the discrete time dynamics as a <em>chain</em> of multiple <strong>transitions</strong>. Given the initial probability <span class="math inline">\(\mathbb{P}(X_1)\)</span>, the dynamics of the process <span class="math inline">\(\{ X_t(\omega) \}_{t=1,\dots, T}\)</span> is completely described by the <em>(one-step) transition probabilities</em>
<span class="math display">\[\mbox{P}_{t}(x,\mathcal{A})=\mathbb{P}\left(X_{t+1}(\omega)\in\mathcal{A}\,|\,X_{t}(\omega)=x\right)\]</span>
that are well defined for any appropriate initial <a href="ch-DE.html#sub:ode">state</a> (of the transition) <span class="math inline">\(x \in \mathcal{X}\)</span> and the target set (of the transition) <span class="math inline">\(\mathcal{A}\in \sigma(\mathcal{X})\)</span>, with <span class="math inline">\(\mbox{P}_{t}(x,\mathcal{X})=1\)</span> at time <span class="math inline">\(t=1,2,\dots, T-1\)</span>. Any multiple step of the transition can be presented as the integral of the transition probabilities. For example, the conditional probability of a two step ahead transition is
<span class="math display">\[\begin{align*}
\mathbb{P}(X_{t+2}\in \mathcal{A} | X_{t}=x) &amp;= \int_{x' \in \mathcal{X}} \mathbb{P}(X_{t+2}\in \mathcal{A} | X_{t+1}=x') \mathbb{P}(X_{t+1}\in dx' | X_{t}=x) 
\\ &amp;=  \int_{x' \in \mathcal{X}} \mbox{P}_{t+1}  (x',\mathcal{A}) \mbox{P}_{t} (x,dx'),
\end{align*}
\]</span>
where the intermediate step on the state variable <span class="math inline">\(X_{t+1}\)</span> is “integrated out.”<label for="tufte-sn-261" class="margin-toggle sidenote-number">261</label><input type="checkbox" id="tufte-sn-261" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">261</span> Note that the infinitesimal state <span class="math inline">\(dx'\)</span> is <a href="ch-CalUn.html#sub:rv">measurable</a> in <span class="math inline">\(\sigma (\mathcal{X})\)</span>.</span> The discrete time <strong>Markov process</strong> is also known as <em>Markov chain</em>. The above integral equation is called <em>Chapman–Kolmogorov equation</em> that identifies the joint probability law via chainning up the transition probabilities.</p>
<p>The <strong>transition probability</strong> is <em>time-homogeneous</em> if <span class="math display">\[\mbox{P}_{t}(x,\mathcal{A})=\mbox{P}(x,\mathcal{A})\]</span>
for all <span class="math inline">\(t=1,\dots, T-1\)</span>. The joint probability of the <strong>time-homogeneous Markov chain</strong> is then chainned up by the identical transition probabilities. The <span class="math inline">\(t+k\)</span>-step ahead <strong>Chapman-Kolmogorov equation</strong> of the <strong>time-homogeneous Markov chain</strong> is
<span class="math display">\[
\begin{align*}
&amp; \mathbb{P}(X_{t+k}\in \mathcal{A} | X_{1}=x)  \\
&amp;= \int_{x'\in\mathcal{X}} \mathbb{P}(X_{t+k}\in \mathcal{A} | X_{t}=x')   \mathbb{P}(X_{t}\in \mbox{d}x' | X_{1}=x) \\
&amp;= 
\int_{x_{t+k-1} \in \mathcal{X}}\cdots \int_{x_{2} \in \mathcal{X}} \mbox{P}(x_{t+k-1},\mathcal{A}) \cdots \mbox{P} (x_{2},\mbox{d}x_{3}) \mbox{P} (x,\mbox{d}x_{2}).
\end{align*}
\]</span></p>
<p>Many practial stochastic models satisfy the <strong>Markov’s principle</strong>. For example, <a href="ch-eigen.html#sub:matNorms">AR(1) model</a> of random variables, <span class="math inline">\(X_{t+1}=\phi X_{t}+\varepsilon_{t}\)</span> is a <strong>Markov chain</strong>, as <span class="math inline">\(X_{t+1}\)</span> is independent of <span class="math inline">\(X_{t-1},X_{t-2},\dots\)</span>, given the current value <span class="math inline">\(X_{t}=x\)</span>.<label for="tufte-sn-262" class="margin-toggle sidenote-number">262</label><input type="checkbox" id="tufte-sn-262" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">262</span> The AR(<span class="math inline">\(k\)</span>) model is also a <strong>Markov chain</strong> in the general sense, but in our context, we only consider the (first-order) <strong>Markov’s principle</strong> with the one-step ahead rather than <span class="math inline">\(k\)</span>-step ahead transitions. However, we can always make an AR(<span class="math inline">\(k\)</span>) model <span class="math inline">\(X_{t+1}=\phi_{1}X_{t}+\cdots\phi_{k}X_{t-k+1}+\varepsilon_{t}\)</span> satisfy the (first-order) <strong>Markov’s principle</strong> by constructing a <em>vector AR(1)</em> model <span class="math inline">\(\mathbf{X}_{t+1} = \Phi \mathbf{X}_{t} +\mathbf{e}_t\)</span> in terms of the <a href="ch-UnMulti.html#sub:MultiVar">multivariate</a> vector <span class="math inline">\(\mathbf{X}_{t+1}=[X_{t+1}(\omega),\dots X_{t-k+1}(\omega)]^{\top}\)</span> where
<span class="math display">\[\Phi=\left[\begin{array}{cccc}
\phi_{1} &amp; \cdots &amp; \cdots &amp; \phi_{k}\\
1 &amp;  &amp; 0 &amp; 0\\
 &amp; \ddots &amp;  &amp; \vdots\\
0 &amp;  &amp; 1 &amp; 0
\end{array}\right]\]</span> and <span class="math inline">\(\mathbf{e}_t = [\varepsilon_{t}, 0, \dots ,0]^\top\)</span>.</span> Let’s consider a specific AR(1) model whose errors are <a href="ch-CalUn.html#sub:divRV">normal random variables</a> <span class="math inline">\(\varepsilon_t \sim \mathcal{N}(0,\sigma^2)\)</span>. Given the state value <span class="math inline">\(X_{t}(\omega)=x\)</span>, the linear representation of the normal random <span class="math inline">\(\varepsilon_t\)</span> tells that <span class="math inline">\(\phi x +\varepsilon_t \sim \mathcal{N}(\phi x, \sigma^2)\)</span>. So the conditional probability <span class="math inline">\(\mathbb{P}(X_{t+1} \in \mathcal{A}| X_{t}=x)\)</span> of this AR(1) has the <strong>transition probability</strong> <span class="math inline">\(\mbox{P}_t(x,\mathcal{A})=\Pr (\mathcal{N}(\phi x, \sigma^2)\in \mathcal{A})\)</span>.</p>
<p><strong>Markov’s principle</strong> will simplify a surprisingly wide variety of phenomena if we only consider the <strong>state space</strong> of <a href="ch-CalUn.html#sub:rv">discrete random variables</a>. That is, all possilbe states are <a href="ch-CalUn.html#sub:rv">discrete states</a>. Then the <strong>transition probability</strong> for <strong>Markov chain</strong> can be expressed compactly by a <strong>(probability) transition matrix</strong>.</p>
<p>A <em>stochastic matrix</em> is a square matrix <span class="math inline">\(\mathbf{P}\)</span> with entries <span class="math inline">\(\{p_{ij}\}\)</span> such that <span class="math inline">\(p_{ij}\geq0\)</span> for all <span class="math inline">\(i,j\)</span>, and for each row <span class="math inline">\(i\)</span>, <span class="math inline">\(\sum_{j}p_{ij}=1\)</span>. This matrix becomes the <em>probability transition matrix</em> for <strong>Markov chain</strong> if each entries represents the probability of a one-step transition amongst the states. Let’s illustrate this matrix through a social mobility model. Sociologists broadly categorize the population of a country into upper- (U), middle- (M), and lower (L)-class brackets. One of their concerns is to monitor the movement of successive generations among these three classes. We can model these three classes as three states. Let <span class="math inline">\(X_t(\omega)\)</span> be the class for the <span class="math inline">\(t\)</span>-th generation of a family. Then the state space of <span class="math inline">\(X_t(\omega)\)</span> is <span class="math inline">\((\mathcal{X}, \sigma(\mathcal{X}))\)</span> with <span class="math inline">\(\mathcal{X}=\{\mbox{L}, \mbox{M}, \mbox{U}\}\)</span>. <strong>Markov’s principle</strong> implies that the class of any generation does not depend on the ancestry but only on the class of its parent generation. If we assume that this the social mobility transition patterns holds for any generation <span class="math inline">\(t\)</span> in the family, then <span class="math inline">\(X_t(\omega)\)</span> is a <strong>time-homogenous Markov chain</strong>. The <strong>time-homogenous transition probabilities</strong> <span class="math inline">\(\mbox{P}(x,x')=\mathbb{P}\left(X_{t+1}(\omega)=x'\,|\,X_{t}(\omega)=x\right)\)</span> for <span class="math inline">\(x,x'\in \{\mbox{L}, \mbox{M}, \mbox{U}\}\)</span> is contained in the following table</p>
<table>
<thead><tr class="header">
<th align="center"><span class="math inline">\(X_{t+1} \backslash X_{t}\)</span></th>
<th align="center"><span class="math inline">\(\mbox{L}\)</span></th>
<th align="center"><span class="math inline">\(\mbox{M}\)</span></th>
<th align="center"><span class="math inline">\(\mbox{U}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\mbox{L}\)</span></td>
<td align="center">0.45</td>
<td align="center">0.5</td>
<td align="center">0.05</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\mbox{M}\)</span></td>
<td align="center">0.15</td>
<td align="center">0.65</td>
<td align="center">0.2</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\mbox{U}\)</span></td>
<td align="center">0</td>
<td align="center">0.5</td>
<td align="center">0.5</td>
</tr>
</tbody>
</table>
<p>In addition, if we label <span class="math inline">\(\mbox{L}=1\)</span>, <span class="math inline">\(\mbox{M}=2\)</span>, <span class="math inline">\(\mbox{U}=3\)</span>, then we can use a <strong>probability transition matrix</strong> to present the previous table:
<span class="math display">\[\mathbf{P}=[p_{ij}]_{1\leq i,j \leq 3} = \left[\begin{array}{ccc}
0.45 &amp; 0.5 &amp; 0.05\\
0.15 &amp; 0.65 &amp; 0.2\\
0 &amp; 0.5 &amp; 0.5
\end{array}\right]\]</span>
where <span class="math inline">\(p_{ij}=\mathbb{P}(X_{t+1}=j|X_{t}=i)\)</span> represents the probability of one-step transition from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> at any generation <span class="math inline">\(t\)</span>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span id="fig:socialM"></span>
<img src="fig/Part3/socialM.png" alt="Graphic representation of the probability transition matrix " width="100%"><!--
<p class="caption marginnote">-->Figure 13.3: Graphic representation of the probability transition matrix <!--</p>-->
<!--</div>--></span>
</p>
<p>Sometimes, it more straightforward to visualize the <strong>probability transition matrix</strong> through its graphic representation. The states are the nodes in the graph, and each probability entry is the flow between two nodes. That is, a (probabilistic) flow departs from the vertex <span class="math inline">\(j\)</span> and enters the vertex <span class="math inline">\(i\)</span> with the weight <span class="math inline">\(p_{ij}\)</span>.<label for="tufte-sn-263" class="margin-toggle sidenote-number">263</label><input type="checkbox" id="tufte-sn-263" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">263</span> The reason of existing a graphic representation for each <strong>Markov chain</strong> comes from that fact that any complete network is a <strong>chain</strong>. Recall that a graph or a network corresponds a collection of order pairs <span class="math inline">\((\mathcal{V},\mathcal{E})\)</span>. In the set theoretic sense, a <strong>chain</strong> means a <a href="sub-set-theory.html#sub:order">total ordered set</a>. The completeness here refers to the fact that <span class="math inline">\((\mathcal{V},\mathcal{E})\)</span> gives a <a href="sub-set-theory.html#sub:order">total ordered set</a>: namely any two vertices in the network can be decided whether they are connected or not.</span></p>
<div class="solution">
<p class="solution-begin">
Code <span id="sol-start-177" class="fa fa-plus-square solution-icon clickable" onclick="toggle_visibility('sol-body-177', 'sol-start-177')"></span>
</p>
<div id="sol-body-177" class="solution-body" style="display: none;">
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb39-1" data-line-number="1">P=<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.45</span>,<span class="fl">0.5</span>,<span class="fl">0.05</span>,</a>
<a class="sourceLine" id="cb39-2" data-line-number="2">           <span class="fl">0.15</span>,<span class="fl">0.65</span>,<span class="fl">0.2</span>,</a>
<a class="sourceLine" id="cb39-3" data-line-number="3">           <span class="dv">0</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>), <span class="dt">nrow=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb39-4" data-line-number="4"><span class="kw">library</span>(diagram)</a>
<a class="sourceLine" id="cb39-5" data-line-number="5"><span class="kw">rownames</span>(P)=<span class="kw">c</span>(<span class="st">"L"</span>, <span class="st">"M"</span>, <span class="st">"U"</span>); <span class="kw">plotmat</span>(P, <span class="dt">lwd =</span> <span class="dv">1</span>, <span class="dt">box.lwd =</span> <span class="dv">2</span>, <span class="dt">cex.txt =</span> <span class="fl">0.8</span>, </a>
<a class="sourceLine" id="cb39-6" data-line-number="6">               <span class="dt">box.size =</span> <span class="fl">0.1</span>, <span class="dt">box.prop =</span> <span class="fl">0.4</span>, <span class="dt">relsize=</span><span class="fl">0.7</span>,</a>
<a class="sourceLine" id="cb39-7" data-line-number="7">               <span class="dt">main =</span> <span class="st">"Social mobility"</span>)</a></code></pre></div>
<p class="solution-end">
<span class="fa fa-square-o solution-icon"></span>
</p>
</div>
</div>
<p>The <strong>Chapman-Kolmogorov equation</strong> for <strong>time-homogenous Markov chain</strong> now is simplified as a matrix-matrix multiplication
<span class="math display">\[\mathbf{P}^{k+t}=\mathbf{P}^{k}\mathbf{P}^{t}\]</span>
where <span class="math inline">\(p^{t+k}_{ij}=\sum_{s=1,2,3}p^{k}_{is}p^{t}_{sj}\)</span> for states <span class="math inline">\(i, j\)</span> and the state <span class="math inline">\(s\)</span> stands for the intermediate state that can take any of the three possibilities. To derive the above result, let’s consider the meaning of <span class="math inline">\(\mathbf{P}^2\)</span>. Notice that the meaning of <span class="math inline">\(p_{is}p_{sj}\)</span> is <span class="math display">\[\begin{align*}
p_{is}p_{sj}&amp;=\mathbb{P}(X_{t+2}=i|X_{t+1}=s)\mathbb{P}(X_{t+1}=s|X_t=j)\\
&amp;=\mathbb{P}(X_{t+2}=i,X_{t+1}=s|X_t=j)
\end{align*}
\]</span>
where the second equality comes from the <strong>Markov’s principle</strong> of the joint probability. Then by calculating all the possible states of <span class="math inline">\(s\)</span>, we have <span class="math inline">\(\mathbb{P}(X_{t+2}=i|X_t=j)= \sum_{s=1,2,3} p_{is}p_{sj}\)</span>. Because
<span class="math display">\[\mathbf{P}^2 = \left[\sum_{s}p_{is}p_{sj}\right]_{1\leq i\,,\,j\leq3}\]</span> we know that <span class="math inline">\(\mathbf{P}^2\)</span> is <strong>probability transition matrix</strong> of two-steps ahead, namely <span class="math inline">\(\mathbb{P}(X_{t+2}|X_t)\)</span>.
We can generalize the result that the <span class="math inline">\(k\)</span>-step <strong>probability transition matrix</strong>of this <strong>time-homogeneous Markov chain</strong> must be <span class="math inline">\(\mathbf{P}^k\)</span>.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb40-1" data-line-number="1">P<span class="op">%*%</span>P </a></code></pre></div>
<pre><code>##        [,1]   [,2]   [,3]
## [1,] 0.2775 0.5750 0.1475
## [2,] 0.1650 0.5975 0.2375
## [3,] 0.0750 0.5750 0.3500</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" data-line-number="1"><span class="kw">rowSums</span>(P<span class="op">%*%</span>P)</a></code></pre></div>
<pre><code>## [1] 1 1 1</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb44-1" data-line-number="1"><span class="kw">library</span>(expm) <span class="co"># Matrix power </span></a>
<a class="sourceLine" id="cb44-2" data-line-number="2">P<span class="op">%^%</span><span class="dv">10</span> </a></code></pre></div>
<pre><code>##           [,1]      [,2]      [,3]
## [1,] 0.1606135 0.5882353 0.2511512
## [2,] 0.1604433 0.5882353 0.2513214
## [3,] 0.1602730 0.5882353 0.2514917</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb46-1" data-line-number="1"><span class="kw">rowSums</span>(P<span class="op">%^%</span><span class="dv">10</span>)</a></code></pre></div>
<pre><code>## [1] 1 1 1</code></pre>
<p>Because any <span class="math inline">\(k\)</span> power of the probability transition matrix <span class="math inline">\(\mathbf{P}\)</span> can be splitted as the sum of products of <span class="math inline">\(l\)</span>-step and <span class="math inline">\(k − l\)</span>-step transition probability matrices, we can split <span class="math inline">\(\mathbf{P}^{k+t}\)</span> as the products of <span class="math inline">\(\mathbf{P}^{k}\)</span> and <span class="math inline">\(\mathbf{P}^{t}\)</span> that exactly coincides with the purpose of <strong>Chapman-Kolmogorov equation</strong>.</p>
<p>The idea of modeling dynamics by chains originated from a metaphysical topic called the <em>great chain of being</em> where the chain was conceived as a static hierarchy, starting with God at the top and descending through angels, human beings, animals, etc. This progression of the life forms gave the basis for the idea of <em>evolution</em>: complex structures emerge from simpler forms through (natural) selection mechanisms.<label for="tufte-sn-264" class="margin-toggle sidenote-number">264</label><input type="checkbox" id="tufte-sn-264" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">264</span> In political and social science, creating secular governmental structures that vested power into various classes of citizens was viewed as an evolutionary movements in Hegel, Marx and Engels’ work. However, the theory of evolution plays several different roles over there, some toward increasing the order and complexity while some mean just the opposite.</span> Another feature of the chain is that although it was viewed as one continuous whole, it has the potential for the missing and overlapping links. Each link in the chain might be splitted further. Based on the dual nature of the chain - divided yet united, Carl Linnaeus, who is considered as one of the founders of ecology, formalised his modern system of naming organisms (taxonomy).</p>
</div>
</div></body></html>

<p style="text-align: center;">
<a href="ch-eigen.html"><button class="btn btn-default">Previous</button></a>
<a href="bibliography.html"><button class="btn btn-default">Next</button></a>
</p>
<p class="build-date">Page built: 
2020-07-30
</p>
</div>
</div>



</body>
</html>
